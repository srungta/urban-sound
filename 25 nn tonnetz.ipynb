{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set up code to visualize a sound form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "from librosa import load, display\n",
    "import glob\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "import pickle\n",
    "from common import save_as_pickle\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should change these paths according to the path of the files on your system.\n",
    "PATH_TO_TRAIN_LABELS = \"data/train/train.csv\"\n",
    "PATH_TO_TEST_LABELS = \"data/test/test.csv\"\n",
    "PATH_TO_TRAIN_AUDIO_FILES = \"data/train/wav/\"\n",
    "PATH_TO_TEST_AUDIO_FILES = \"data/test/wav/\"\n",
    "PATH_TO_SUBMISSION = \"submission/\"\n",
    "PATH_TO_PICKLE = \"pickles/\"\n",
    "SUBMISSION_TITLE = \"nn tonnetz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is easier to deal with csv if you can load it into a structure you can work with.\n",
    "# Pandas are the most convenient way to do that and are available with \n",
    "# inbuilt functionality to handle csv file.\n",
    "\n",
    "# Pandas assumes that the first row in your file is the header adn not the actual values.\n",
    "# This behavior can be overriden by passing header=None as a parameter.\n",
    "train = pd.read_csv(PATH_TO_TRAIN_LABELS)\n",
    "test = pd.read_csv(PATH_TO_TEST_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can reactivate this cell to make sure your model is working correctly in terms of dimensions.\n",
    "#train = train[:2]\n",
    "#test = test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_count = 0\n",
    "train_error_labels = []\n",
    "test_error_count = 0\n",
    "test_error_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start with classification, we first need to convert the wav sound files into a format we can work \n",
    "# with. It is easier to take the amplitude at each sampling point and use that \n",
    "# numeric value to form a feature vector.\n",
    "def train_parser(row):\n",
    "    global train_error_count\n",
    "    global train_error_labels\n",
    "    path_to_wav_files = PATH_TO_TRAIN_AUDIO_FILES\n",
    "    file_path = path_to_wav_files + str(row.ID) + \".wav\"\n",
    "    try:\n",
    "        data, sampling_rate = librosa.load(file_path)\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(data),sr=sampling_rate).T,axis=0)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        train_error_count += 1\n",
    "        train_error_labels.append(row.ID)\n",
    "        return [0]*6, row.Class\n",
    "    features = tonnetz\n",
    "    label = row.Class\n",
    "    return [features, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4527de52dd242dfa54c24892f49dba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5435), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\librosa\\core\\pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n",
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\librosa\\util\\utils.py:1467: RuntimeWarning: invalid value encountered in less\n",
      "  if np.any(X < 0) or np.any(X_ref < 0):\n",
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\librosa\\util\\utils.py:1480: RuntimeWarning: invalid value encountered in less\n",
      "  bad_idx = (Z < np.finfo(dtype).tiny)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio buffer is not finite everywhere\n",
      "Audio buffer is not finite everywhere\n",
      "Audio buffer is not finite everywhere\n",
      "\n",
      "3 samples had errors while parsing\n",
      "Errorneous samples [300, 1182, 1488]\n"
     ]
    }
   ],
   "source": [
    "# To create the training feature matrix, we can apply our parser to each training sample.\n",
    "train_features = train.progress_apply(train_parser,axis=1)\n",
    "print(\"%d samples had errors while parsing\" % train_error_count)\n",
    "print(\"Errorneous samples\", train_error_labels)\n",
    "save_as_pickle(data=train_features,pickle_file=PATH_TO_PICKLE + SUBMISSION_TITLE + \" train.pickle\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to singnify what they mean helps with documentation,\n",
    "# and also helps you keep track of them later on.\n",
    "train_features.columns = ['feature','label']\n",
    "# train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# this library helps us convert string labels into easy to handle encoded labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_features.feature.tolist())\n",
    "Y = np.array(train_features.label.tolist())\n",
    "lb = LabelEncoder()\n",
    "# Since labels are categories they dont inherently have an order amongst themselves.\n",
    "# For example, Apples > oranges does not make any sense. So to madel such categorical \n",
    "# variables, we can convert them to one hot vectors.\n",
    "Y = to_categorical(lb.fit_transform(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_labels = Y.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, input_shape=(6,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(number_of_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 70,154\n",
      "Trainable params: 70,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5435/5435 [==============================] - ETA: 1:29 - loss: 2.3026 - acc: 0.062 - ETA: 11s - loss: 2.3015 - acc: 0.113 - ETA: 6s - loss: 2.3002 - acc: 0.1146 - ETA: 4s - loss: 2.2979 - acc: 0.120 - ETA: 3s - loss: 2.2956 - acc: 0.123 - ETA: 2s - loss: 2.2913 - acc: 0.135 - ETA: 2s - loss: 2.2875 - acc: 0.136 - ETA: 1s - loss: 2.2850 - acc: 0.134 - ETA: 1s - loss: 2.2810 - acc: 0.136 - ETA: 1s - loss: 2.2799 - acc: 0.132 - ETA: 1s - loss: 2.2766 - acc: 0.134 - ETA: 1s - loss: 2.2747 - acc: 0.132 - ETA: 1s - loss: 2.2731 - acc: 0.133 - ETA: 1s - loss: 2.2711 - acc: 0.132 - ETA: 0s - loss: 2.2665 - acc: 0.134 - ETA: 0s - loss: 2.2649 - acc: 0.136 - ETA: 0s - loss: 2.2622 - acc: 0.136 - ETA: 0s - loss: 2.2609 - acc: 0.140 - ETA: 0s - loss: 2.2565 - acc: 0.145 - ETA: 0s - loss: 2.2523 - acc: 0.145 - ETA: 0s - loss: 2.2499 - acc: 0.146 - ETA: 0s - loss: 2.2458 - acc: 0.147 - ETA: 0s - loss: 2.2444 - acc: 0.147 - ETA: 0s - loss: 2.2416 - acc: 0.147 - 2s 331us/step - loss: 2.2392 - acc: 0.1490\n",
      "Epoch 2/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 2.2200 - acc: 0.312 - ETA: 0s - loss: 2.1639 - acc: 0.215 - ETA: 1s - loss: 2.1867 - acc: 0.202 - ETA: 1s - loss: 2.1927 - acc: 0.195 - ETA: 1s - loss: 2.1889 - acc: 0.205 - ETA: 1s - loss: 2.1739 - acc: 0.204 - ETA: 0s - loss: 2.1631 - acc: 0.203 - ETA: 0s - loss: 2.1515 - acc: 0.207 - ETA: 0s - loss: 2.1522 - acc: 0.202 - ETA: 0s - loss: 2.1483 - acc: 0.207 - ETA: 0s - loss: 2.1534 - acc: 0.207 - ETA: 0s - loss: 2.1465 - acc: 0.210 - ETA: 0s - loss: 2.1411 - acc: 0.213 - ETA: 0s - loss: 2.1400 - acc: 0.213 - ETA: 0s - loss: 2.1432 - acc: 0.213 - ETA: 0s - loss: 2.1345 - acc: 0.217 - ETA: 0s - loss: 2.1357 - acc: 0.214 - ETA: 0s - loss: 2.1315 - acc: 0.216 - ETA: 0s - loss: 2.1312 - acc: 0.214 - ETA: 0s - loss: 2.1295 - acc: 0.212 - ETA: 0s - loss: 2.1320 - acc: 0.212 - ETA: 0s - loss: 2.1327 - acc: 0.212 - 1s 217us/step - loss: 2.1320 - acc: 0.2125\n",
      "Epoch 3/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1139 - acc: 0.250 - ETA: 1s - loss: 2.1472 - acc: 0.194 - ETA: 0s - loss: 2.1226 - acc: 0.213 - ETA: 0s - loss: 2.1184 - acc: 0.213 - ETA: 0s - loss: 2.1128 - acc: 0.207 - ETA: 0s - loss: 2.1229 - acc: 0.202 - ETA: 0s - loss: 2.1099 - acc: 0.207 - ETA: 0s - loss: 2.1084 - acc: 0.205 - ETA: 0s - loss: 2.1023 - acc: 0.212 - ETA: 0s - loss: 2.1016 - acc: 0.214 - ETA: 0s - loss: 2.0989 - acc: 0.214 - ETA: 0s - loss: 2.0977 - acc: 0.213 - ETA: 0s - loss: 2.0989 - acc: 0.213 - ETA: 0s - loss: 2.0945 - acc: 0.215 - ETA: 0s - loss: 2.0879 - acc: 0.219 - ETA: 0s - loss: 2.0895 - acc: 0.218 - ETA: 0s - loss: 2.0827 - acc: 0.223 - ETA: 0s - loss: 2.0847 - acc: 0.225 - ETA: 0s - loss: 2.0865 - acc: 0.224 - 1s 184us/step - loss: 2.0875 - acc: 0.2256\n",
      "Epoch 4/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1439 - acc: 0.125 - ETA: 0s - loss: 2.1324 - acc: 0.192 - ETA: 0s - loss: 2.1075 - acc: 0.207 - ETA: 0s - loss: 2.0943 - acc: 0.215 - ETA: 0s - loss: 2.0874 - acc: 0.216 - ETA: 0s - loss: 2.0812 - acc: 0.224 - ETA: 0s - loss: 2.0820 - acc: 0.227 - ETA: 0s - loss: 2.0779 - acc: 0.227 - ETA: 0s - loss: 2.0767 - acc: 0.231 - ETA: 0s - loss: 2.0776 - acc: 0.229 - ETA: 0s - loss: 2.0739 - acc: 0.235 - ETA: 0s - loss: 2.0710 - acc: 0.237 - ETA: 0s - loss: 2.0732 - acc: 0.234 - ETA: 0s - loss: 2.0698 - acc: 0.234 - ETA: 0s - loss: 2.0708 - acc: 0.233 - ETA: 0s - loss: 2.0735 - acc: 0.232 - ETA: 0s - loss: 2.0721 - acc: 0.234 - ETA: 0s - loss: 2.0706 - acc: 0.235 - 1s 175us/step - loss: 2.0709 - acc: 0.2348\n",
      "Epoch 5/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8320 - acc: 0.375 - ETA: 1s - loss: 2.0604 - acc: 0.274 - ETA: 1s - loss: 2.0554 - acc: 0.261 - ETA: 1s - loss: 2.0503 - acc: 0.248 - ETA: 0s - loss: 2.0532 - acc: 0.246 - ETA: 0s - loss: 2.0533 - acc: 0.250 - ETA: 0s - loss: 2.0532 - acc: 0.246 - ETA: 0s - loss: 2.0558 - acc: 0.247 - ETA: 0s - loss: 2.0620 - acc: 0.239 - ETA: 0s - loss: 2.0601 - acc: 0.240 - ETA: 0s - loss: 2.0550 - acc: 0.243 - ETA: 0s - loss: 2.0559 - acc: 0.244 - ETA: 0s - loss: 2.0561 - acc: 0.241 - ETA: 0s - loss: 2.0530 - acc: 0.241 - ETA: 0s - loss: 2.0561 - acc: 0.238 - ETA: 0s - loss: 2.0562 - acc: 0.238 - ETA: 0s - loss: 2.0559 - acc: 0.238 - ETA: 0s - loss: 2.0552 - acc: 0.239 - ETA: 0s - loss: 2.0535 - acc: 0.241 - ETA: 0s - loss: 2.0538 - acc: 0.240 - 1s 187us/step - loss: 2.0539 - acc: 0.2410\n",
      "Epoch 6/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0419 - acc: 0.125 - ETA: 0s - loss: 2.0540 - acc: 0.213 - ETA: 0s - loss: 2.0658 - acc: 0.225 - ETA: 0s - loss: 2.0720 - acc: 0.234 - ETA: 0s - loss: 2.0643 - acc: 0.236 - ETA: 0s - loss: 2.0568 - acc: 0.243 - ETA: 0s - loss: 2.0496 - acc: 0.248 - ETA: 0s - loss: 2.0483 - acc: 0.252 - ETA: 0s - loss: 2.0497 - acc: 0.251 - ETA: 0s - loss: 2.0473 - acc: 0.252 - ETA: 0s - loss: 2.0459 - acc: 0.248 - ETA: 0s - loss: 2.0459 - acc: 0.249 - ETA: 0s - loss: 2.0444 - acc: 0.251 - ETA: 0s - loss: 2.0463 - acc: 0.250 - ETA: 0s - loss: 2.0448 - acc: 0.250 - ETA: 0s - loss: 2.0457 - acc: 0.250 - ETA: 0s - loss: 2.0448 - acc: 0.250 - ETA: 0s - loss: 2.0420 - acc: 0.251 - ETA: 0s - loss: 2.0413 - acc: 0.251 - ETA: 0s - loss: 2.0419 - acc: 0.247 - ETA: 0s - loss: 2.0395 - acc: 0.246 - ETA: 0s - loss: 2.0411 - acc: 0.246 - 1s 212us/step - loss: 2.0401 - acc: 0.2464\n",
      "Epoch 7/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1419 - acc: 0.156 - ETA: 1s - loss: 2.0272 - acc: 0.290 - ETA: 1s - loss: 2.0130 - acc: 0.262 - ETA: 1s - loss: 2.0038 - acc: 0.260 - ETA: 1s - loss: 2.0126 - acc: 0.259 - ETA: 1s - loss: 2.0110 - acc: 0.251 - ETA: 1s - loss: 2.0137 - acc: 0.250 - ETA: 1s - loss: 2.0136 - acc: 0.246 - ETA: 1s - loss: 2.0191 - acc: 0.242 - ETA: 1s - loss: 2.0174 - acc: 0.246 - ETA: 0s - loss: 2.0055 - acc: 0.258 - ETA: 0s - loss: 2.0105 - acc: 0.256 - ETA: 0s - loss: 2.0159 - acc: 0.252 - ETA: 0s - loss: 2.0165 - acc: 0.253 - ETA: 0s - loss: 2.0171 - acc: 0.253 - ETA: 0s - loss: 2.0209 - acc: 0.252 - ETA: 0s - loss: 2.0188 - acc: 0.253 - ETA: 0s - loss: 2.0214 - acc: 0.254 - ETA: 0s - loss: 2.0210 - acc: 0.252 - ETA: 0s - loss: 2.0243 - acc: 0.250 - ETA: 0s - loss: 2.0240 - acc: 0.251 - ETA: 0s - loss: 2.0259 - acc: 0.251 - ETA: 0s - loss: 2.0216 - acc: 0.252 - ETA: 0s - loss: 2.0249 - acc: 0.251 - ETA: 0s - loss: 2.0258 - acc: 0.252 - ETA: 0s - loss: 2.0216 - acc: 0.253 - ETA: 0s - loss: 2.0242 - acc: 0.252 - ETA: 0s - loss: 2.0251 - acc: 0.253 - ETA: 0s - loss: 2.0254 - acc: 0.252 - ETA: 0s - loss: 2.0262 - acc: 0.251 - ETA: 0s - loss: 2.0272 - acc: 0.251 - ETA: 0s - loss: 2.0275 - acc: 0.252 - 2s 322us/step - loss: 2.0269 - acc: 0.2526\n",
      "Epoch 8/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0075 - acc: 0.343 - ETA: 1s - loss: 2.0716 - acc: 0.242 - ETA: 1s - loss: 2.0726 - acc: 0.243 - ETA: 1s - loss: 2.0510 - acc: 0.248 - ETA: 1s - loss: 2.0559 - acc: 0.239 - ETA: 1s - loss: 2.0679 - acc: 0.235 - ETA: 1s - loss: 2.0684 - acc: 0.232 - ETA: 1s - loss: 2.0633 - acc: 0.232 - ETA: 1s - loss: 2.0582 - acc: 0.236 - ETA: 1s - loss: 2.0548 - acc: 0.238 - ETA: 1s - loss: 2.0437 - acc: 0.243 - ETA: 1s - loss: 2.0405 - acc: 0.243 - ETA: 1s - loss: 2.0319 - acc: 0.247 - ETA: 1s - loss: 2.0364 - acc: 0.249 - ETA: 1s - loss: 2.0366 - acc: 0.248 - ETA: 1s - loss: 2.0356 - acc: 0.248 - ETA: 1s - loss: 2.0330 - acc: 0.248 - ETA: 0s - loss: 2.0289 - acc: 0.247 - ETA: 0s - loss: 2.0240 - acc: 0.250 - ETA: 0s - loss: 2.0247 - acc: 0.250 - ETA: 0s - loss: 2.0223 - acc: 0.251 - ETA: 0s - loss: 2.0241 - acc: 0.251 - ETA: 0s - loss: 2.0280 - acc: 0.251 - ETA: 0s - loss: 2.0266 - acc: 0.253 - ETA: 0s - loss: 2.0273 - acc: 0.252 - ETA: 0s - loss: 2.0294 - acc: 0.251 - ETA: 0s - loss: 2.0267 - acc: 0.253 - ETA: 0s - loss: 2.0265 - acc: 0.254 - ETA: 0s - loss: 2.0288 - acc: 0.254 - ETA: 0s - loss: 2.0271 - acc: 0.256 - ETA: 0s - loss: 2.0227 - acc: 0.257 - ETA: 0s - loss: 2.0245 - acc: 0.256 - ETA: 0s - loss: 2.0223 - acc: 0.255 - ETA: 0s - loss: 2.0236 - acc: 0.254 - ETA: 0s - loss: 2.0229 - acc: 0.256 - 2s 352us/step - loss: 2.0250 - acc: 0.2550\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 2s - loss: 2.1627 - acc: 0.218 - ETA: 5s - loss: 2.1010 - acc: 0.281 - ETA: 2s - loss: 2.0532 - acc: 0.273 - ETA: 1s - loss: 2.0163 - acc: 0.270 - ETA: 1s - loss: 2.0118 - acc: 0.269 - ETA: 1s - loss: 2.0218 - acc: 0.261 - ETA: 1s - loss: 2.0236 - acc: 0.255 - ETA: 1s - loss: 2.0186 - acc: 0.255 - ETA: 1s - loss: 2.0020 - acc: 0.263 - ETA: 1s - loss: 2.0060 - acc: 0.263 - ETA: 1s - loss: 2.0036 - acc: 0.265 - ETA: 1s - loss: 1.9993 - acc: 0.271 - ETA: 1s - loss: 1.9979 - acc: 0.273 - ETA: 1s - loss: 2.0007 - acc: 0.273 - ETA: 0s - loss: 2.0048 - acc: 0.268 - ETA: 0s - loss: 2.0017 - acc: 0.271 - ETA: 0s - loss: 2.0044 - acc: 0.272 - ETA: 0s - loss: 2.0046 - acc: 0.275 - ETA: 0s - loss: 2.0090 - acc: 0.274 - ETA: 0s - loss: 2.0095 - acc: 0.271 - ETA: 0s - loss: 2.0109 - acc: 0.269 - ETA: 0s - loss: 2.0095 - acc: 0.268 - ETA: 0s - loss: 2.0083 - acc: 0.269 - ETA: 0s - loss: 2.0082 - acc: 0.269 - ETA: 0s - loss: 2.0081 - acc: 0.269 - ETA: 0s - loss: 2.0094 - acc: 0.268 - ETA: 0s - loss: 2.0098 - acc: 0.267 - ETA: 0s - loss: 2.0128 - acc: 0.264 - ETA: 0s - loss: 2.0120 - acc: 0.264 - ETA: 0s - loss: 2.0117 - acc: 0.263 - ETA: 0s - loss: 2.0130 - acc: 0.263 - ETA: 0s - loss: 2.0159 - acc: 0.262 - 2s 353us/step - loss: 2.0141 - acc: 0.2620\n",
      "Epoch 10/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0346 - acc: 0.343 - ETA: 1s - loss: 2.0133 - acc: 0.298 - ETA: 1s - loss: 1.9981 - acc: 0.302 - ETA: 1s - loss: 1.9900 - acc: 0.300 - ETA: 1s - loss: 1.9971 - acc: 0.293 - ETA: 1s - loss: 2.0024 - acc: 0.284 - ETA: 1s - loss: 2.0004 - acc: 0.281 - ETA: 1s - loss: 1.9889 - acc: 0.283 - ETA: 1s - loss: 1.9838 - acc: 0.281 - ETA: 1s - loss: 1.9893 - acc: 0.282 - ETA: 1s - loss: 1.9900 - acc: 0.281 - ETA: 0s - loss: 1.9905 - acc: 0.278 - ETA: 0s - loss: 1.9956 - acc: 0.273 - ETA: 0s - loss: 1.9934 - acc: 0.275 - ETA: 0s - loss: 1.9974 - acc: 0.273 - ETA: 0s - loss: 1.9952 - acc: 0.278 - ETA: 0s - loss: 1.9950 - acc: 0.275 - ETA: 0s - loss: 1.9941 - acc: 0.275 - ETA: 0s - loss: 1.9940 - acc: 0.273 - ETA: 0s - loss: 1.9949 - acc: 0.271 - ETA: 0s - loss: 1.9979 - acc: 0.269 - ETA: 0s - loss: 2.0003 - acc: 0.267 - ETA: 0s - loss: 2.0015 - acc: 0.266 - ETA: 0s - loss: 2.0027 - acc: 0.266 - ETA: 0s - loss: 2.0040 - acc: 0.266 - ETA: 0s - loss: 2.0069 - acc: 0.264 - ETA: 0s - loss: 2.0083 - acc: 0.263 - ETA: 0s - loss: 2.0082 - acc: 0.264 - 2s 301us/step - loss: 2.0097 - acc: 0.2644\n",
      "Epoch 11/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0869 - acc: 0.281 - ETA: 1s - loss: 2.0416 - acc: 0.267 - ETA: 1s - loss: 2.0241 - acc: 0.259 - ETA: 0s - loss: 2.0084 - acc: 0.247 - ETA: 0s - loss: 2.0006 - acc: 0.262 - ETA: 0s - loss: 2.0001 - acc: 0.268 - ETA: 0s - loss: 2.0025 - acc: 0.265 - ETA: 0s - loss: 1.9982 - acc: 0.270 - ETA: 0s - loss: 2.0014 - acc: 0.267 - ETA: 0s - loss: 1.9918 - acc: 0.270 - ETA: 0s - loss: 1.9923 - acc: 0.269 - ETA: 0s - loss: 1.9960 - acc: 0.269 - ETA: 0s - loss: 2.0012 - acc: 0.269 - ETA: 0s - loss: 1.9964 - acc: 0.268 - ETA: 0s - loss: 1.9935 - acc: 0.271 - ETA: 0s - loss: 1.9919 - acc: 0.271 - ETA: 0s - loss: 1.9931 - acc: 0.270 - ETA: 0s - loss: 1.9945 - acc: 0.271 - ETA: 0s - loss: 1.9920 - acc: 0.272 - ETA: 0s - loss: 1.9930 - acc: 0.271 - ETA: 0s - loss: 1.9930 - acc: 0.271 - ETA: 0s - loss: 1.9935 - acc: 0.270 - ETA: 0s - loss: 1.9969 - acc: 0.267 - ETA: 0s - loss: 1.9954 - acc: 0.268 - ETA: 0s - loss: 1.9959 - acc: 0.267 - 1s 243us/step - loss: 1.9966 - acc: 0.2679\n",
      "Epoch 12/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9772 - acc: 0.156 - ETA: 1s - loss: 1.9111 - acc: 0.261 - ETA: 1s - loss: 1.9627 - acc: 0.257 - ETA: 1s - loss: 1.9776 - acc: 0.263 - ETA: 0s - loss: 1.9815 - acc: 0.274 - ETA: 0s - loss: 1.9786 - acc: 0.265 - ETA: 0s - loss: 1.9784 - acc: 0.267 - ETA: 0s - loss: 1.9814 - acc: 0.264 - ETA: 0s - loss: 1.9782 - acc: 0.264 - ETA: 0s - loss: 1.9721 - acc: 0.269 - ETA: 0s - loss: 1.9798 - acc: 0.266 - ETA: 0s - loss: 1.9834 - acc: 0.264 - ETA: 0s - loss: 1.9827 - acc: 0.265 - ETA: 0s - loss: 1.9808 - acc: 0.268 - ETA: 0s - loss: 1.9782 - acc: 0.270 - ETA: 0s - loss: 1.9838 - acc: 0.269 - ETA: 0s - loss: 1.9846 - acc: 0.270 - ETA: 0s - loss: 1.9828 - acc: 0.271 - ETA: 0s - loss: 1.9830 - acc: 0.274 - ETA: 0s - loss: 1.9863 - acc: 0.274 - ETA: 0s - loss: 1.9886 - acc: 0.275 - ETA: 0s - loss: 1.9904 - acc: 0.274 - ETA: 0s - loss: 1.9903 - acc: 0.275 - ETA: 0s - loss: 1.9924 - acc: 0.273 - ETA: 0s - loss: 1.9937 - acc: 0.271 - 1s 249us/step - loss: 1.9931 - acc: 0.2729\n",
      "Epoch 13/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.9256 - acc: 0.281 - ETA: 1s - loss: 2.0421 - acc: 0.263 - ETA: 1s - loss: 2.0360 - acc: 0.281 - ETA: 1s - loss: 2.0157 - acc: 0.278 - ETA: 1s - loss: 2.0059 - acc: 0.293 - ETA: 1s - loss: 1.9910 - acc: 0.293 - ETA: 0s - loss: 1.9928 - acc: 0.287 - ETA: 0s - loss: 1.9895 - acc: 0.288 - ETA: 0s - loss: 1.9907 - acc: 0.285 - ETA: 0s - loss: 1.9941 - acc: 0.282 - ETA: 0s - loss: 1.9921 - acc: 0.280 - ETA: 0s - loss: 1.9885 - acc: 0.279 - ETA: 0s - loss: 1.9887 - acc: 0.279 - ETA: 0s - loss: 1.9849 - acc: 0.279 - ETA: 0s - loss: 1.9808 - acc: 0.283 - ETA: 0s - loss: 1.9780 - acc: 0.282 - ETA: 0s - loss: 1.9811 - acc: 0.279 - ETA: 0s - loss: 1.9781 - acc: 0.279 - ETA: 0s - loss: 1.9800 - acc: 0.278 - ETA: 0s - loss: 1.9816 - acc: 0.277 - ETA: 0s - loss: 1.9864 - acc: 0.277 - ETA: 0s - loss: 1.9836 - acc: 0.278 - ETA: 0s - loss: 1.9863 - acc: 0.277 - ETA: 0s - loss: 1.9872 - acc: 0.277 - ETA: 0s - loss: 1.9883 - acc: 0.277 - ETA: 0s - loss: 1.9865 - acc: 0.278 - ETA: 0s - loss: 1.9851 - acc: 0.280 - 1s 262us/step - loss: 1.9847 - acc: 0.2800\n",
      "Epoch 14/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0299 - acc: 0.187 - ETA: 0s - loss: 1.9684 - acc: 0.262 - ETA: 0s - loss: 1.9406 - acc: 0.279 - ETA: 0s - loss: 1.9389 - acc: 0.289 - ETA: 0s - loss: 1.9583 - acc: 0.281 - ETA: 0s - loss: 1.9632 - acc: 0.280 - ETA: 0s - loss: 1.9667 - acc: 0.281 - ETA: 0s - loss: 1.9714 - acc: 0.284 - ETA: 0s - loss: 1.9740 - acc: 0.282 - ETA: 0s - loss: 1.9687 - acc: 0.286 - ETA: 0s - loss: 1.9776 - acc: 0.283 - ETA: 0s - loss: 1.9701 - acc: 0.286 - ETA: 0s - loss: 1.9723 - acc: 0.286 - ETA: 0s - loss: 1.9818 - acc: 0.283 - ETA: 0s - loss: 1.9823 - acc: 0.284 - ETA: 0s - loss: 1.9822 - acc: 0.285 - ETA: 0s - loss: 1.9817 - acc: 0.284 - ETA: 0s - loss: 1.9810 - acc: 0.285 - ETA: 0s - loss: 1.9796 - acc: 0.284 - ETA: 0s - loss: 1.9792 - acc: 0.286 - ETA: 0s - loss: 1.9771 - acc: 0.285 - 1s 201us/step - loss: 1.9762 - acc: 0.2865\n",
      "Epoch 15/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0333 - acc: 0.218 - ETA: 0s - loss: 2.0098 - acc: 0.265 - ETA: 0s - loss: 2.0132 - acc: 0.250 - ETA: 0s - loss: 1.9874 - acc: 0.265 - ETA: 0s - loss: 1.9637 - acc: 0.288 - ETA: 0s - loss: 1.9690 - acc: 0.286 - ETA: 0s - loss: 1.9695 - acc: 0.285 - ETA: 0s - loss: 1.9604 - acc: 0.288 - ETA: 0s - loss: 1.9559 - acc: 0.293 - ETA: 0s - loss: 1.9510 - acc: 0.294 - ETA: 0s - loss: 1.9538 - acc: 0.290 - ETA: 0s - loss: 1.9577 - acc: 0.289 - ETA: 0s - loss: 1.9565 - acc: 0.288 - ETA: 0s - loss: 1.9522 - acc: 0.289 - ETA: 0s - loss: 1.9566 - acc: 0.287 - ETA: 0s - loss: 1.9574 - acc: 0.286 - ETA: 0s - loss: 1.9574 - acc: 0.285 - ETA: 0s - loss: 1.9610 - acc: 0.283 - ETA: 0s - loss: 1.9605 - acc: 0.282 - ETA: 0s - loss: 1.9613 - acc: 0.283 - ETA: 0s - loss: 1.9635 - acc: 0.282 - ETA: 0s - loss: 1.9654 - acc: 0.282 - 1s 215us/step - loss: 1.9695 - acc: 0.2821\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1921 - acc: 0.218 - ETA: 1s - loss: 2.0220 - acc: 0.246 - ETA: 1s - loss: 2.0220 - acc: 0.260 - ETA: 1s - loss: 2.0066 - acc: 0.270 - ETA: 1s - loss: 1.9907 - acc: 0.281 - ETA: 0s - loss: 1.9754 - acc: 0.285 - ETA: 0s - loss: 1.9716 - acc: 0.283 - ETA: 0s - loss: 1.9683 - acc: 0.286 - ETA: 0s - loss: 1.9739 - acc: 0.287 - ETA: 0s - loss: 1.9744 - acc: 0.284 - ETA: 0s - loss: 1.9875 - acc: 0.283 - ETA: 0s - loss: 1.9857 - acc: 0.280 - ETA: 0s - loss: 1.9801 - acc: 0.283 - ETA: 0s - loss: 1.9785 - acc: 0.282 - ETA: 0s - loss: 1.9761 - acc: 0.285 - ETA: 0s - loss: 1.9762 - acc: 0.287 - ETA: 0s - loss: 1.9744 - acc: 0.287 - ETA: 0s - loss: 1.9729 - acc: 0.288 - ETA: 0s - loss: 1.9728 - acc: 0.288 - ETA: 0s - loss: 1.9705 - acc: 0.289 - ETA: 0s - loss: 1.9685 - acc: 0.289 - 1s 207us/step - loss: 1.9696 - acc: 0.2878\n",
      "Epoch 17/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0217 - acc: 0.250 - ETA: 0s - loss: 1.9865 - acc: 0.286 - ETA: 0s - loss: 1.9810 - acc: 0.282 - ETA: 0s - loss: 1.9579 - acc: 0.286 - ETA: 0s - loss: 1.9658 - acc: 0.276 - ETA: 0s - loss: 1.9671 - acc: 0.277 - ETA: 0s - loss: 1.9724 - acc: 0.275 - ETA: 0s - loss: 1.9702 - acc: 0.280 - ETA: 0s - loss: 1.9811 - acc: 0.277 - ETA: 0s - loss: 1.9716 - acc: 0.282 - ETA: 0s - loss: 1.9747 - acc: 0.283 - ETA: 0s - loss: 1.9718 - acc: 0.284 - ETA: 0s - loss: 1.9680 - acc: 0.284 - ETA: 0s - loss: 1.9686 - acc: 0.282 - ETA: 0s - loss: 1.9653 - acc: 0.285 - ETA: 0s - loss: 1.9642 - acc: 0.288 - ETA: 0s - loss: 1.9631 - acc: 0.290 - ETA: 0s - loss: 1.9584 - acc: 0.293 - ETA: 0s - loss: 1.9620 - acc: 0.291 - ETA: 0s - loss: 1.9601 - acc: 0.292 - ETA: 0s - loss: 1.9599 - acc: 0.291 - 1s 200us/step - loss: 1.9603 - acc: 0.2916\n",
      "Epoch 18/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1303 - acc: 0.250 - ETA: 1s - loss: 2.0071 - acc: 0.295 - ETA: 1s - loss: 1.9765 - acc: 0.298 - ETA: 1s - loss: 1.9806 - acc: 0.294 - ETA: 1s - loss: 1.9674 - acc: 0.300 - ETA: 1s - loss: 1.9779 - acc: 0.296 - ETA: 1s - loss: 1.9661 - acc: 0.298 - ETA: 1s - loss: 1.9661 - acc: 0.295 - ETA: 1s - loss: 1.9706 - acc: 0.296 - ETA: 1s - loss: 1.9695 - acc: 0.296 - ETA: 0s - loss: 1.9666 - acc: 0.296 - ETA: 0s - loss: 1.9646 - acc: 0.294 - ETA: 0s - loss: 1.9646 - acc: 0.298 - ETA: 0s - loss: 1.9616 - acc: 0.299 - ETA: 0s - loss: 1.9658 - acc: 0.298 - ETA: 0s - loss: 1.9597 - acc: 0.302 - ETA: 0s - loss: 1.9623 - acc: 0.300 - ETA: 0s - loss: 1.9665 - acc: 0.298 - ETA: 0s - loss: 1.9639 - acc: 0.299 - ETA: 0s - loss: 1.9662 - acc: 0.298 - ETA: 0s - loss: 1.9628 - acc: 0.298 - ETA: 0s - loss: 1.9608 - acc: 0.299 - ETA: 0s - loss: 1.9589 - acc: 0.298 - ETA: 0s - loss: 1.9579 - acc: 0.300 - ETA: 0s - loss: 1.9562 - acc: 0.299 - 1s 246us/step - loss: 1.9567 - acc: 0.2975\n",
      "Epoch 19/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9321 - acc: 0.281 - ETA: 0s - loss: 2.0234 - acc: 0.268 - ETA: 0s - loss: 1.9705 - acc: 0.276 - ETA: 0s - loss: 1.9572 - acc: 0.292 - ETA: 0s - loss: 1.9495 - acc: 0.302 - ETA: 0s - loss: 1.9398 - acc: 0.305 - ETA: 0s - loss: 1.9361 - acc: 0.303 - ETA: 0s - loss: 1.9415 - acc: 0.298 - ETA: 0s - loss: 1.9403 - acc: 0.300 - ETA: 0s - loss: 1.9486 - acc: 0.295 - ETA: 0s - loss: 1.9449 - acc: 0.299 - ETA: 0s - loss: 1.9502 - acc: 0.298 - ETA: 0s - loss: 1.9456 - acc: 0.298 - ETA: 0s - loss: 1.9534 - acc: 0.295 - ETA: 0s - loss: 1.9570 - acc: 0.292 - ETA: 0s - loss: 1.9548 - acc: 0.293 - ETA: 0s - loss: 1.9579 - acc: 0.293 - ETA: 0s - loss: 1.9562 - acc: 0.295 - ETA: 0s - loss: 1.9529 - acc: 0.297 - ETA: 0s - loss: 1.9494 - acc: 0.299 - ETA: 0s - loss: 1.9506 - acc: 0.297 - ETA: 0s - loss: 1.9510 - acc: 0.299 - ETA: 0s - loss: 1.9483 - acc: 0.298 - 1s 222us/step - loss: 1.9486 - acc: 0.2986\n",
      "Epoch 20/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8578 - acc: 0.343 - ETA: 0s - loss: 1.9165 - acc: 0.315 - ETA: 0s - loss: 1.9190 - acc: 0.312 - ETA: 0s - loss: 1.9322 - acc: 0.315 - ETA: 0s - loss: 1.9326 - acc: 0.320 - ETA: 0s - loss: 1.9325 - acc: 0.311 - ETA: 0s - loss: 1.9272 - acc: 0.313 - ETA: 0s - loss: 1.9267 - acc: 0.312 - ETA: 0s - loss: 1.9321 - acc: 0.312 - ETA: 0s - loss: 1.9421 - acc: 0.308 - ETA: 0s - loss: 1.9493 - acc: 0.307 - ETA: 0s - loss: 1.9491 - acc: 0.305 - ETA: 0s - loss: 1.9493 - acc: 0.302 - ETA: 0s - loss: 1.9482 - acc: 0.304 - ETA: 0s - loss: 1.9461 - acc: 0.303 - ETA: 0s - loss: 1.9501 - acc: 0.301 - ETA: 0s - loss: 1.9526 - acc: 0.301 - ETA: 0s - loss: 1.9507 - acc: 0.300 - ETA: 0s - loss: 1.9491 - acc: 0.300 - 1s 186us/step - loss: 1.9458 - acc: 0.3025\n",
      "Epoch 21/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6813 - acc: 0.468 - ETA: 1s - loss: 1.8934 - acc: 0.320 - ETA: 1s - loss: 1.9207 - acc: 0.316 - ETA: 0s - loss: 1.9287 - acc: 0.312 - ETA: 0s - loss: 1.9257 - acc: 0.318 - ETA: 0s - loss: 1.9301 - acc: 0.314 - ETA: 0s - loss: 1.9315 - acc: 0.307 - ETA: 0s - loss: 1.9267 - acc: 0.306 - ETA: 0s - loss: 1.9300 - acc: 0.302 - ETA: 0s - loss: 1.9303 - acc: 0.300 - ETA: 0s - loss: 1.9405 - acc: 0.292 - ETA: 0s - loss: 1.9382 - acc: 0.294 - ETA: 0s - loss: 1.9384 - acc: 0.295 - ETA: 0s - loss: 1.9393 - acc: 0.294 - ETA: 0s - loss: 1.9352 - acc: 0.295 - ETA: 0s - loss: 1.9394 - acc: 0.290 - ETA: 0s - loss: 1.9398 - acc: 0.292 - ETA: 0s - loss: 1.9438 - acc: 0.290 - ETA: 0s - loss: 1.9441 - acc: 0.291 - ETA: 0s - loss: 1.9404 - acc: 0.295 - ETA: 0s - loss: 1.9402 - acc: 0.295 - ETA: 0s - loss: 1.9404 - acc: 0.295 - 1s 217us/step - loss: 1.9397 - acc: 0.2957\n",
      "Epoch 22/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1781 - acc: 0.250 - ETA: 1s - loss: 1.8960 - acc: 0.305 - ETA: 1s - loss: 1.9255 - acc: 0.324 - ETA: 0s - loss: 1.9220 - acc: 0.313 - ETA: 0s - loss: 1.9473 - acc: 0.303 - ETA: 0s - loss: 1.9336 - acc: 0.307 - ETA: 0s - loss: 1.9310 - acc: 0.309 - ETA: 0s - loss: 1.9280 - acc: 0.315 - ETA: 0s - loss: 1.9209 - acc: 0.315 - ETA: 0s - loss: 1.9207 - acc: 0.318 - ETA: 0s - loss: 1.9232 - acc: 0.312 - ETA: 0s - loss: 1.9246 - acc: 0.312 - ETA: 0s - loss: 1.9237 - acc: 0.313 - ETA: 0s - loss: 1.9271 - acc: 0.313 - ETA: 0s - loss: 1.9290 - acc: 0.312 - ETA: 0s - loss: 1.9321 - acc: 0.309 - ETA: 0s - loss: 1.9305 - acc: 0.308 - ETA: 0s - loss: 1.9288 - acc: 0.308 - ETA: 0s - loss: 1.9302 - acc: 0.307 - ETA: 0s - loss: 1.9310 - acc: 0.305 - ETA: 0s - loss: 1.9318 - acc: 0.306 - 1s 205us/step - loss: 1.9333 - acc: 0.3030\n",
      "Epoch 23/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9105 - acc: 0.250 - ETA: 1s - loss: 1.9946 - acc: 0.241 - ETA: 1s - loss: 1.9872 - acc: 0.255 - ETA: 1s - loss: 1.9826 - acc: 0.269 - ETA: 1s - loss: 1.9648 - acc: 0.287 - ETA: 1s - loss: 1.9527 - acc: 0.293 - ETA: 1s - loss: 1.9504 - acc: 0.290 - ETA: 1s - loss: 1.9553 - acc: 0.286 - ETA: 1s - loss: 1.9506 - acc: 0.290 - ETA: 1s - loss: 1.9425 - acc: 0.293 - ETA: 0s - loss: 1.9462 - acc: 0.291 - ETA: 0s - loss: 1.9432 - acc: 0.290 - ETA: 0s - loss: 1.9491 - acc: 0.285 - ETA: 0s - loss: 1.9524 - acc: 0.287 - ETA: 0s - loss: 1.9472 - acc: 0.291 - ETA: 0s - loss: 1.9494 - acc: 0.291 - ETA: 0s - loss: 1.9537 - acc: 0.288 - ETA: 0s - loss: 1.9507 - acc: 0.290 - ETA: 0s - loss: 1.9518 - acc: 0.290 - ETA: 0s - loss: 1.9460 - acc: 0.291 - ETA: 0s - loss: 1.9464 - acc: 0.291 - ETA: 0s - loss: 1.9443 - acc: 0.293 - ETA: 0s - loss: 1.9438 - acc: 0.294 - ETA: 0s - loss: 1.9447 - acc: 0.293 - 1s 237us/step - loss: 1.9427 - acc: 0.2944\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1657 - acc: 0.250 - ETA: 0s - loss: 1.8681 - acc: 0.315 - ETA: 0s - loss: 1.9027 - acc: 0.300 - ETA: 0s - loss: 1.9088 - acc: 0.294 - ETA: 0s - loss: 1.9308 - acc: 0.288 - ETA: 0s - loss: 1.9295 - acc: 0.289 - ETA: 0s - loss: 1.9263 - acc: 0.299 - ETA: 0s - loss: 1.9244 - acc: 0.297 - ETA: 0s - loss: 1.9295 - acc: 0.293 - ETA: 0s - loss: 1.9317 - acc: 0.294 - ETA: 0s - loss: 1.9315 - acc: 0.294 - ETA: 0s - loss: 1.9299 - acc: 0.295 - ETA: 0s - loss: 1.9225 - acc: 0.299 - ETA: 0s - loss: 1.9291 - acc: 0.297 - ETA: 0s - loss: 1.9298 - acc: 0.298 - ETA: 0s - loss: 1.9291 - acc: 0.300 - ETA: 0s - loss: 1.9291 - acc: 0.303 - ETA: 0s - loss: 1.9308 - acc: 0.303 - ETA: 0s - loss: 1.9329 - acc: 0.302 - 1s 185us/step - loss: 1.9328 - acc: 0.3029\n",
      "Epoch 25/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9489 - acc: 0.343 - ETA: 0s - loss: 1.9298 - acc: 0.340 - ETA: 0s - loss: 1.9425 - acc: 0.317 - ETA: 0s - loss: 1.9248 - acc: 0.320 - ETA: 0s - loss: 1.9283 - acc: 0.310 - ETA: 0s - loss: 1.9211 - acc: 0.315 - ETA: 0s - loss: 1.9409 - acc: 0.297 - ETA: 0s - loss: 1.9403 - acc: 0.297 - ETA: 0s - loss: 1.9406 - acc: 0.293 - ETA: 0s - loss: 1.9466 - acc: 0.291 - ETA: 0s - loss: 1.9404 - acc: 0.294 - ETA: 0s - loss: 1.9444 - acc: 0.292 - ETA: 0s - loss: 1.9467 - acc: 0.288 - ETA: 0s - loss: 1.9400 - acc: 0.292 - ETA: 0s - loss: 1.9378 - acc: 0.292 - ETA: 0s - loss: 1.9367 - acc: 0.293 - ETA: 0s - loss: 1.9369 - acc: 0.294 - ETA: 0s - loss: 1.9355 - acc: 0.295 - ETA: 0s - loss: 1.9325 - acc: 0.295 - ETA: 0s - loss: 1.9334 - acc: 0.294 - ETA: 0s - loss: 1.9339 - acc: 0.292 - ETA: 0s - loss: 1.9281 - acc: 0.296 - ETA: 0s - loss: 1.9321 - acc: 0.295 - ETA: 0s - loss: 1.9313 - acc: 0.296 - ETA: 0s - loss: 1.9293 - acc: 0.297 - 1s 241us/step - loss: 1.9275 - acc: 0.2986\n",
      "Epoch 26/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0212 - acc: 0.312 - ETA: 1s - loss: 1.9209 - acc: 0.299 - ETA: 1s - loss: 1.9172 - acc: 0.305 - ETA: 1s - loss: 1.9206 - acc: 0.306 - ETA: 1s - loss: 1.9330 - acc: 0.299 - ETA: 1s - loss: 1.9370 - acc: 0.296 - ETA: 1s - loss: 1.9394 - acc: 0.295 - ETA: 0s - loss: 1.9434 - acc: 0.293 - ETA: 0s - loss: 1.9371 - acc: 0.294 - ETA: 0s - loss: 1.9259 - acc: 0.297 - ETA: 0s - loss: 1.9318 - acc: 0.294 - ETA: 0s - loss: 1.9413 - acc: 0.287 - ETA: 0s - loss: 1.9313 - acc: 0.291 - ETA: 0s - loss: 1.9310 - acc: 0.292 - ETA: 0s - loss: 1.9245 - acc: 0.298 - ETA: 0s - loss: 1.9213 - acc: 0.300 - ETA: 0s - loss: 1.9245 - acc: 0.296 - ETA: 0s - loss: 1.9247 - acc: 0.296 - ETA: 0s - loss: 1.9250 - acc: 0.295 - ETA: 0s - loss: 1.9231 - acc: 0.297 - ETA: 0s - loss: 1.9218 - acc: 0.299 - ETA: 0s - loss: 1.9207 - acc: 0.300 - ETA: 0s - loss: 1.9192 - acc: 0.301 - ETA: 0s - loss: 1.9158 - acc: 0.302 - ETA: 0s - loss: 1.9154 - acc: 0.301 - 1s 237us/step - loss: 1.9189 - acc: 0.3008\n",
      "Epoch 27/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9416 - acc: 0.218 - ETA: 1s - loss: 1.9191 - acc: 0.302 - ETA: 1s - loss: 1.9211 - acc: 0.300 - ETA: 1s - loss: 1.9209 - acc: 0.305 - ETA: 1s - loss: 1.8923 - acc: 0.313 - ETA: 1s - loss: 1.8971 - acc: 0.311 - ETA: 0s - loss: 1.8828 - acc: 0.314 - ETA: 0s - loss: 1.8900 - acc: 0.314 - ETA: 0s - loss: 1.8953 - acc: 0.310 - ETA: 0s - loss: 1.9013 - acc: 0.312 - ETA: 0s - loss: 1.9085 - acc: 0.313 - ETA: 0s - loss: 1.9097 - acc: 0.314 - ETA: 0s - loss: 1.9094 - acc: 0.314 - ETA: 0s - loss: 1.9073 - acc: 0.312 - ETA: 0s - loss: 1.9048 - acc: 0.313 - ETA: 0s - loss: 1.9088 - acc: 0.308 - ETA: 0s - loss: 1.9106 - acc: 0.307 - ETA: 0s - loss: 1.9116 - acc: 0.307 - ETA: 0s - loss: 1.9118 - acc: 0.308 - ETA: 0s - loss: 1.9162 - acc: 0.308 - ETA: 0s - loss: 1.9136 - acc: 0.309 - ETA: 0s - loss: 1.9167 - acc: 0.307 - ETA: 0s - loss: 1.9146 - acc: 0.307 - 1s 229us/step - loss: 1.9160 - acc: 0.3063\n",
      "Epoch 28/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0498 - acc: 0.312 - ETA: 0s - loss: 1.9281 - acc: 0.290 - ETA: 0s - loss: 1.9202 - acc: 0.291 - ETA: 0s - loss: 1.8877 - acc: 0.313 - ETA: 0s - loss: 1.8768 - acc: 0.326 - ETA: 0s - loss: 1.8770 - acc: 0.327 - ETA: 0s - loss: 1.8769 - acc: 0.329 - ETA: 0s - loss: 1.8810 - acc: 0.330 - ETA: 0s - loss: 1.8891 - acc: 0.325 - ETA: 0s - loss: 1.8977 - acc: 0.318 - ETA: 0s - loss: 1.9041 - acc: 0.317 - ETA: 0s - loss: 1.9055 - acc: 0.316 - ETA: 0s - loss: 1.9153 - acc: 0.311 - ETA: 0s - loss: 1.9093 - acc: 0.313 - ETA: 0s - loss: 1.9131 - acc: 0.312 - ETA: 0s - loss: 1.9163 - acc: 0.309 - ETA: 0s - loss: 1.9160 - acc: 0.308 - ETA: 0s - loss: 1.9187 - acc: 0.307 - ETA: 0s - loss: 1.9201 - acc: 0.307 - ETA: 0s - loss: 1.9177 - acc: 0.306 - 1s 193us/step - loss: 1.9123 - acc: 0.3091\n",
      "Epoch 29/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9438 - acc: 0.281 - ETA: 1s - loss: 1.8820 - acc: 0.309 - ETA: 1s - loss: 1.8929 - acc: 0.307 - ETA: 0s - loss: 1.9083 - acc: 0.305 - ETA: 0s - loss: 1.8874 - acc: 0.318 - ETA: 0s - loss: 1.9033 - acc: 0.311 - ETA: 0s - loss: 1.9129 - acc: 0.311 - ETA: 0s - loss: 1.9106 - acc: 0.313 - ETA: 0s - loss: 1.9147 - acc: 0.312 - ETA: 0s - loss: 1.9105 - acc: 0.312 - ETA: 0s - loss: 1.9023 - acc: 0.314 - ETA: 0s - loss: 1.8988 - acc: 0.315 - ETA: 0s - loss: 1.8954 - acc: 0.316 - ETA: 0s - loss: 1.8973 - acc: 0.319 - ETA: 0s - loss: 1.9051 - acc: 0.313 - ETA: 0s - loss: 1.9044 - acc: 0.313 - ETA: 0s - loss: 1.9032 - acc: 0.312 - ETA: 0s - loss: 1.9031 - acc: 0.313 - ETA: 0s - loss: 1.9068 - acc: 0.311 - ETA: 0s - loss: 1.9127 - acc: 0.308 - ETA: 0s - loss: 1.9104 - acc: 0.308 - 1s 208us/step - loss: 1.9102 - acc: 0.3086\n",
      "Epoch 30/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0373 - acc: 0.250 - ETA: 0s - loss: 1.9019 - acc: 0.325 - ETA: 0s - loss: 1.9148 - acc: 0.305 - ETA: 0s - loss: 1.9169 - acc: 0.309 - ETA: 0s - loss: 1.9083 - acc: 0.314 - ETA: 0s - loss: 1.9095 - acc: 0.308 - ETA: 0s - loss: 1.8955 - acc: 0.314 - ETA: 0s - loss: 1.8957 - acc: 0.317 - ETA: 0s - loss: 1.9051 - acc: 0.313 - ETA: 0s - loss: 1.9109 - acc: 0.311 - ETA: 0s - loss: 1.9115 - acc: 0.308 - ETA: 0s - loss: 1.9080 - acc: 0.309 - ETA: 0s - loss: 1.9057 - acc: 0.309 - ETA: 0s - loss: 1.9129 - acc: 0.308 - ETA: 0s - loss: 1.9152 - acc: 0.306 - ETA: 0s - loss: 1.9148 - acc: 0.305 - ETA: 0s - loss: 1.9132 - acc: 0.306 - ETA: 0s - loss: 1.9180 - acc: 0.304 - ETA: 0s - loss: 1.9148 - acc: 0.304 - ETA: 0s - loss: 1.9117 - acc: 0.305 - ETA: 0s - loss: 1.9126 - acc: 0.306 - 1s 204us/step - loss: 1.9088 - acc: 0.3075\n",
      "Epoch 31/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7595 - acc: 0.437 - ETA: 0s - loss: 1.8871 - acc: 0.328 - ETA: 0s - loss: 1.8860 - acc: 0.322 - ETA: 0s - loss: 1.8767 - acc: 0.322 - ETA: 0s - loss: 1.8855 - acc: 0.320 - ETA: 0s - loss: 1.9025 - acc: 0.315 - ETA: 0s - loss: 1.8915 - acc: 0.314 - ETA: 0s - loss: 1.8922 - acc: 0.315 - ETA: 0s - loss: 1.8925 - acc: 0.313 - ETA: 0s - loss: 1.8966 - acc: 0.314 - ETA: 0s - loss: 1.8949 - acc: 0.314 - ETA: 0s - loss: 1.8978 - acc: 0.311 - ETA: 0s - loss: 1.9003 - acc: 0.313 - ETA: 0s - loss: 1.8973 - acc: 0.314 - ETA: 0s - loss: 1.9066 - acc: 0.310 - ETA: 0s - loss: 1.9119 - acc: 0.305 - ETA: 0s - loss: 1.9082 - acc: 0.307 - ETA: 0s - loss: 1.9080 - acc: 0.306 - ETA: 0s - loss: 1.9105 - acc: 0.306 - ETA: 0s - loss: 1.9075 - acc: 0.307 - 1s 189us/step - loss: 1.9058 - acc: 0.3091\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7825 - acc: 0.250 - ETA: 0s - loss: 1.8470 - acc: 0.335 - ETA: 0s - loss: 1.8373 - acc: 0.343 - ETA: 0s - loss: 1.8519 - acc: 0.339 - ETA: 0s - loss: 1.8639 - acc: 0.322 - ETA: 0s - loss: 1.8637 - acc: 0.322 - ETA: 0s - loss: 1.8652 - acc: 0.326 - ETA: 0s - loss: 1.8781 - acc: 0.320 - ETA: 0s - loss: 1.8823 - acc: 0.319 - ETA: 0s - loss: 1.8926 - acc: 0.316 - ETA: 0s - loss: 1.8993 - acc: 0.312 - ETA: 0s - loss: 1.9043 - acc: 0.311 - ETA: 0s - loss: 1.9077 - acc: 0.311 - ETA: 0s - loss: 1.9107 - acc: 0.311 - ETA: 0s - loss: 1.9097 - acc: 0.314 - ETA: 0s - loss: 1.9071 - acc: 0.314 - ETA: 0s - loss: 1.9029 - acc: 0.314 - ETA: 0s - loss: 1.9007 - acc: 0.314 - ETA: 0s - loss: 1.9012 - acc: 0.310 - ETA: 0s - loss: 1.9030 - acc: 0.309 - ETA: 0s - loss: 1.9007 - acc: 0.311 - ETA: 0s - loss: 1.8995 - acc: 0.311 - 1s 210us/step - loss: 1.8993 - acc: 0.3109\n",
      "Epoch 33/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7436 - acc: 0.406 - ETA: 2s - loss: 1.8456 - acc: 0.325 - ETA: 1s - loss: 1.8534 - acc: 0.321 - ETA: 1s - loss: 1.9028 - acc: 0.301 - ETA: 1s - loss: 1.8613 - acc: 0.304 - ETA: 1s - loss: 1.8968 - acc: 0.299 - ETA: 0s - loss: 1.8990 - acc: 0.303 - ETA: 0s - loss: 1.8967 - acc: 0.315 - ETA: 0s - loss: 1.8999 - acc: 0.318 - ETA: 0s - loss: 1.8992 - acc: 0.318 - ETA: 0s - loss: 1.8934 - acc: 0.319 - ETA: 0s - loss: 1.8916 - acc: 0.315 - ETA: 0s - loss: 1.8906 - acc: 0.314 - ETA: 0s - loss: 1.8913 - acc: 0.312 - ETA: 0s - loss: 1.8899 - acc: 0.315 - ETA: 0s - loss: 1.8921 - acc: 0.315 - ETA: 0s - loss: 1.8931 - acc: 0.314 - ETA: 0s - loss: 1.8921 - acc: 0.314 - ETA: 0s - loss: 1.8936 - acc: 0.315 - ETA: 0s - loss: 1.8886 - acc: 0.318 - ETA: 0s - loss: 1.8886 - acc: 0.320 - 1s 201us/step - loss: 1.8890 - acc: 0.3190\n",
      "Epoch 34/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1087 - acc: 0.250 - ETA: 1s - loss: 1.9018 - acc: 0.300 - ETA: 0s - loss: 1.9064 - acc: 0.294 - ETA: 0s - loss: 1.9178 - acc: 0.304 - ETA: 0s - loss: 1.9280 - acc: 0.295 - ETA: 0s - loss: 1.9252 - acc: 0.301 - ETA: 0s - loss: 1.9165 - acc: 0.302 - ETA: 0s - loss: 1.9215 - acc: 0.303 - ETA: 0s - loss: 1.9145 - acc: 0.308 - ETA: 0s - loss: 1.9136 - acc: 0.312 - ETA: 0s - loss: 1.9064 - acc: 0.314 - ETA: 0s - loss: 1.9068 - acc: 0.313 - ETA: 0s - loss: 1.9043 - acc: 0.315 - ETA: 0s - loss: 1.8967 - acc: 0.317 - ETA: 0s - loss: 1.8959 - acc: 0.315 - ETA: 0s - loss: 1.8983 - acc: 0.316 - ETA: 0s - loss: 1.8976 - acc: 0.314 - ETA: 0s - loss: 1.8963 - acc: 0.315 - ETA: 0s - loss: 1.8901 - acc: 0.318 - ETA: 0s - loss: 1.8900 - acc: 0.318 - 1s 191us/step - loss: 1.8893 - acc: 0.3187\n",
      "Epoch 35/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6745 - acc: 0.437 - ETA: 0s - loss: 1.8208 - acc: 0.377 - ETA: 0s - loss: 1.8663 - acc: 0.343 - ETA: 0s - loss: 1.8806 - acc: 0.324 - ETA: 0s - loss: 1.8791 - acc: 0.325 - ETA: 0s - loss: 1.8789 - acc: 0.324 - ETA: 0s - loss: 1.8788 - acc: 0.326 - ETA: 0s - loss: 1.8724 - acc: 0.333 - ETA: 0s - loss: 1.8713 - acc: 0.335 - ETA: 0s - loss: 1.8740 - acc: 0.333 - ETA: 0s - loss: 1.8845 - acc: 0.329 - ETA: 0s - loss: 1.8831 - acc: 0.332 - ETA: 0s - loss: 1.8883 - acc: 0.327 - ETA: 0s - loss: 1.8923 - acc: 0.321 - ETA: 0s - loss: 1.8873 - acc: 0.322 - ETA: 0s - loss: 1.8910 - acc: 0.321 - ETA: 0s - loss: 1.8847 - acc: 0.322 - ETA: 0s - loss: 1.8853 - acc: 0.320 - ETA: 0s - loss: 1.8873 - acc: 0.319 - ETA: 0s - loss: 1.8895 - acc: 0.316 - ETA: 0s - loss: 1.8880 - acc: 0.317 - 1s 204us/step - loss: 1.8875 - acc: 0.3181\n",
      "Epoch 36/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 2.0303 - acc: 0.406 - ETA: 1s - loss: 1.8146 - acc: 0.364 - ETA: 0s - loss: 1.8345 - acc: 0.354 - ETA: 0s - loss: 1.8569 - acc: 0.353 - ETA: 0s - loss: 1.8684 - acc: 0.345 - ETA: 0s - loss: 1.8715 - acc: 0.339 - ETA: 0s - loss: 1.8694 - acc: 0.333 - ETA: 0s - loss: 1.8759 - acc: 0.331 - ETA: 0s - loss: 1.8788 - acc: 0.325 - ETA: 0s - loss: 1.8925 - acc: 0.319 - ETA: 0s - loss: 1.8915 - acc: 0.320 - ETA: 0s - loss: 1.8940 - acc: 0.320 - ETA: 0s - loss: 1.8936 - acc: 0.320 - ETA: 0s - loss: 1.8932 - acc: 0.321 - ETA: 0s - loss: 1.8937 - acc: 0.319 - ETA: 0s - loss: 1.8952 - acc: 0.320 - ETA: 0s - loss: 1.8921 - acc: 0.321 - ETA: 0s - loss: 1.8884 - acc: 0.322 - ETA: 0s - loss: 1.8861 - acc: 0.322 - ETA: 0s - loss: 1.8830 - acc: 0.324 - 1s 198us/step - loss: 1.8874 - acc: 0.3242\n",
      "Epoch 37/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.9136 - acc: 0.343 - ETA: 0s - loss: 1.8655 - acc: 0.335 - ETA: 0s - loss: 1.8587 - acc: 0.340 - ETA: 0s - loss: 1.8662 - acc: 0.333 - ETA: 0s - loss: 1.8848 - acc: 0.326 - ETA: 0s - loss: 1.8886 - acc: 0.324 - ETA: 0s - loss: 1.8846 - acc: 0.323 - ETA: 0s - loss: 1.8769 - acc: 0.324 - ETA: 0s - loss: 1.8826 - acc: 0.321 - ETA: 0s - loss: 1.8765 - acc: 0.323 - ETA: 0s - loss: 1.8758 - acc: 0.324 - ETA: 0s - loss: 1.8667 - acc: 0.326 - ETA: 0s - loss: 1.8751 - acc: 0.321 - ETA: 0s - loss: 1.8748 - acc: 0.322 - ETA: 0s - loss: 1.8774 - acc: 0.320 - ETA: 0s - loss: 1.8764 - acc: 0.320 - ETA: 0s - loss: 1.8827 - acc: 0.317 - ETA: 0s - loss: 1.8835 - acc: 0.318 - ETA: 0s - loss: 1.8847 - acc: 0.318 - ETA: 0s - loss: 1.8846 - acc: 0.317 - 1s 189us/step - loss: 1.8831 - acc: 0.3179\n",
      "Epoch 38/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 2.0343 - acc: 0.281 - ETA: 0s - loss: 1.9073 - acc: 0.318 - ETA: 0s - loss: 1.9261 - acc: 0.301 - ETA: 0s - loss: 1.9125 - acc: 0.303 - ETA: 0s - loss: 1.8995 - acc: 0.308 - ETA: 0s - loss: 1.8792 - acc: 0.315 - ETA: 0s - loss: 1.8676 - acc: 0.321 - ETA: 0s - loss: 1.8666 - acc: 0.318 - ETA: 0s - loss: 1.8665 - acc: 0.318 - ETA: 0s - loss: 1.8756 - acc: 0.318 - ETA: 0s - loss: 1.8801 - acc: 0.317 - ETA: 0s - loss: 1.8802 - acc: 0.317 - ETA: 0s - loss: 1.8691 - acc: 0.322 - ETA: 0s - loss: 1.8663 - acc: 0.320 - ETA: 0s - loss: 1.8685 - acc: 0.321 - ETA: 0s - loss: 1.8674 - acc: 0.324 - ETA: 0s - loss: 1.8685 - acc: 0.324 - ETA: 0s - loss: 1.8681 - acc: 0.324 - ETA: 0s - loss: 1.8748 - acc: 0.323 - ETA: 0s - loss: 1.8799 - acc: 0.322 - 1s 188us/step - loss: 1.8812 - acc: 0.3222\n",
      "Epoch 39/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.8712 - acc: 0.312 - ETA: 1s - loss: 1.8449 - acc: 0.350 - ETA: 0s - loss: 1.8402 - acc: 0.350 - ETA: 0s - loss: 1.8566 - acc: 0.341 - ETA: 0s - loss: 1.8758 - acc: 0.330 - ETA: 0s - loss: 1.8787 - acc: 0.328 - ETA: 0s - loss: 1.8749 - acc: 0.328 - ETA: 0s - loss: 1.8891 - acc: 0.322 - ETA: 0s - loss: 1.8954 - acc: 0.320 - ETA: 0s - loss: 1.8840 - acc: 0.325 - ETA: 0s - loss: 1.8841 - acc: 0.324 - ETA: 0s - loss: 1.8789 - acc: 0.325 - ETA: 0s - loss: 1.8777 - acc: 0.326 - ETA: 0s - loss: 1.8830 - acc: 0.325 - ETA: 0s - loss: 1.8841 - acc: 0.324 - ETA: 0s - loss: 1.8886 - acc: 0.323 - ETA: 0s - loss: 1.8835 - acc: 0.325 - ETA: 0s - loss: 1.8818 - acc: 0.326 - ETA: 0s - loss: 1.8798 - acc: 0.328 - ETA: 0s - loss: 1.8781 - acc: 0.328 - ETA: 0s - loss: 1.8760 - acc: 0.328 - 1s 200us/step - loss: 1.8767 - acc: 0.3282\n",
      "Epoch 40/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7087 - acc: 0.343 - ETA: 1s - loss: 1.8626 - acc: 0.350 - ETA: 0s - loss: 1.8452 - acc: 0.350 - ETA: 0s - loss: 1.8363 - acc: 0.346 - ETA: 0s - loss: 1.8439 - acc: 0.330 - ETA: 0s - loss: 1.8468 - acc: 0.329 - ETA: 0s - loss: 1.8434 - acc: 0.334 - ETA: 0s - loss: 1.8428 - acc: 0.334 - ETA: 0s - loss: 1.8496 - acc: 0.336 - ETA: 0s - loss: 1.8496 - acc: 0.335 - ETA: 0s - loss: 1.8510 - acc: 0.335 - ETA: 0s - loss: 1.8576 - acc: 0.333 - ETA: 0s - loss: 1.8623 - acc: 0.331 - ETA: 0s - loss: 1.8650 - acc: 0.328 - ETA: 0s - loss: 1.8657 - acc: 0.327 - ETA: 0s - loss: 1.8664 - acc: 0.327 - ETA: 0s - loss: 1.8650 - acc: 0.325 - ETA: 0s - loss: 1.8682 - acc: 0.325 - ETA: 0s - loss: 1.8651 - acc: 0.327 - ETA: 0s - loss: 1.8628 - acc: 0.327 - ETA: 0s - loss: 1.8631 - acc: 0.328 - 1s 199us/step - loss: 1.8635 - acc: 0.3282\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8422 - acc: 0.281 - ETA: 0s - loss: 1.8228 - acc: 0.362 - ETA: 0s - loss: 1.8454 - acc: 0.354 - ETA: 0s - loss: 1.8688 - acc: 0.336 - ETA: 0s - loss: 1.8839 - acc: 0.329 - ETA: 0s - loss: 1.8864 - acc: 0.325 - ETA: 0s - loss: 1.8852 - acc: 0.324 - ETA: 0s - loss: 1.8808 - acc: 0.324 - ETA: 0s - loss: 1.8802 - acc: 0.328 - ETA: 0s - loss: 1.8805 - acc: 0.320 - ETA: 0s - loss: 1.8798 - acc: 0.323 - ETA: 0s - loss: 1.8796 - acc: 0.322 - ETA: 0s - loss: 1.8803 - acc: 0.321 - ETA: 0s - loss: 1.8782 - acc: 0.322 - ETA: 0s - loss: 1.8795 - acc: 0.320 - ETA: 0s - loss: 1.8765 - acc: 0.321 - ETA: 0s - loss: 1.8770 - acc: 0.320 - ETA: 0s - loss: 1.8745 - acc: 0.320 - ETA: 0s - loss: 1.8733 - acc: 0.320 - ETA: 0s - loss: 1.8699 - acc: 0.322 - ETA: 0s - loss: 1.8676 - acc: 0.324 - ETA: 0s - loss: 1.8672 - acc: 0.324 - 1s 205us/step - loss: 1.8654 - acc: 0.3257\n",
      "Epoch 42/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6632 - acc: 0.437 - ETA: 1s - loss: 1.8475 - acc: 0.336 - ETA: 0s - loss: 1.8578 - acc: 0.312 - ETA: 0s - loss: 1.8601 - acc: 0.312 - ETA: 0s - loss: 1.8574 - acc: 0.319 - ETA: 0s - loss: 1.8756 - acc: 0.315 - ETA: 0s - loss: 1.8706 - acc: 0.315 - ETA: 0s - loss: 1.8730 - acc: 0.314 - ETA: 0s - loss: 1.8602 - acc: 0.320 - ETA: 0s - loss: 1.8685 - acc: 0.315 - ETA: 0s - loss: 1.8611 - acc: 0.317 - ETA: 0s - loss: 1.8614 - acc: 0.315 - ETA: 0s - loss: 1.8612 - acc: 0.317 - ETA: 0s - loss: 1.8644 - acc: 0.319 - ETA: 0s - loss: 1.8685 - acc: 0.317 - ETA: 0s - loss: 1.8633 - acc: 0.323 - ETA: 0s - loss: 1.8672 - acc: 0.320 - ETA: 0s - loss: 1.8638 - acc: 0.322 - ETA: 0s - loss: 1.8672 - acc: 0.321 - 1s 182us/step - loss: 1.8650 - acc: 0.3240\n",
      "Epoch 43/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 2.0132 - acc: 0.250 - ETA: 0s - loss: 1.8173 - acc: 0.340 - ETA: 0s - loss: 1.8295 - acc: 0.318 - ETA: 0s - loss: 1.8481 - acc: 0.320 - ETA: 0s - loss: 1.8581 - acc: 0.318 - ETA: 0s - loss: 1.8702 - acc: 0.313 - ETA: 0s - loss: 1.8510 - acc: 0.324 - ETA: 0s - loss: 1.8562 - acc: 0.321 - ETA: 0s - loss: 1.8568 - acc: 0.321 - ETA: 0s - loss: 1.8616 - acc: 0.320 - ETA: 0s - loss: 1.8511 - acc: 0.325 - ETA: 0s - loss: 1.8543 - acc: 0.326 - ETA: 0s - loss: 1.8537 - acc: 0.326 - ETA: 0s - loss: 1.8569 - acc: 0.325 - ETA: 0s - loss: 1.8674 - acc: 0.324 - ETA: 0s - loss: 1.8647 - acc: 0.324 - ETA: 0s - loss: 1.8701 - acc: 0.320 - ETA: 0s - loss: 1.8678 - acc: 0.322 - ETA: 0s - loss: 1.8677 - acc: 0.325 - 1s 184us/step - loss: 1.8702 - acc: 0.3235\n",
      "Epoch 44/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8608 - acc: 0.312 - ETA: 0s - loss: 1.8594 - acc: 0.321 - ETA: 0s - loss: 1.8686 - acc: 0.346 - ETA: 0s - loss: 1.8732 - acc: 0.336 - ETA: 0s - loss: 1.8590 - acc: 0.341 - ETA: 0s - loss: 1.8436 - acc: 0.346 - ETA: 0s - loss: 1.8489 - acc: 0.340 - ETA: 0s - loss: 1.8518 - acc: 0.337 - ETA: 0s - loss: 1.8530 - acc: 0.343 - ETA: 0s - loss: 1.8437 - acc: 0.343 - ETA: 0s - loss: 1.8471 - acc: 0.340 - ETA: 0s - loss: 1.8543 - acc: 0.335 - ETA: 0s - loss: 1.8570 - acc: 0.335 - ETA: 0s - loss: 1.8589 - acc: 0.332 - ETA: 0s - loss: 1.8605 - acc: 0.330 - ETA: 0s - loss: 1.8625 - acc: 0.331 - ETA: 0s - loss: 1.8633 - acc: 0.330 - ETA: 0s - loss: 1.8642 - acc: 0.328 - ETA: 0s - loss: 1.8622 - acc: 0.330 - ETA: 0s - loss: 1.8635 - acc: 0.329 - 1s 188us/step - loss: 1.8624 - acc: 0.3290\n",
      "Epoch 45/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7871 - acc: 0.312 - ETA: 1s - loss: 1.8267 - acc: 0.326 - ETA: 1s - loss: 1.8154 - acc: 0.364 - ETA: 1s - loss: 1.8088 - acc: 0.365 - ETA: 0s - loss: 1.8179 - acc: 0.359 - ETA: 0s - loss: 1.8210 - acc: 0.352 - ETA: 0s - loss: 1.8256 - acc: 0.351 - ETA: 0s - loss: 1.8183 - acc: 0.350 - ETA: 0s - loss: 1.8235 - acc: 0.347 - ETA: 0s - loss: 1.8303 - acc: 0.343 - ETA: 0s - loss: 1.8316 - acc: 0.339 - ETA: 0s - loss: 1.8405 - acc: 0.333 - ETA: 0s - loss: 1.8433 - acc: 0.330 - ETA: 0s - loss: 1.8529 - acc: 0.324 - ETA: 0s - loss: 1.8539 - acc: 0.324 - ETA: 0s - loss: 1.8506 - acc: 0.327 - ETA: 0s - loss: 1.8533 - acc: 0.325 - ETA: 0s - loss: 1.8542 - acc: 0.325 - ETA: 0s - loss: 1.8560 - acc: 0.324 - ETA: 0s - loss: 1.8579 - acc: 0.324 - 1s 190us/step - loss: 1.8575 - acc: 0.3236\n",
      "Epoch 46/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8907 - acc: 0.250 - ETA: 0s - loss: 1.9139 - acc: 0.309 - ETA: 0s - loss: 1.9036 - acc: 0.318 - ETA: 0s - loss: 1.9099 - acc: 0.301 - ETA: 0s - loss: 1.9136 - acc: 0.295 - ETA: 0s - loss: 1.8976 - acc: 0.304 - ETA: 0s - loss: 1.8977 - acc: 0.299 - ETA: 0s - loss: 1.8900 - acc: 0.307 - ETA: 0s - loss: 1.8805 - acc: 0.315 - ETA: 0s - loss: 1.8770 - acc: 0.316 - ETA: 0s - loss: 1.8741 - acc: 0.318 - ETA: 0s - loss: 1.8750 - acc: 0.316 - ETA: 0s - loss: 1.8676 - acc: 0.320 - ETA: 0s - loss: 1.8718 - acc: 0.317 - ETA: 0s - loss: 1.8639 - acc: 0.322 - ETA: 0s - loss: 1.8613 - acc: 0.324 - ETA: 0s - loss: 1.8662 - acc: 0.322 - ETA: 0s - loss: 1.8706 - acc: 0.321 - ETA: 0s - loss: 1.8695 - acc: 0.323 - 1s 177us/step - loss: 1.8688 - acc: 0.3235\n",
      "Epoch 47/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7124 - acc: 0.375 - ETA: 0s - loss: 1.7913 - acc: 0.380 - ETA: 0s - loss: 1.8327 - acc: 0.360 - ETA: 0s - loss: 1.8496 - acc: 0.345 - ETA: 0s - loss: 1.8431 - acc: 0.343 - ETA: 0s - loss: 1.8471 - acc: 0.339 - ETA: 0s - loss: 1.8539 - acc: 0.330 - ETA: 0s - loss: 1.8513 - acc: 0.331 - ETA: 0s - loss: 1.8515 - acc: 0.334 - ETA: 0s - loss: 1.8493 - acc: 0.336 - ETA: 0s - loss: 1.8510 - acc: 0.335 - ETA: 0s - loss: 1.8539 - acc: 0.332 - ETA: 0s - loss: 1.8545 - acc: 0.332 - ETA: 0s - loss: 1.8550 - acc: 0.333 - ETA: 0s - loss: 1.8557 - acc: 0.331 - ETA: 0s - loss: 1.8528 - acc: 0.334 - ETA: 0s - loss: 1.8524 - acc: 0.335 - ETA: 0s - loss: 1.8528 - acc: 0.335 - ETA: 0s - loss: 1.8535 - acc: 0.333 - ETA: 0s - loss: 1.8522 - acc: 0.333 - 1s 188us/step - loss: 1.8520 - acc: 0.3338\n",
      "Epoch 48/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7097 - acc: 0.437 - ETA: 0s - loss: 1.7639 - acc: 0.387 - ETA: 0s - loss: 1.7910 - acc: 0.376 - ETA: 0s - loss: 1.8075 - acc: 0.378 - ETA: 0s - loss: 1.8495 - acc: 0.353 - ETA: 0s - loss: 1.8479 - acc: 0.356 - ETA: 0s - loss: 1.8442 - acc: 0.355 - ETA: 0s - loss: 1.8404 - acc: 0.351 - ETA: 0s - loss: 1.8399 - acc: 0.351 - ETA: 0s - loss: 1.8518 - acc: 0.346 - ETA: 0s - loss: 1.8468 - acc: 0.345 - ETA: 0s - loss: 1.8481 - acc: 0.342 - ETA: 0s - loss: 1.8494 - acc: 0.340 - ETA: 0s - loss: 1.8504 - acc: 0.339 - ETA: 0s - loss: 1.8511 - acc: 0.337 - ETA: 0s - loss: 1.8516 - acc: 0.334 - ETA: 0s - loss: 1.8543 - acc: 0.333 - ETA: 0s - loss: 1.8525 - acc: 0.335 - ETA: 0s - loss: 1.8532 - acc: 0.334 - 1s 181us/step - loss: 1.8532 - acc: 0.3338\n",
      "Epoch 49/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0891 - acc: 0.281 - ETA: 1s - loss: 1.8146 - acc: 0.352 - ETA: 1s - loss: 1.8369 - acc: 0.334 - ETA: 1s - loss: 1.8582 - acc: 0.333 - ETA: 1s - loss: 1.8676 - acc: 0.324 - ETA: 0s - loss: 1.8604 - acc: 0.320 - ETA: 0s - loss: 1.8529 - acc: 0.322 - ETA: 0s - loss: 1.8596 - acc: 0.323 - ETA: 0s - loss: 1.8446 - acc: 0.331 - ETA: 0s - loss: 1.8434 - acc: 0.333 - ETA: 0s - loss: 1.8505 - acc: 0.331 - ETA: 0s - loss: 1.8412 - acc: 0.334 - ETA: 0s - loss: 1.8487 - acc: 0.333 - ETA: 0s - loss: 1.8402 - acc: 0.336 - ETA: 0s - loss: 1.8416 - acc: 0.334 - ETA: 0s - loss: 1.8394 - acc: 0.334 - ETA: 0s - loss: 1.8435 - acc: 0.331 - ETA: 0s - loss: 1.8417 - acc: 0.331 - ETA: 0s - loss: 1.8430 - acc: 0.331 - ETA: 0s - loss: 1.8417 - acc: 0.330 - 1s 190us/step - loss: 1.8460 - acc: 0.3292\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8123 - acc: 0.343 - ETA: 0s - loss: 1.8603 - acc: 0.346 - ETA: 0s - loss: 1.8534 - acc: 0.335 - ETA: 0s - loss: 1.8121 - acc: 0.354 - ETA: 0s - loss: 1.8084 - acc: 0.359 - ETA: 0s - loss: 1.8084 - acc: 0.354 - ETA: 0s - loss: 1.8164 - acc: 0.352 - ETA: 0s - loss: 1.8189 - acc: 0.347 - ETA: 0s - loss: 1.8228 - acc: 0.349 - ETA: 0s - loss: 1.8186 - acc: 0.351 - ETA: 0s - loss: 1.8295 - acc: 0.347 - ETA: 0s - loss: 1.8392 - acc: 0.343 - ETA: 0s - loss: 1.8391 - acc: 0.344 - ETA: 0s - loss: 1.8440 - acc: 0.339 - ETA: 0s - loss: 1.8472 - acc: 0.338 - ETA: 0s - loss: 1.8457 - acc: 0.337 - ETA: 0s - loss: 1.8473 - acc: 0.339 - ETA: 0s - loss: 1.8502 - acc: 0.338 - ETA: 0s - loss: 1.8510 - acc: 0.338 - 1s 176us/step - loss: 1.8508 - acc: 0.3384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dad082dd68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parser(row):\n",
    "    global test_error_count\n",
    "    global test_error_labels\n",
    "    path_to_wav_files = PATH_TO_TEST_AUDIO_FILES\n",
    "    file_path = path_to_wav_files + str(row.ID) + \".wav\"\n",
    "    try:\n",
    "        data, sampling_rate = librosa.load(file_path)\n",
    "        stft = np.abs(librosa.stft(data))\n",
    "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(data),sr=sampling_rate).T,axis=0)\n",
    "    except Exception as ex:\n",
    "        test_error_count += 1\n",
    "        test_error_labels.append(row.ID)\n",
    "        return pd.Series([0]*6)\n",
    "    features = tonnetz\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c808f9727c43a392e2695af49483a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3297), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\librosa\\core\\pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 samples had errors while parsing\n",
      "Errorneous samples []\n"
     ]
    }
   ],
   "source": [
    "test_features = test.progress_apply(test_parser,axis=1, reduce = True)\n",
    "print(\"%d samples had errors while parsing\" % test_error_count)\n",
    "print(\"Errorneous samples\", test_error_labels)\n",
    "save_as_pickle(data=train_features,pickle_file=PATH_TO_PICKLE + SUBMISSION_TITLE + \" test.pickle\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_features\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "test_labels_strings = lb.inverse_transform(test_labels.argmax(axis=1))\n",
    "# test_labels_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Class'] = test_labels_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(PATH_TO_SUBMISSION + SUBMISSION_TITLE + \".csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gives 56% accuracy with the above setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
