{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set up code to visualize a sound form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "from librosa import load, display\n",
    "import glob\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "import pickle\n",
    "from common import save_as_pickle\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should change these paths according to the path of the files on your system.\n",
    "PATH_TO_TRAIN_LABELS = \"data/train/train.csv\"\n",
    "PATH_TO_TEST_LABELS = \"data/test/test.csv\"\n",
    "PATH_TO_TRAIN_AUDIO_FILES = \"data/train/wav/\"\n",
    "PATH_TO_TEST_AUDIO_FILES = \"data/test/wav/\"\n",
    "PATH_TO_SUBMISSION = \"submission/\"\n",
    "PATH_TO_PICKLE = \"pickles/\"\n",
    "SUBMISSION_TITLE = \"nn contrast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is easier to deal with csv if you can load it into a structure you can work with.\n",
    "# Pandas are the most convenient way to do that and are available with \n",
    "# inbuilt functionality to handle csv file.\n",
    "\n",
    "# Pandas assumes that the first row in your file is the header adn not the actual values.\n",
    "# This behavior can be overriden by passing header=None as a parameter.\n",
    "train = pd.read_csv(PATH_TO_TRAIN_LABELS)\n",
    "test = pd.read_csv(PATH_TO_TEST_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can reactivate this cell to make sure your model is working correctly in terms of dimensions.\n",
    "#train = train[:2]\n",
    "#test = test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_count = 0\n",
    "train_error_labels = []\n",
    "test_error_count = 0\n",
    "test_error_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start with classification, we first need to convert the wav sound files into a format we can work \n",
    "# with. It is easier to take the amplitude at each sampling point and use that \n",
    "# numeric value to form a feature vector.\n",
    "def train_parser(row):\n",
    "    global train_error_count\n",
    "    global train_error_labels\n",
    "    path_to_wav_files = PATH_TO_TRAIN_AUDIO_FILES\n",
    "    file_path = path_to_wav_files + str(row.ID) + \".wav\"\n",
    "    try:\n",
    "        data, sampling_rate = librosa.load(file_path)\n",
    "        stft = np.abs(librosa.stft(data))\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sampling_rate).T,axis=0)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        train_error_count += 1\n",
    "        train_error_labels.append(row.ID)\n",
    "        return [0]*7, row.Class\n",
    "    features = contrast\n",
    "    label = row.Class\n",
    "    return [features, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81e7c702d9c4f43a6bad7e52cb271ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5435), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 samples had errors while parsing\n",
      "Errorneous samples []\n"
     ]
    }
   ],
   "source": [
    "# To create the training feature matrix, we can apply our parser to each training sample.\n",
    "train_features = train.progress_apply(train_parser,axis=1)\n",
    "print(\"%d samples had errors while parsing\" % train_error_count)\n",
    "print(\"Errorneous samples\", train_error_labels)\n",
    "save_as_pickle(data=train_features,pickle_file=PATH_TO_PICKLE + SUBMISSION_TITLE + \" train.pickle\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to singnify what they mean helps with documentation,\n",
    "# and also helps you keep track of them later on.\n",
    "train_features.columns = ['feature','label']\n",
    "# train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# this library helps us convert string labels into easy to handle encoded labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_features.feature.tolist())\n",
    "Y = np.array(train_features.label.tolist())\n",
    "lb = LabelEncoder()\n",
    "# Since labels are categories they dont inherently have an order amongst themselves.\n",
    "# For example, Apples > oranges does not make any sense. So to madel such categorical \n",
    "# variables, we can convert them to one hot vectors.\n",
    "Y = to_categorical(lb.fit_transform(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_labels = Y.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, input_shape=(7,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(number_of_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 70,410\n",
      "Trainable params: 70,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5435/5435 [==============================] - ETA: 1:35 - loss: 8.9289 - acc: 0.0000e+0 - ETA: 12s - loss: 6.9537 - acc: 0.1367    - ETA: 6s - loss: 6.6928 - acc: 0.1208 - ETA: 4s - loss: 6.1828 - acc: 0.131 - ETA: 3s - loss: 5.6485 - acc: 0.127 - ETA: 2s - loss: 5.1909 - acc: 0.127 - ETA: 2s - loss: 4.9400 - acc: 0.127 - ETA: 2s - loss: 4.6570 - acc: 0.129 - ETA: 1s - loss: 4.3951 - acc: 0.129 - ETA: 1s - loss: 4.1674 - acc: 0.128 - ETA: 1s - loss: 4.0465 - acc: 0.128 - ETA: 1s - loss: 3.9030 - acc: 0.130 - ETA: 1s - loss: 3.7685 - acc: 0.132 - ETA: 1s - loss: 3.6630 - acc: 0.134 - ETA: 0s - loss: 3.5555 - acc: 0.133 - ETA: 0s - loss: 3.4597 - acc: 0.137 - ETA: 0s - loss: 3.3721 - acc: 0.140 - ETA: 0s - loss: 3.3655 - acc: 0.140 - ETA: 0s - loss: 3.2923 - acc: 0.145 - ETA: 0s - loss: 3.2378 - acc: 0.146 - ETA: 0s - loss: 3.1838 - acc: 0.148 - ETA: 0s - loss: 3.1337 - acc: 0.150 - ETA: 0s - loss: 3.0999 - acc: 0.152 - ETA: 0s - loss: 3.0662 - acc: 0.154 - 2s 373us/step - loss: 3.0399 - acc: 0.1566\n",
      "Epoch 2/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 2.1474 - acc: 0.125 - ETA: 1s - loss: 2.2211 - acc: 0.187 - ETA: 1s - loss: 2.1979 - acc: 0.187 - ETA: 1s - loss: 2.1753 - acc: 0.186 - ETA: 1s - loss: 2.1815 - acc: 0.183 - ETA: 0s - loss: 2.1768 - acc: 0.188 - ETA: 0s - loss: 2.1791 - acc: 0.186 - ETA: 0s - loss: 2.1711 - acc: 0.192 - ETA: 0s - loss: 2.1603 - acc: 0.194 - ETA: 0s - loss: 2.1566 - acc: 0.201 - ETA: 0s - loss: 2.1565 - acc: 0.203 - ETA: 0s - loss: 2.1451 - acc: 0.208 - ETA: 0s - loss: 2.1368 - acc: 0.210 - ETA: 0s - loss: 2.1398 - acc: 0.208 - ETA: 0s - loss: 2.1363 - acc: 0.211 - ETA: 0s - loss: 2.1309 - acc: 0.212 - ETA: 0s - loss: 2.1283 - acc: 0.211 - ETA: 0s - loss: 2.1223 - acc: 0.212 - ETA: 0s - loss: 2.1195 - acc: 0.213 - ETA: 0s - loss: 2.1171 - acc: 0.215 - ETA: 0s - loss: 2.1173 - acc: 0.213 - ETA: 0s - loss: 2.1121 - acc: 0.216 - ETA: 0s - loss: 2.1074 - acc: 0.217 - 1s 244us/step - loss: 2.1061 - acc: 0.2182\n",
      "Epoch 3/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1245 - acc: 0.218 - ETA: 1s - loss: 2.0158 - acc: 0.237 - ETA: 1s - loss: 1.9994 - acc: 0.246 - ETA: 1s - loss: 2.0186 - acc: 0.233 - ETA: 1s - loss: 2.0339 - acc: 0.234 - ETA: 1s - loss: 2.0404 - acc: 0.236 - ETA: 0s - loss: 2.0422 - acc: 0.238 - ETA: 0s - loss: 2.0227 - acc: 0.247 - ETA: 0s - loss: 2.0148 - acc: 0.252 - ETA: 0s - loss: 2.0189 - acc: 0.247 - ETA: 0s - loss: 2.0209 - acc: 0.248 - ETA: 0s - loss: 2.0125 - acc: 0.250 - ETA: 0s - loss: 2.0125 - acc: 0.250 - ETA: 0s - loss: 2.0125 - acc: 0.250 - ETA: 0s - loss: 2.0123 - acc: 0.253 - ETA: 0s - loss: 2.0113 - acc: 0.254 - ETA: 0s - loss: 2.0110 - acc: 0.255 - ETA: 0s - loss: 2.0063 - acc: 0.258 - ETA: 0s - loss: 2.0086 - acc: 0.256 - ETA: 0s - loss: 2.0094 - acc: 0.258 - ETA: 0s - loss: 2.0063 - acc: 0.259 - ETA: 0s - loss: 2.0011 - acc: 0.262 - ETA: 0s - loss: 2.0000 - acc: 0.262 - 1s 243us/step - loss: 1.9960 - acc: 0.2631\n",
      "Epoch 4/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.8744 - acc: 0.281 - ETA: 1s - loss: 1.9248 - acc: 0.271 - ETA: 1s - loss: 1.9341 - acc: 0.259 - ETA: 1s - loss: 1.9452 - acc: 0.262 - ETA: 1s - loss: 1.9647 - acc: 0.255 - ETA: 1s - loss: 1.9523 - acc: 0.262 - ETA: 0s - loss: 1.9355 - acc: 0.271 - ETA: 0s - loss: 1.9318 - acc: 0.275 - ETA: 0s - loss: 1.9391 - acc: 0.275 - ETA: 0s - loss: 1.9480 - acc: 0.275 - ETA: 0s - loss: 1.9499 - acc: 0.274 - ETA: 0s - loss: 1.9477 - acc: 0.280 - ETA: 0s - loss: 1.9529 - acc: 0.278 - ETA: 0s - loss: 1.9447 - acc: 0.284 - ETA: 0s - loss: 1.9424 - acc: 0.286 - ETA: 0s - loss: 1.9358 - acc: 0.288 - ETA: 0s - loss: 1.9354 - acc: 0.288 - ETA: 0s - loss: 1.9371 - acc: 0.287 - ETA: 0s - loss: 1.9441 - acc: 0.284 - ETA: 0s - loss: 1.9379 - acc: 0.286 - ETA: 0s - loss: 1.9354 - acc: 0.288 - ETA: 0s - loss: 1.9366 - acc: 0.289 - ETA: 0s - loss: 1.9378 - acc: 0.289 - ETA: 0s - loss: 1.9358 - acc: 0.289 - 1s 250us/step - loss: 1.9344 - acc: 0.2894\n",
      "Epoch 5/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9395 - acc: 0.218 - ETA: 1s - loss: 1.8864 - acc: 0.303 - ETA: 1s - loss: 1.9576 - acc: 0.291 - ETA: 1s - loss: 1.9431 - acc: 0.289 - ETA: 0s - loss: 1.9362 - acc: 0.289 - ETA: 0s - loss: 1.9368 - acc: 0.282 - ETA: 0s - loss: 1.9328 - acc: 0.288 - ETA: 0s - loss: 1.9159 - acc: 0.299 - ETA: 0s - loss: 1.9178 - acc: 0.297 - ETA: 0s - loss: 1.9098 - acc: 0.299 - ETA: 0s - loss: 1.9051 - acc: 0.304 - ETA: 0s - loss: 1.8972 - acc: 0.302 - ETA: 0s - loss: 1.9045 - acc: 0.299 - ETA: 0s - loss: 1.9004 - acc: 0.299 - ETA: 0s - loss: 1.8988 - acc: 0.300 - ETA: 0s - loss: 1.8946 - acc: 0.302 - ETA: 0s - loss: 1.8966 - acc: 0.301 - ETA: 0s - loss: 1.8962 - acc: 0.298 - ETA: 0s - loss: 1.8978 - acc: 0.297 - ETA: 0s - loss: 1.8973 - acc: 0.298 - ETA: 0s - loss: 1.9004 - acc: 0.297 - ETA: 0s - loss: 1.9019 - acc: 0.298 - ETA: 0s - loss: 1.8979 - acc: 0.299 - 1s 241us/step - loss: 1.8972 - acc: 0.2992\n",
      "Epoch 6/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9895 - acc: 0.250 - ETA: 1s - loss: 1.9566 - acc: 0.281 - ETA: 1s - loss: 1.8969 - acc: 0.304 - ETA: 1s - loss: 1.8989 - acc: 0.313 - ETA: 1s - loss: 1.9006 - acc: 0.310 - ETA: 1s - loss: 1.9127 - acc: 0.305 - ETA: 0s - loss: 1.8970 - acc: 0.305 - ETA: 0s - loss: 1.9078 - acc: 0.298 - ETA: 0s - loss: 1.9002 - acc: 0.304 - ETA: 0s - loss: 1.8898 - acc: 0.305 - ETA: 0s - loss: 1.8878 - acc: 0.308 - ETA: 0s - loss: 1.8885 - acc: 0.309 - ETA: 0s - loss: 1.8821 - acc: 0.310 - ETA: 0s - loss: 1.8873 - acc: 0.307 - ETA: 0s - loss: 1.8889 - acc: 0.309 - ETA: 0s - loss: 1.8890 - acc: 0.306 - ETA: 0s - loss: 1.8907 - acc: 0.308 - ETA: 0s - loss: 1.8840 - acc: 0.312 - ETA: 0s - loss: 1.8831 - acc: 0.313 - ETA: 0s - loss: 1.8815 - acc: 0.313 - ETA: 0s - loss: 1.8807 - acc: 0.313 - ETA: 0s - loss: 1.8783 - acc: 0.314 - 1s 240us/step - loss: 1.8770 - acc: 0.3152\n",
      "Epoch 7/50\n",
      "5435/5435 [==============================] - ETA: 3s - loss: 1.8127 - acc: 0.312 - ETA: 1s - loss: 1.8242 - acc: 0.340 - ETA: 1s - loss: 1.8357 - acc: 0.318 - ETA: 1s - loss: 1.8220 - acc: 0.309 - ETA: 1s - loss: 1.8382 - acc: 0.310 - ETA: 1s - loss: 1.8500 - acc: 0.298 - ETA: 1s - loss: 1.8560 - acc: 0.306 - ETA: 0s - loss: 1.8407 - acc: 0.308 - ETA: 0s - loss: 1.8424 - acc: 0.314 - ETA: 0s - loss: 1.8381 - acc: 0.313 - ETA: 0s - loss: 1.8375 - acc: 0.313 - ETA: 0s - loss: 1.8284 - acc: 0.318 - ETA: 0s - loss: 1.8357 - acc: 0.312 - ETA: 0s - loss: 1.8411 - acc: 0.311 - ETA: 0s - loss: 1.8343 - acc: 0.311 - ETA: 0s - loss: 1.8385 - acc: 0.311 - ETA: 0s - loss: 1.8395 - acc: 0.312 - ETA: 0s - loss: 1.8445 - acc: 0.311 - ETA: 0s - loss: 1.8479 - acc: 0.311 - ETA: 0s - loss: 1.8422 - acc: 0.314 - ETA: 0s - loss: 1.8432 - acc: 0.313 - ETA: 0s - loss: 1.8400 - acc: 0.315 - ETA: 0s - loss: 1.8452 - acc: 0.314 - ETA: 0s - loss: 1.8459 - acc: 0.312 - 1s 249us/step - loss: 1.8430 - acc: 0.3146\n",
      "Epoch 8/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.9676 - acc: 0.343 - ETA: 1s - loss: 1.8322 - acc: 0.347 - ETA: 1s - loss: 1.8509 - acc: 0.332 - ETA: 1s - loss: 1.8225 - acc: 0.345 - ETA: 1s - loss: 1.8212 - acc: 0.342 - ETA: 1s - loss: 1.8315 - acc: 0.344 - ETA: 0s - loss: 1.8211 - acc: 0.345 - ETA: 0s - loss: 1.8180 - acc: 0.349 - ETA: 0s - loss: 1.8238 - acc: 0.346 - ETA: 0s - loss: 1.8231 - acc: 0.345 - ETA: 0s - loss: 1.8284 - acc: 0.336 - ETA: 0s - loss: 1.8305 - acc: 0.332 - ETA: 0s - loss: 1.8288 - acc: 0.333 - ETA: 0s - loss: 1.8311 - acc: 0.332 - ETA: 0s - loss: 1.8395 - acc: 0.330 - ETA: 0s - loss: 1.8351 - acc: 0.331 - ETA: 0s - loss: 1.8330 - acc: 0.331 - ETA: 0s - loss: 1.8365 - acc: 0.327 - ETA: 0s - loss: 1.8373 - acc: 0.327 - ETA: 0s - loss: 1.8344 - acc: 0.328 - ETA: 0s - loss: 1.8344 - acc: 0.327 - ETA: 0s - loss: 1.8320 - acc: 0.327 - ETA: 0s - loss: 1.8341 - acc: 0.326 - ETA: 0s - loss: 1.8321 - acc: 0.327 - 1s 250us/step - loss: 1.8339 - acc: 0.3281\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0541 - acc: 0.312 - ETA: 1s - loss: 1.8840 - acc: 0.363 - ETA: 1s - loss: 1.7707 - acc: 0.391 - ETA: 1s - loss: 1.7500 - acc: 0.383 - ETA: 1s - loss: 1.7452 - acc: 0.379 - ETA: 0s - loss: 1.7664 - acc: 0.370 - ETA: 0s - loss: 1.7826 - acc: 0.359 - ETA: 0s - loss: 1.7877 - acc: 0.351 - ETA: 0s - loss: 1.7918 - acc: 0.347 - ETA: 0s - loss: 1.8010 - acc: 0.342 - ETA: 0s - loss: 1.8013 - acc: 0.342 - ETA: 0s - loss: 1.8063 - acc: 0.339 - ETA: 0s - loss: 1.8126 - acc: 0.337 - ETA: 0s - loss: 1.8126 - acc: 0.338 - ETA: 0s - loss: 1.8201 - acc: 0.335 - ETA: 0s - loss: 1.8165 - acc: 0.336 - ETA: 0s - loss: 1.8168 - acc: 0.338 - ETA: 0s - loss: 1.8103 - acc: 0.337 - ETA: 0s - loss: 1.8096 - acc: 0.339 - ETA: 0s - loss: 1.8087 - acc: 0.338 - ETA: 0s - loss: 1.8043 - acc: 0.341 - ETA: 0s - loss: 1.8042 - acc: 0.342 - ETA: 0s - loss: 1.8043 - acc: 0.341 - ETA: 0s - loss: 1.8084 - acc: 0.339 - 1s 244us/step - loss: 1.8095 - acc: 0.3395\n",
      "Epoch 10/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.1634 - acc: 0.218 - ETA: 1s - loss: 1.8806 - acc: 0.316 - ETA: 1s - loss: 1.7855 - acc: 0.337 - ETA: 1s - loss: 1.7778 - acc: 0.343 - ETA: 1s - loss: 1.7973 - acc: 0.335 - ETA: 1s - loss: 1.8130 - acc: 0.330 - ETA: 1s - loss: 1.8053 - acc: 0.335 - ETA: 0s - loss: 1.8145 - acc: 0.331 - ETA: 0s - loss: 1.8109 - acc: 0.336 - ETA: 0s - loss: 1.8111 - acc: 0.338 - ETA: 0s - loss: 1.8157 - acc: 0.339 - ETA: 0s - loss: 1.8135 - acc: 0.338 - ETA: 0s - loss: 1.8079 - acc: 0.341 - ETA: 0s - loss: 1.8060 - acc: 0.342 - ETA: 0s - loss: 1.8069 - acc: 0.342 - ETA: 0s - loss: 1.8062 - acc: 0.340 - ETA: 0s - loss: 1.8024 - acc: 0.343 - ETA: 0s - loss: 1.8011 - acc: 0.345 - ETA: 0s - loss: 1.8037 - acc: 0.344 - ETA: 0s - loss: 1.8060 - acc: 0.342 - ETA: 0s - loss: 1.8064 - acc: 0.341 - ETA: 0s - loss: 1.8068 - acc: 0.341 - ETA: 0s - loss: 1.8078 - acc: 0.342 - 1s 240us/step - loss: 1.8076 - acc: 0.3431\n",
      "Epoch 11/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.8218 - acc: 0.281 - ETA: 1s - loss: 1.7610 - acc: 0.359 - ETA: 1s - loss: 1.7492 - acc: 0.334 - ETA: 1s - loss: 1.7691 - acc: 0.335 - ETA: 1s - loss: 1.7711 - acc: 0.335 - ETA: 1s - loss: 1.7715 - acc: 0.347 - ETA: 0s - loss: 1.7781 - acc: 0.343 - ETA: 0s - loss: 1.7778 - acc: 0.345 - ETA: 0s - loss: 1.7743 - acc: 0.349 - ETA: 0s - loss: 1.7795 - acc: 0.350 - ETA: 0s - loss: 1.7844 - acc: 0.347 - ETA: 0s - loss: 1.7756 - acc: 0.350 - ETA: 0s - loss: 1.7749 - acc: 0.352 - ETA: 0s - loss: 1.7704 - acc: 0.351 - ETA: 0s - loss: 1.7797 - acc: 0.347 - ETA: 0s - loss: 1.7787 - acc: 0.348 - ETA: 0s - loss: 1.7772 - acc: 0.350 - ETA: 0s - loss: 1.7826 - acc: 0.347 - ETA: 0s - loss: 1.7805 - acc: 0.348 - ETA: 0s - loss: 1.7834 - acc: 0.347 - ETA: 0s - loss: 1.7822 - acc: 0.347 - ETA: 0s - loss: 1.7824 - acc: 0.348 - ETA: 0s - loss: 1.7892 - acc: 0.347 - 1s 245us/step - loss: 1.7896 - acc: 0.3474\n",
      "Epoch 12/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6836 - acc: 0.312 - ETA: 1s - loss: 1.7426 - acc: 0.371 - ETA: 1s - loss: 1.7481 - acc: 0.371 - ETA: 1s - loss: 1.7479 - acc: 0.381 - ETA: 1s - loss: 1.7379 - acc: 0.377 - ETA: 0s - loss: 1.7269 - acc: 0.375 - ETA: 0s - loss: 1.7305 - acc: 0.369 - ETA: 0s - loss: 1.7348 - acc: 0.367 - ETA: 0s - loss: 1.7409 - acc: 0.369 - ETA: 0s - loss: 1.7472 - acc: 0.365 - ETA: 0s - loss: 1.7545 - acc: 0.363 - ETA: 0s - loss: 1.7519 - acc: 0.361 - ETA: 0s - loss: 1.7485 - acc: 0.360 - ETA: 0s - loss: 1.7538 - acc: 0.359 - ETA: 0s - loss: 1.7523 - acc: 0.360 - ETA: 0s - loss: 1.7539 - acc: 0.357 - ETA: 0s - loss: 1.7626 - acc: 0.354 - ETA: 0s - loss: 1.7636 - acc: 0.354 - ETA: 0s - loss: 1.7628 - acc: 0.352 - ETA: 0s - loss: 1.7614 - acc: 0.353 - ETA: 0s - loss: 1.7641 - acc: 0.350 - ETA: 0s - loss: 1.7614 - acc: 0.351 - ETA: 0s - loss: 1.7605 - acc: 0.350 - 1s 236us/step - loss: 1.7600 - acc: 0.3512\n",
      "Epoch 13/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.9312 - acc: 0.437 - ETA: 1s - loss: 1.8645 - acc: 0.335 - ETA: 1s - loss: 1.8373 - acc: 0.347 - ETA: 1s - loss: 1.8139 - acc: 0.349 - ETA: 1s - loss: 1.8342 - acc: 0.338 - ETA: 1s - loss: 1.8099 - acc: 0.336 - ETA: 1s - loss: 1.8056 - acc: 0.337 - ETA: 0s - loss: 1.7949 - acc: 0.347 - ETA: 0s - loss: 1.7936 - acc: 0.348 - ETA: 0s - loss: 1.7840 - acc: 0.353 - ETA: 0s - loss: 1.7857 - acc: 0.353 - ETA: 0s - loss: 1.7883 - acc: 0.354 - ETA: 0s - loss: 1.7792 - acc: 0.357 - ETA: 0s - loss: 1.7766 - acc: 0.357 - ETA: 0s - loss: 1.7706 - acc: 0.359 - ETA: 0s - loss: 1.7687 - acc: 0.358 - ETA: 0s - loss: 1.7693 - acc: 0.358 - ETA: 0s - loss: 1.7677 - acc: 0.360 - ETA: 0s - loss: 1.7663 - acc: 0.362 - ETA: 0s - loss: 1.7660 - acc: 0.359 - ETA: 0s - loss: 1.7632 - acc: 0.360 - ETA: 0s - loss: 1.7652 - acc: 0.359 - ETA: 0s - loss: 1.7661 - acc: 0.357 - 1s 244us/step - loss: 1.7663 - acc: 0.3577\n",
      "Epoch 14/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.8295 - acc: 0.312 - ETA: 1s - loss: 1.7512 - acc: 0.354 - ETA: 1s - loss: 1.7596 - acc: 0.345 - ETA: 1s - loss: 1.7413 - acc: 0.343 - ETA: 1s - loss: 1.7324 - acc: 0.361 - ETA: 1s - loss: 1.7429 - acc: 0.359 - ETA: 0s - loss: 1.7527 - acc: 0.362 - ETA: 0s - loss: 1.7573 - acc: 0.360 - ETA: 0s - loss: 1.7538 - acc: 0.356 - ETA: 0s - loss: 1.7478 - acc: 0.355 - ETA: 0s - loss: 1.7504 - acc: 0.354 - ETA: 0s - loss: 1.7494 - acc: 0.354 - ETA: 0s - loss: 1.7480 - acc: 0.355 - ETA: 0s - loss: 1.7433 - acc: 0.358 - ETA: 0s - loss: 1.7373 - acc: 0.359 - ETA: 0s - loss: 1.7417 - acc: 0.357 - ETA: 0s - loss: 1.7410 - acc: 0.360 - ETA: 0s - loss: 1.7408 - acc: 0.361 - ETA: 0s - loss: 1.7436 - acc: 0.361 - ETA: 0s - loss: 1.7399 - acc: 0.362 - ETA: 0s - loss: 1.7393 - acc: 0.362 - ETA: 0s - loss: 1.7391 - acc: 0.360 - 1s 237us/step - loss: 1.7387 - acc: 0.3623\n",
      "Epoch 15/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7889 - acc: 0.343 - ETA: 1s - loss: 1.7393 - acc: 0.365 - ETA: 1s - loss: 1.7997 - acc: 0.340 - ETA: 1s - loss: 1.7804 - acc: 0.352 - ETA: 1s - loss: 1.7795 - acc: 0.352 - ETA: 1s - loss: 1.7796 - acc: 0.358 - ETA: 0s - loss: 1.7511 - acc: 0.366 - ETA: 0s - loss: 1.7449 - acc: 0.366 - ETA: 0s - loss: 1.7452 - acc: 0.373 - ETA: 0s - loss: 1.7392 - acc: 0.371 - ETA: 0s - loss: 1.7486 - acc: 0.366 - ETA: 0s - loss: 1.7548 - acc: 0.366 - ETA: 0s - loss: 1.7555 - acc: 0.365 - ETA: 0s - loss: 1.7522 - acc: 0.364 - ETA: 0s - loss: 1.7524 - acc: 0.362 - ETA: 0s - loss: 1.7513 - acc: 0.362 - ETA: 0s - loss: 1.7529 - acc: 0.360 - ETA: 0s - loss: 1.7500 - acc: 0.359 - ETA: 0s - loss: 1.7516 - acc: 0.358 - ETA: 0s - loss: 1.7535 - acc: 0.356 - ETA: 0s - loss: 1.7597 - acc: 0.356 - ETA: 0s - loss: 1.7523 - acc: 0.361 - ETA: 0s - loss: 1.7511 - acc: 0.361 - 1s 239us/step - loss: 1.7513 - acc: 0.3604\n",
      "Epoch 16/50\n",
      "5435/5435 [==============================] - ETA: 3s - loss: 1.8225 - acc: 0.375 - ETA: 1s - loss: 1.6689 - acc: 0.357 - ETA: 1s - loss: 1.6871 - acc: 0.368 - ETA: 1s - loss: 1.7246 - acc: 0.353 - ETA: 1s - loss: 1.7405 - acc: 0.352 - ETA: 1s - loss: 1.7410 - acc: 0.358 - ETA: 0s - loss: 1.7204 - acc: 0.356 - ETA: 0s - loss: 1.7237 - acc: 0.354 - ETA: 0s - loss: 1.7216 - acc: 0.354 - ETA: 0s - loss: 1.7216 - acc: 0.351 - ETA: 0s - loss: 1.7274 - acc: 0.354 - ETA: 0s - loss: 1.7297 - acc: 0.358 - ETA: 0s - loss: 1.7316 - acc: 0.359 - ETA: 0s - loss: 1.7286 - acc: 0.362 - ETA: 0s - loss: 1.7349 - acc: 0.359 - ETA: 0s - loss: 1.7375 - acc: 0.359 - ETA: 0s - loss: 1.7355 - acc: 0.358 - ETA: 0s - loss: 1.7344 - acc: 0.359 - ETA: 0s - loss: 1.7306 - acc: 0.360 - ETA: 0s - loss: 1.7303 - acc: 0.360 - ETA: 0s - loss: 1.7290 - acc: 0.359 - ETA: 0s - loss: 1.7300 - acc: 0.360 - ETA: 0s - loss: 1.7297 - acc: 0.360 - 1s 242us/step - loss: 1.7286 - acc: 0.3604\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 3s - loss: 1.6143 - acc: 0.437 - ETA: 1s - loss: 1.6967 - acc: 0.392 - ETA: 1s - loss: 1.6944 - acc: 0.394 - ETA: 1s - loss: 1.7056 - acc: 0.394 - ETA: 1s - loss: 1.7136 - acc: 0.389 - ETA: 1s - loss: 1.7107 - acc: 0.389 - ETA: 1s - loss: 1.7179 - acc: 0.383 - ETA: 1s - loss: 1.7266 - acc: 0.382 - ETA: 1s - loss: 1.7410 - acc: 0.375 - ETA: 0s - loss: 1.7401 - acc: 0.372 - ETA: 0s - loss: 1.7416 - acc: 0.371 - ETA: 0s - loss: 1.7364 - acc: 0.370 - ETA: 0s - loss: 1.7319 - acc: 0.368 - ETA: 0s - loss: 1.7340 - acc: 0.368 - ETA: 0s - loss: 1.7361 - acc: 0.368 - ETA: 0s - loss: 1.7282 - acc: 0.372 - ETA: 0s - loss: 1.7193 - acc: 0.374 - ETA: 0s - loss: 1.7168 - acc: 0.374 - ETA: 0s - loss: 1.7127 - acc: 0.377 - ETA: 0s - loss: 1.7129 - acc: 0.374 - ETA: 0s - loss: 1.7159 - acc: 0.371 - ETA: 0s - loss: 1.7142 - acc: 0.373 - ETA: 0s - loss: 1.7184 - acc: 0.371 - 1s 251us/step - loss: 1.7187 - acc: 0.3726\n",
      "Epoch 18/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.8522 - acc: 0.343 - ETA: 1s - loss: 1.6768 - acc: 0.368 - ETA: 1s - loss: 1.7077 - acc: 0.363 - ETA: 1s - loss: 1.7396 - acc: 0.346 - ETA: 1s - loss: 1.7394 - acc: 0.353 - ETA: 1s - loss: 1.7551 - acc: 0.342 - ETA: 0s - loss: 1.7487 - acc: 0.349 - ETA: 0s - loss: 1.7454 - acc: 0.351 - ETA: 0s - loss: 1.7466 - acc: 0.349 - ETA: 0s - loss: 1.7422 - acc: 0.352 - ETA: 0s - loss: 1.7451 - acc: 0.354 - ETA: 0s - loss: 1.7474 - acc: 0.356 - ETA: 0s - loss: 1.7379 - acc: 0.363 - ETA: 0s - loss: 1.7269 - acc: 0.367 - ETA: 0s - loss: 1.7284 - acc: 0.368 - ETA: 0s - loss: 1.7258 - acc: 0.369 - ETA: 0s - loss: 1.7167 - acc: 0.374 - ETA: 0s - loss: 1.7179 - acc: 0.373 - ETA: 0s - loss: 1.7161 - acc: 0.374 - ETA: 0s - loss: 1.7174 - acc: 0.375 - ETA: 0s - loss: 1.7190 - acc: 0.375 - ETA: 0s - loss: 1.7200 - acc: 0.375 - ETA: 0s - loss: 1.7182 - acc: 0.377 - ETA: 0s - loss: 1.7150 - acc: 0.378 - 1s 249us/step - loss: 1.7147 - acc: 0.3781\n",
      "Epoch 19/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.7829 - acc: 0.406 - ETA: 1s - loss: 1.7475 - acc: 0.421 - ETA: 1s - loss: 1.7429 - acc: 0.406 - ETA: 1s - loss: 1.6701 - acc: 0.424 - ETA: 1s - loss: 1.6902 - acc: 0.414 - ETA: 1s - loss: 1.6931 - acc: 0.402 - ETA: 1s - loss: 1.6957 - acc: 0.404 - ETA: 1s - loss: 1.6924 - acc: 0.397 - ETA: 1s - loss: 1.7078 - acc: 0.389 - ETA: 1s - loss: 1.7018 - acc: 0.392 - ETA: 1s - loss: 1.6972 - acc: 0.396 - ETA: 1s - loss: 1.7234 - acc: 0.383 - ETA: 0s - loss: 1.7221 - acc: 0.384 - ETA: 0s - loss: 1.7220 - acc: 0.382 - ETA: 0s - loss: 1.7212 - acc: 0.385 - ETA: 0s - loss: 1.7238 - acc: 0.384 - ETA: 0s - loss: 1.7152 - acc: 0.383 - ETA: 0s - loss: 1.7176 - acc: 0.383 - ETA: 0s - loss: 1.7134 - acc: 0.381 - ETA: 0s - loss: 1.7157 - acc: 0.381 - ETA: 0s - loss: 1.7110 - acc: 0.384 - ETA: 0s - loss: 1.7119 - acc: 0.381 - ETA: 0s - loss: 1.7127 - acc: 0.379 - ETA: 0s - loss: 1.7115 - acc: 0.379 - ETA: 0s - loss: 1.7127 - acc: 0.379 - ETA: 0s - loss: 1.7105 - acc: 0.380 - ETA: 0s - loss: 1.7075 - acc: 0.380 - ETA: 0s - loss: 1.7137 - acc: 0.378 - 2s 287us/step - loss: 1.7155 - acc: 0.3777\n",
      "Epoch 20/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6410 - acc: 0.406 - ETA: 1s - loss: 1.6554 - acc: 0.387 - ETA: 1s - loss: 1.6896 - acc: 0.386 - ETA: 1s - loss: 1.7228 - acc: 0.367 - ETA: 1s - loss: 1.7321 - acc: 0.369 - ETA: 1s - loss: 1.7290 - acc: 0.366 - ETA: 0s - loss: 1.7345 - acc: 0.364 - ETA: 0s - loss: 1.7399 - acc: 0.360 - ETA: 0s - loss: 1.7326 - acc: 0.367 - ETA: 0s - loss: 1.7288 - acc: 0.375 - ETA: 0s - loss: 1.7297 - acc: 0.374 - ETA: 0s - loss: 1.7292 - acc: 0.375 - ETA: 0s - loss: 1.7235 - acc: 0.376 - ETA: 0s - loss: 1.7116 - acc: 0.382 - ETA: 0s - loss: 1.7118 - acc: 0.382 - ETA: 0s - loss: 1.7041 - acc: 0.383 - ETA: 0s - loss: 1.7026 - acc: 0.382 - ETA: 0s - loss: 1.7064 - acc: 0.380 - ETA: 0s - loss: 1.7044 - acc: 0.382 - ETA: 0s - loss: 1.6986 - acc: 0.384 - ETA: 0s - loss: 1.6944 - acc: 0.386 - ETA: 0s - loss: 1.6990 - acc: 0.385 - ETA: 0s - loss: 1.7005 - acc: 0.383 - 1s 243us/step - loss: 1.7028 - acc: 0.3823\n",
      "Epoch 21/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8737 - acc: 0.312 - ETA: 1s - loss: 1.7150 - acc: 0.343 - ETA: 1s - loss: 1.7135 - acc: 0.359 - ETA: 1s - loss: 1.7321 - acc: 0.363 - ETA: 1s - loss: 1.6959 - acc: 0.368 - ETA: 0s - loss: 1.6968 - acc: 0.371 - ETA: 0s - loss: 1.6924 - acc: 0.376 - ETA: 0s - loss: 1.6951 - acc: 0.378 - ETA: 0s - loss: 1.6831 - acc: 0.387 - ETA: 0s - loss: 1.6964 - acc: 0.380 - ETA: 0s - loss: 1.7064 - acc: 0.379 - ETA: 0s - loss: 1.7065 - acc: 0.374 - ETA: 0s - loss: 1.7067 - acc: 0.370 - ETA: 0s - loss: 1.6996 - acc: 0.374 - ETA: 0s - loss: 1.6981 - acc: 0.377 - ETA: 0s - loss: 1.6968 - acc: 0.376 - ETA: 0s - loss: 1.7001 - acc: 0.376 - ETA: 0s - loss: 1.6935 - acc: 0.377 - ETA: 0s - loss: 1.6960 - acc: 0.374 - ETA: 0s - loss: 1.6949 - acc: 0.374 - ETA: 0s - loss: 1.6917 - acc: 0.375 - ETA: 0s - loss: 1.6877 - acc: 0.376 - 1s 238us/step - loss: 1.6928 - acc: 0.3757\n",
      "Epoch 22/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6886 - acc: 0.281 - ETA: 1s - loss: 1.5600 - acc: 0.455 - ETA: 1s - loss: 1.6246 - acc: 0.420 - ETA: 1s - loss: 1.6369 - acc: 0.403 - ETA: 1s - loss: 1.6367 - acc: 0.405 - ETA: 1s - loss: 1.6480 - acc: 0.397 - ETA: 1s - loss: 1.6546 - acc: 0.396 - ETA: 0s - loss: 1.6598 - acc: 0.396 - ETA: 0s - loss: 1.6727 - acc: 0.392 - ETA: 0s - loss: 1.6758 - acc: 0.395 - ETA: 0s - loss: 1.6716 - acc: 0.395 - ETA: 0s - loss: 1.6730 - acc: 0.395 - ETA: 0s - loss: 1.6730 - acc: 0.395 - ETA: 0s - loss: 1.6732 - acc: 0.393 - ETA: 0s - loss: 1.6700 - acc: 0.394 - ETA: 0s - loss: 1.6697 - acc: 0.391 - ETA: 0s - loss: 1.6738 - acc: 0.388 - ETA: 0s - loss: 1.6759 - acc: 0.388 - ETA: 0s - loss: 1.6764 - acc: 0.387 - ETA: 0s - loss: 1.6838 - acc: 0.386 - ETA: 0s - loss: 1.6865 - acc: 0.387 - ETA: 0s - loss: 1.6902 - acc: 0.384 - ETA: 0s - loss: 1.6891 - acc: 0.384 - 1s 247us/step - loss: 1.6880 - acc: 0.3860\n",
      "Epoch 23/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.5627 - acc: 0.406 - ETA: 1s - loss: 1.6324 - acc: 0.416 - ETA: 1s - loss: 1.5801 - acc: 0.453 - ETA: 1s - loss: 1.6363 - acc: 0.424 - ETA: 1s - loss: 1.6281 - acc: 0.423 - ETA: 1s - loss: 1.6288 - acc: 0.416 - ETA: 0s - loss: 1.6458 - acc: 0.418 - ETA: 0s - loss: 1.6523 - acc: 0.408 - ETA: 0s - loss: 1.6574 - acc: 0.407 - ETA: 0s - loss: 1.6626 - acc: 0.405 - ETA: 0s - loss: 1.6632 - acc: 0.405 - ETA: 0s - loss: 1.6615 - acc: 0.408 - ETA: 0s - loss: 1.6609 - acc: 0.407 - ETA: 0s - loss: 1.6646 - acc: 0.406 - ETA: 0s - loss: 1.6678 - acc: 0.407 - ETA: 0s - loss: 1.6707 - acc: 0.405 - ETA: 0s - loss: 1.6727 - acc: 0.404 - ETA: 0s - loss: 1.6753 - acc: 0.403 - ETA: 0s - loss: 1.6763 - acc: 0.400 - ETA: 0s - loss: 1.6779 - acc: 0.400 - ETA: 0s - loss: 1.6743 - acc: 0.402 - ETA: 0s - loss: 1.6723 - acc: 0.401 - ETA: 0s - loss: 1.6709 - acc: 0.401 - 1s 240us/step - loss: 1.6725 - acc: 0.4009\n",
      "Epoch 24/50\n",
      "5435/5435 [==============================] - ETA: 4s - loss: 1.6239 - acc: 0.343 - ETA: 1s - loss: 1.5667 - acc: 0.423 - ETA: 1s - loss: 1.6435 - acc: 0.397 - ETA: 1s - loss: 1.6236 - acc: 0.399 - ETA: 1s - loss: 1.6313 - acc: 0.403 - ETA: 1s - loss: 1.6267 - acc: 0.399 - ETA: 0s - loss: 1.6157 - acc: 0.403 - ETA: 0s - loss: 1.6206 - acc: 0.397 - ETA: 0s - loss: 1.6235 - acc: 0.399 - ETA: 0s - loss: 1.6483 - acc: 0.393 - ETA: 0s - loss: 1.6606 - acc: 0.391 - ETA: 0s - loss: 1.6640 - acc: 0.389 - ETA: 0s - loss: 1.6615 - acc: 0.387 - ETA: 0s - loss: 1.6591 - acc: 0.392 - ETA: 0s - loss: 1.6576 - acc: 0.392 - ETA: 0s - loss: 1.6580 - acc: 0.390 - ETA: 0s - loss: 1.6620 - acc: 0.391 - ETA: 0s - loss: 1.6608 - acc: 0.393 - ETA: 0s - loss: 1.6597 - acc: 0.393 - ETA: 0s - loss: 1.6617 - acc: 0.393 - ETA: 0s - loss: 1.6631 - acc: 0.393 - ETA: 0s - loss: 1.6624 - acc: 0.394 - ETA: 0s - loss: 1.6641 - acc: 0.392 - 1s 244us/step - loss: 1.6640 - acc: 0.3934\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6729 - acc: 0.312 - ETA: 1s - loss: 1.6472 - acc: 0.386 - ETA: 1s - loss: 1.6272 - acc: 0.396 - ETA: 1s - loss: 1.6287 - acc: 0.407 - ETA: 1s - loss: 1.6229 - acc: 0.418 - ETA: 1s - loss: 1.6387 - acc: 0.413 - ETA: 0s - loss: 1.6361 - acc: 0.416 - ETA: 0s - loss: 1.6544 - acc: 0.410 - ETA: 0s - loss: 1.6517 - acc: 0.414 - ETA: 0s - loss: 1.6491 - acc: 0.413 - ETA: 0s - loss: 1.6459 - acc: 0.413 - ETA: 0s - loss: 1.6499 - acc: 0.411 - ETA: 0s - loss: 1.6510 - acc: 0.407 - ETA: 0s - loss: 1.6540 - acc: 0.403 - ETA: 0s - loss: 1.6593 - acc: 0.399 - ETA: 0s - loss: 1.6619 - acc: 0.398 - ETA: 0s - loss: 1.6628 - acc: 0.396 - ETA: 0s - loss: 1.6663 - acc: 0.395 - ETA: 0s - loss: 1.6647 - acc: 0.395 - ETA: 0s - loss: 1.6618 - acc: 0.396 - ETA: 0s - loss: 1.6659 - acc: 0.395 - ETA: 0s - loss: 1.6674 - acc: 0.396 - ETA: 0s - loss: 1.6683 - acc: 0.395 - 1s 243us/step - loss: 1.6664 - acc: 0.3961\n",
      "Epoch 26/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4842 - acc: 0.343 - ETA: 1s - loss: 1.6350 - acc: 0.378 - ETA: 1s - loss: 1.7018 - acc: 0.371 - ETA: 1s - loss: 1.6682 - acc: 0.376 - ETA: 0s - loss: 1.6618 - acc: 0.378 - ETA: 0s - loss: 1.6501 - acc: 0.389 - ETA: 0s - loss: 1.6500 - acc: 0.389 - ETA: 0s - loss: 1.6555 - acc: 0.390 - ETA: 0s - loss: 1.6527 - acc: 0.388 - ETA: 0s - loss: 1.6575 - acc: 0.390 - ETA: 0s - loss: 1.6585 - acc: 0.398 - ETA: 0s - loss: 1.6606 - acc: 0.395 - ETA: 0s - loss: 1.6652 - acc: 0.393 - ETA: 0s - loss: 1.6499 - acc: 0.403 - ETA: 0s - loss: 1.6504 - acc: 0.405 - ETA: 0s - loss: 1.6482 - acc: 0.406 - ETA: 0s - loss: 1.6523 - acc: 0.404 - ETA: 0s - loss: 1.6547 - acc: 0.400 - ETA: 0s - loss: 1.6549 - acc: 0.400 - ETA: 0s - loss: 1.6583 - acc: 0.399 - ETA: 0s - loss: 1.6567 - acc: 0.400 - ETA: 0s - loss: 1.6534 - acc: 0.403 - ETA: 0s - loss: 1.6504 - acc: 0.403 - 1s 238us/step - loss: 1.6508 - acc: 0.4037\n",
      "Epoch 27/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6312 - acc: 0.375 - ETA: 1s - loss: 1.5501 - acc: 0.437 - ETA: 1s - loss: 1.5396 - acc: 0.443 - ETA: 1s - loss: 1.5691 - acc: 0.428 - ETA: 1s - loss: 1.5800 - acc: 0.421 - ETA: 0s - loss: 1.6099 - acc: 0.407 - ETA: 0s - loss: 1.6284 - acc: 0.403 - ETA: 0s - loss: 1.6236 - acc: 0.404 - ETA: 0s - loss: 1.6505 - acc: 0.398 - ETA: 0s - loss: 1.6466 - acc: 0.397 - ETA: 0s - loss: 1.6410 - acc: 0.400 - ETA: 0s - loss: 1.6448 - acc: 0.397 - ETA: 0s - loss: 1.6381 - acc: 0.403 - ETA: 0s - loss: 1.6401 - acc: 0.402 - ETA: 0s - loss: 1.6396 - acc: 0.402 - ETA: 0s - loss: 1.6427 - acc: 0.402 - ETA: 0s - loss: 1.6487 - acc: 0.397 - ETA: 0s - loss: 1.6494 - acc: 0.398 - ETA: 0s - loss: 1.6458 - acc: 0.398 - ETA: 0s - loss: 1.6481 - acc: 0.397 - ETA: 0s - loss: 1.6476 - acc: 0.400 - ETA: 0s - loss: 1.6516 - acc: 0.398 - ETA: 0s - loss: 1.6469 - acc: 0.399 - ETA: 0s - loss: 1.6471 - acc: 0.399 - 1s 244us/step - loss: 1.6445 - acc: 0.4006\n",
      "Epoch 28/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.3929 - acc: 0.468 - ETA: 1s - loss: 1.6458 - acc: 0.425 - ETA: 1s - loss: 1.6126 - acc: 0.418 - ETA: 1s - loss: 1.6374 - acc: 0.411 - ETA: 1s - loss: 1.6394 - acc: 0.415 - ETA: 1s - loss: 1.6299 - acc: 0.430 - ETA: 0s - loss: 1.6360 - acc: 0.421 - ETA: 0s - loss: 1.6306 - acc: 0.416 - ETA: 0s - loss: 1.6357 - acc: 0.420 - ETA: 0s - loss: 1.6440 - acc: 0.422 - ETA: 0s - loss: 1.6397 - acc: 0.417 - ETA: 0s - loss: 1.6344 - acc: 0.416 - ETA: 0s - loss: 1.6355 - acc: 0.415 - ETA: 0s - loss: 1.6371 - acc: 0.415 - ETA: 0s - loss: 1.6345 - acc: 0.413 - ETA: 0s - loss: 1.6357 - acc: 0.413 - ETA: 0s - loss: 1.6328 - acc: 0.414 - ETA: 0s - loss: 1.6256 - acc: 0.416 - ETA: 0s - loss: 1.6209 - acc: 0.417 - ETA: 0s - loss: 1.6213 - acc: 0.415 - ETA: 0s - loss: 1.6254 - acc: 0.414 - ETA: 0s - loss: 1.6275 - acc: 0.413 - ETA: 0s - loss: 1.6297 - acc: 0.412 - 1s 239us/step - loss: 1.6289 - acc: 0.4114\n",
      "Epoch 29/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6582 - acc: 0.500 - ETA: 1s - loss: 1.6646 - acc: 0.437 - ETA: 1s - loss: 1.6616 - acc: 0.421 - ETA: 1s - loss: 1.6538 - acc: 0.407 - ETA: 1s - loss: 1.6250 - acc: 0.416 - ETA: 0s - loss: 1.6519 - acc: 0.402 - ETA: 0s - loss: 1.6287 - acc: 0.407 - ETA: 0s - loss: 1.6329 - acc: 0.403 - ETA: 0s - loss: 1.6440 - acc: 0.397 - ETA: 0s - loss: 1.6485 - acc: 0.392 - ETA: 0s - loss: 1.6478 - acc: 0.388 - ETA: 0s - loss: 1.6408 - acc: 0.393 - ETA: 0s - loss: 1.6453 - acc: 0.393 - ETA: 0s - loss: 1.6478 - acc: 0.394 - ETA: 0s - loss: 1.6473 - acc: 0.395 - ETA: 0s - loss: 1.6407 - acc: 0.399 - ETA: 0s - loss: 1.6342 - acc: 0.402 - ETA: 0s - loss: 1.6311 - acc: 0.403 - ETA: 0s - loss: 1.6369 - acc: 0.404 - ETA: 0s - loss: 1.6348 - acc: 0.404 - ETA: 0s - loss: 1.6376 - acc: 0.402 - ETA: 0s - loss: 1.6410 - acc: 0.400 - ETA: 0s - loss: 1.6412 - acc: 0.402 - ETA: 0s - loss: 1.6377 - acc: 0.402 - 1s 243us/step - loss: 1.6355 - acc: 0.4031\n",
      "Epoch 30/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3768 - acc: 0.531 - ETA: 1s - loss: 1.5727 - acc: 0.418 - ETA: 1s - loss: 1.5894 - acc: 0.410 - ETA: 1s - loss: 1.6218 - acc: 0.389 - ETA: 1s - loss: 1.6062 - acc: 0.396 - ETA: 1s - loss: 1.5958 - acc: 0.400 - ETA: 0s - loss: 1.6027 - acc: 0.401 - ETA: 0s - loss: 1.6010 - acc: 0.404 - ETA: 0s - loss: 1.6024 - acc: 0.407 - ETA: 0s - loss: 1.6101 - acc: 0.406 - ETA: 0s - loss: 1.6200 - acc: 0.403 - ETA: 0s - loss: 1.6127 - acc: 0.405 - ETA: 0s - loss: 1.6163 - acc: 0.406 - ETA: 0s - loss: 1.6235 - acc: 0.405 - ETA: 0s - loss: 1.6256 - acc: 0.401 - ETA: 0s - loss: 1.6272 - acc: 0.399 - ETA: 0s - loss: 1.6257 - acc: 0.399 - ETA: 0s - loss: 1.6281 - acc: 0.401 - ETA: 0s - loss: 1.6286 - acc: 0.401 - ETA: 0s - loss: 1.6337 - acc: 0.401 - ETA: 0s - loss: 1.6296 - acc: 0.405 - ETA: 0s - loss: 1.6289 - acc: 0.406 - ETA: 0s - loss: 1.6294 - acc: 0.406 - 1s 245us/step - loss: 1.6275 - acc: 0.4081\n",
      "Epoch 31/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.5980 - acc: 0.468 - ETA: 1s - loss: 1.6557 - acc: 0.414 - ETA: 1s - loss: 1.5933 - acc: 0.422 - ETA: 1s - loss: 1.6020 - acc: 0.406 - ETA: 1s - loss: 1.6103 - acc: 0.399 - ETA: 1s - loss: 1.5941 - acc: 0.403 - ETA: 0s - loss: 1.5938 - acc: 0.413 - ETA: 0s - loss: 1.5974 - acc: 0.414 - ETA: 0s - loss: 1.5977 - acc: 0.420 - ETA: 0s - loss: 1.5977 - acc: 0.418 - ETA: 0s - loss: 1.6062 - acc: 0.416 - ETA: 0s - loss: 1.5985 - acc: 0.419 - ETA: 0s - loss: 1.6050 - acc: 0.414 - ETA: 0s - loss: 1.6000 - acc: 0.416 - ETA: 0s - loss: 1.5990 - acc: 0.418 - ETA: 0s - loss: 1.5996 - acc: 0.414 - ETA: 0s - loss: 1.5979 - acc: 0.413 - ETA: 0s - loss: 1.5973 - acc: 0.415 - ETA: 0s - loss: 1.5996 - acc: 0.414 - ETA: 0s - loss: 1.6075 - acc: 0.412 - ETA: 0s - loss: 1.6096 - acc: 0.412 - ETA: 0s - loss: 1.6102 - acc: 0.411 - 1s 236us/step - loss: 1.6136 - acc: 0.4085\n",
      "Epoch 32/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7149 - acc: 0.343 - ETA: 1s - loss: 1.6382 - acc: 0.381 - ETA: 1s - loss: 1.6217 - acc: 0.388 - ETA: 1s - loss: 1.6098 - acc: 0.402 - ETA: 1s - loss: 1.6044 - acc: 0.405 - ETA: 0s - loss: 1.6106 - acc: 0.409 - ETA: 0s - loss: 1.6267 - acc: 0.403 - ETA: 0s - loss: 1.6099 - acc: 0.413 - ETA: 0s - loss: 1.6144 - acc: 0.412 - ETA: 0s - loss: 1.6075 - acc: 0.414 - ETA: 0s - loss: 1.6076 - acc: 0.417 - ETA: 0s - loss: 1.6079 - acc: 0.415 - ETA: 0s - loss: 1.6074 - acc: 0.416 - ETA: 0s - loss: 1.6113 - acc: 0.416 - ETA: 0s - loss: 1.6143 - acc: 0.412 - ETA: 0s - loss: 1.6114 - acc: 0.414 - ETA: 0s - loss: 1.6145 - acc: 0.410 - ETA: 0s - loss: 1.6159 - acc: 0.411 - ETA: 0s - loss: 1.6172 - acc: 0.409 - ETA: 0s - loss: 1.6244 - acc: 0.409 - ETA: 0s - loss: 1.6197 - acc: 0.411 - ETA: 0s - loss: 1.6144 - acc: 0.413 - ETA: 0s - loss: 1.6141 - acc: 0.413 - 1s 242us/step - loss: 1.6180 - acc: 0.4123\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 0s - loss: 1.7195 - acc: 0.406 - ETA: 1s - loss: 1.5758 - acc: 0.399 - ETA: 1s - loss: 1.5735 - acc: 0.404 - ETA: 1s - loss: 1.5717 - acc: 0.410 - ETA: 1s - loss: 1.5940 - acc: 0.404 - ETA: 0s - loss: 1.6039 - acc: 0.407 - ETA: 0s - loss: 1.5929 - acc: 0.422 - ETA: 0s - loss: 1.5947 - acc: 0.422 - ETA: 0s - loss: 1.6038 - acc: 0.413 - ETA: 0s - loss: 1.6085 - acc: 0.413 - ETA: 0s - loss: 1.6097 - acc: 0.408 - ETA: 0s - loss: 1.5949 - acc: 0.414 - ETA: 0s - loss: 1.5954 - acc: 0.415 - ETA: 0s - loss: 1.5936 - acc: 0.418 - ETA: 0s - loss: 1.5949 - acc: 0.417 - ETA: 0s - loss: 1.6058 - acc: 0.415 - ETA: 0s - loss: 1.6060 - acc: 0.413 - ETA: 0s - loss: 1.6065 - acc: 0.412 - ETA: 0s - loss: 1.6070 - acc: 0.413 - ETA: 0s - loss: 1.6073 - acc: 0.411 - ETA: 0s - loss: 1.6086 - acc: 0.411 - ETA: 0s - loss: 1.6112 - acc: 0.410 - ETA: 0s - loss: 1.6077 - acc: 0.412 - ETA: 0s - loss: 1.6098 - acc: 0.411 - 1s 243us/step - loss: 1.6095 - acc: 0.4110\n",
      "Epoch 34/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5039 - acc: 0.562 - ETA: 1s - loss: 1.7126 - acc: 0.397 - ETA: 1s - loss: 1.6383 - acc: 0.430 - ETA: 1s - loss: 1.6022 - acc: 0.437 - ETA: 1s - loss: 1.6262 - acc: 0.425 - ETA: 1s - loss: 1.6254 - acc: 0.421 - ETA: 0s - loss: 1.6169 - acc: 0.411 - ETA: 0s - loss: 1.6287 - acc: 0.406 - ETA: 0s - loss: 1.6210 - acc: 0.406 - ETA: 0s - loss: 1.6206 - acc: 0.405 - ETA: 0s - loss: 1.6204 - acc: 0.400 - ETA: 0s - loss: 1.6133 - acc: 0.403 - ETA: 0s - loss: 1.6090 - acc: 0.408 - ETA: 0s - loss: 1.6151 - acc: 0.406 - ETA: 0s - loss: 1.6081 - acc: 0.407 - ETA: 0s - loss: 1.6115 - acc: 0.405 - ETA: 0s - loss: 1.6079 - acc: 0.406 - ETA: 0s - loss: 1.6034 - acc: 0.409 - ETA: 0s - loss: 1.6013 - acc: 0.411 - ETA: 0s - loss: 1.5985 - acc: 0.413 - ETA: 0s - loss: 1.6009 - acc: 0.412 - ETA: 0s - loss: 1.6012 - acc: 0.412 - ETA: 0s - loss: 1.6065 - acc: 0.411 - ETA: 0s - loss: 1.6090 - acc: 0.410 - 1s 246us/step - loss: 1.6087 - acc: 0.4112\n",
      "Epoch 35/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7075 - acc: 0.312 - ETA: 1s - loss: 1.6493 - acc: 0.402 - ETA: 1s - loss: 1.6536 - acc: 0.402 - ETA: 1s - loss: 1.6456 - acc: 0.402 - ETA: 1s - loss: 1.6376 - acc: 0.413 - ETA: 0s - loss: 1.6308 - acc: 0.407 - ETA: 0s - loss: 1.6374 - acc: 0.403 - ETA: 0s - loss: 1.6268 - acc: 0.404 - ETA: 0s - loss: 1.6334 - acc: 0.406 - ETA: 0s - loss: 1.6365 - acc: 0.404 - ETA: 0s - loss: 1.6324 - acc: 0.403 - ETA: 0s - loss: 1.6332 - acc: 0.403 - ETA: 0s - loss: 1.6245 - acc: 0.403 - ETA: 0s - loss: 1.6243 - acc: 0.406 - ETA: 0s - loss: 1.6240 - acc: 0.406 - ETA: 0s - loss: 1.6222 - acc: 0.407 - ETA: 0s - loss: 1.6162 - acc: 0.408 - ETA: 0s - loss: 1.6185 - acc: 0.406 - ETA: 0s - loss: 1.6150 - acc: 0.408 - ETA: 0s - loss: 1.6101 - acc: 0.410 - ETA: 0s - loss: 1.6050 - acc: 0.411 - ETA: 0s - loss: 1.6034 - acc: 0.411 - ETA: 0s - loss: 1.6028 - acc: 0.412 - ETA: 0s - loss: 1.6014 - acc: 0.412 - ETA: 0s - loss: 1.6010 - acc: 0.412 - 1s 258us/step - loss: 1.6052 - acc: 0.4125\n",
      "Epoch 36/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.4103 - acc: 0.468 - ETA: 1s - loss: 1.4868 - acc: 0.464 - ETA: 1s - loss: 1.5758 - acc: 0.424 - ETA: 1s - loss: 1.5993 - acc: 0.406 - ETA: 1s - loss: 1.5885 - acc: 0.411 - ETA: 1s - loss: 1.5721 - acc: 0.417 - ETA: 1s - loss: 1.5721 - acc: 0.417 - ETA: 0s - loss: 1.5888 - acc: 0.416 - ETA: 0s - loss: 1.5921 - acc: 0.414 - ETA: 0s - loss: 1.5969 - acc: 0.411 - ETA: 0s - loss: 1.5901 - acc: 0.416 - ETA: 0s - loss: 1.5949 - acc: 0.413 - ETA: 0s - loss: 1.5970 - acc: 0.414 - ETA: 0s - loss: 1.5937 - acc: 0.419 - ETA: 0s - loss: 1.5916 - acc: 0.420 - ETA: 0s - loss: 1.5982 - acc: 0.419 - ETA: 0s - loss: 1.6013 - acc: 0.419 - ETA: 0s - loss: 1.5991 - acc: 0.420 - ETA: 0s - loss: 1.5971 - acc: 0.420 - ETA: 0s - loss: 1.5918 - acc: 0.425 - ETA: 0s - loss: 1.5897 - acc: 0.424 - ETA: 0s - loss: 1.5952 - acc: 0.422 - ETA: 0s - loss: 1.5941 - acc: 0.421 - ETA: 0s - loss: 1.5983 - acc: 0.419 - 1s 246us/step - loss: 1.5979 - acc: 0.4210\n",
      "Epoch 37/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7766 - acc: 0.250 - ETA: 1s - loss: 1.5843 - acc: 0.409 - ETA: 1s - loss: 1.5684 - acc: 0.420 - ETA: 1s - loss: 1.5316 - acc: 0.431 - ETA: 1s - loss: 1.5181 - acc: 0.443 - ETA: 0s - loss: 1.5586 - acc: 0.430 - ETA: 0s - loss: 1.5723 - acc: 0.425 - ETA: 0s - loss: 1.5785 - acc: 0.423 - ETA: 0s - loss: 1.6019 - acc: 0.412 - ETA: 0s - loss: 1.6087 - acc: 0.407 - ETA: 0s - loss: 1.6146 - acc: 0.407 - ETA: 0s - loss: 1.6155 - acc: 0.406 - ETA: 0s - loss: 1.6116 - acc: 0.409 - ETA: 0s - loss: 1.6042 - acc: 0.411 - ETA: 0s - loss: 1.6070 - acc: 0.408 - ETA: 0s - loss: 1.6047 - acc: 0.408 - ETA: 0s - loss: 1.6107 - acc: 0.408 - ETA: 0s - loss: 1.6058 - acc: 0.412 - ETA: 0s - loss: 1.6082 - acc: 0.411 - ETA: 0s - loss: 1.6023 - acc: 0.414 - ETA: 0s - loss: 1.6005 - acc: 0.416 - ETA: 0s - loss: 1.5976 - acc: 0.418 - 1s 239us/step - loss: 1.5974 - acc: 0.4190\n",
      "Epoch 38/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.7841 - acc: 0.312 - ETA: 1s - loss: 1.6987 - acc: 0.453 - ETA: 1s - loss: 1.5966 - acc: 0.458 - ETA: 1s - loss: 1.5851 - acc: 0.452 - ETA: 1s - loss: 1.5833 - acc: 0.445 - ETA: 1s - loss: 1.5808 - acc: 0.443 - ETA: 1s - loss: 1.5831 - acc: 0.440 - ETA: 0s - loss: 1.5913 - acc: 0.436 - ETA: 0s - loss: 1.5996 - acc: 0.435 - ETA: 0s - loss: 1.5892 - acc: 0.437 - ETA: 0s - loss: 1.5907 - acc: 0.434 - ETA: 0s - loss: 1.5937 - acc: 0.430 - ETA: 0s - loss: 1.5961 - acc: 0.428 - ETA: 0s - loss: 1.6011 - acc: 0.424 - ETA: 0s - loss: 1.6031 - acc: 0.421 - ETA: 0s - loss: 1.5974 - acc: 0.423 - ETA: 0s - loss: 1.6010 - acc: 0.421 - ETA: 0s - loss: 1.6027 - acc: 0.422 - ETA: 0s - loss: 1.6011 - acc: 0.423 - ETA: 0s - loss: 1.5971 - acc: 0.425 - ETA: 0s - loss: 1.5959 - acc: 0.428 - ETA: 0s - loss: 1.5958 - acc: 0.425 - ETA: 0s - loss: 1.5929 - acc: 0.426 - ETA: 0s - loss: 1.5912 - acc: 0.427 - 1s 243us/step - loss: 1.5934 - acc: 0.4267\n",
      "Epoch 39/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5309 - acc: 0.343 - ETA: 1s - loss: 1.6217 - acc: 0.378 - ETA: 1s - loss: 1.5680 - acc: 0.425 - ETA: 1s - loss: 1.5689 - acc: 0.428 - ETA: 1s - loss: 1.5691 - acc: 0.417 - ETA: 1s - loss: 1.5900 - acc: 0.413 - ETA: 0s - loss: 1.5939 - acc: 0.418 - ETA: 0s - loss: 1.5953 - acc: 0.429 - ETA: 0s - loss: 1.5951 - acc: 0.423 - ETA: 0s - loss: 1.5862 - acc: 0.425 - ETA: 0s - loss: 1.5810 - acc: 0.426 - ETA: 0s - loss: 1.5782 - acc: 0.425 - ETA: 0s - loss: 1.5802 - acc: 0.423 - ETA: 0s - loss: 1.5822 - acc: 0.423 - ETA: 0s - loss: 1.5749 - acc: 0.426 - ETA: 0s - loss: 1.5737 - acc: 0.429 - ETA: 0s - loss: 1.5746 - acc: 0.429 - ETA: 0s - loss: 1.5794 - acc: 0.428 - ETA: 0s - loss: 1.5751 - acc: 0.430 - ETA: 0s - loss: 1.5827 - acc: 0.430 - ETA: 0s - loss: 1.5829 - acc: 0.431 - ETA: 0s - loss: 1.5841 - acc: 0.430 - ETA: 0s - loss: 1.5821 - acc: 0.430 - ETA: 0s - loss: 1.5808 - acc: 0.430 - 1s 251us/step - loss: 1.5794 - acc: 0.4313\n",
      "Epoch 40/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6624 - acc: 0.375 - ETA: 1s - loss: 1.5256 - acc: 0.480 - ETA: 1s - loss: 1.5102 - acc: 0.472 - ETA: 1s - loss: 1.5723 - acc: 0.450 - ETA: 1s - loss: 1.5806 - acc: 0.437 - ETA: 1s - loss: 1.5847 - acc: 0.435 - ETA: 0s - loss: 1.5811 - acc: 0.441 - ETA: 0s - loss: 1.5816 - acc: 0.438 - ETA: 0s - loss: 1.5806 - acc: 0.439 - ETA: 0s - loss: 1.5809 - acc: 0.436 - ETA: 0s - loss: 1.5800 - acc: 0.432 - ETA: 0s - loss: 1.5766 - acc: 0.433 - ETA: 0s - loss: 1.5665 - acc: 0.432 - ETA: 0s - loss: 1.5708 - acc: 0.430 - ETA: 0s - loss: 1.5765 - acc: 0.427 - ETA: 0s - loss: 1.5811 - acc: 0.424 - ETA: 0s - loss: 1.5852 - acc: 0.423 - ETA: 0s - loss: 1.5828 - acc: 0.425 - ETA: 0s - loss: 1.5837 - acc: 0.425 - ETA: 0s - loss: 1.5828 - acc: 0.425 - ETA: 0s - loss: 1.5828 - acc: 0.423 - ETA: 0s - loss: 1.5840 - acc: 0.420 - ETA: 0s - loss: 1.5874 - acc: 0.419 - 1s 240us/step - loss: 1.5919 - acc: 0.4171\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 0s - loss: 1.3977 - acc: 0.406 - ETA: 1s - loss: 1.4728 - acc: 0.447 - ETA: 1s - loss: 1.5938 - acc: 0.415 - ETA: 1s - loss: 1.6211 - acc: 0.416 - ETA: 1s - loss: 1.6104 - acc: 0.431 - ETA: 1s - loss: 1.5965 - acc: 0.432 - ETA: 0s - loss: 1.5752 - acc: 0.434 - ETA: 0s - loss: 1.5953 - acc: 0.424 - ETA: 0s - loss: 1.5960 - acc: 0.423 - ETA: 0s - loss: 1.5952 - acc: 0.427 - ETA: 0s - loss: 1.5853 - acc: 0.429 - ETA: 0s - loss: 1.5895 - acc: 0.429 - ETA: 0s - loss: 1.5838 - acc: 0.425 - ETA: 0s - loss: 1.5843 - acc: 0.420 - ETA: 0s - loss: 1.5829 - acc: 0.424 - ETA: 0s - loss: 1.5847 - acc: 0.424 - ETA: 0s - loss: 1.5856 - acc: 0.425 - ETA: 0s - loss: 1.5875 - acc: 0.422 - ETA: 0s - loss: 1.5893 - acc: 0.422 - ETA: 0s - loss: 1.5881 - acc: 0.421 - ETA: 0s - loss: 1.5882 - acc: 0.421 - ETA: 0s - loss: 1.5896 - acc: 0.420 - ETA: 0s - loss: 1.5874 - acc: 0.420 - 1s 242us/step - loss: 1.5874 - acc: 0.4204\n",
      "Epoch 42/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5633 - acc: 0.500 - ETA: 1s - loss: 1.5906 - acc: 0.409 - ETA: 1s - loss: 1.5868 - acc: 0.404 - ETA: 1s - loss: 1.5624 - acc: 0.430 - ETA: 1s - loss: 1.5635 - acc: 0.432 - ETA: 1s - loss: 1.5582 - acc: 0.432 - ETA: 1s - loss: 1.5677 - acc: 0.432 - ETA: 0s - loss: 1.5681 - acc: 0.434 - ETA: 0s - loss: 1.5650 - acc: 0.429 - ETA: 0s - loss: 1.5718 - acc: 0.427 - ETA: 0s - loss: 1.5677 - acc: 0.426 - ETA: 0s - loss: 1.5691 - acc: 0.423 - ETA: 0s - loss: 1.5716 - acc: 0.424 - ETA: 0s - loss: 1.5819 - acc: 0.422 - ETA: 0s - loss: 1.5778 - acc: 0.423 - ETA: 0s - loss: 1.5848 - acc: 0.420 - ETA: 0s - loss: 1.5831 - acc: 0.417 - ETA: 0s - loss: 1.5811 - acc: 0.416 - ETA: 0s - loss: 1.5823 - acc: 0.417 - ETA: 0s - loss: 1.5804 - acc: 0.416 - ETA: 0s - loss: 1.5736 - acc: 0.420 - ETA: 0s - loss: 1.5685 - acc: 0.423 - ETA: 0s - loss: 1.5677 - acc: 0.422 - 1s 243us/step - loss: 1.5701 - acc: 0.4206\n",
      "Epoch 43/50\n",
      "5435/5435 [==============================] - ETA: 3s - loss: 1.6514 - acc: 0.468 - ETA: 1s - loss: 1.5840 - acc: 0.457 - ETA: 1s - loss: 1.5420 - acc: 0.443 - ETA: 1s - loss: 1.5710 - acc: 0.423 - ETA: 1s - loss: 1.5369 - acc: 0.433 - ETA: 1s - loss: 1.5535 - acc: 0.431 - ETA: 0s - loss: 1.5864 - acc: 0.423 - ETA: 0s - loss: 1.5677 - acc: 0.431 - ETA: 0s - loss: 1.5696 - acc: 0.431 - ETA: 0s - loss: 1.5786 - acc: 0.430 - ETA: 0s - loss: 1.5799 - acc: 0.430 - ETA: 0s - loss: 1.5843 - acc: 0.428 - ETA: 0s - loss: 1.5796 - acc: 0.431 - ETA: 0s - loss: 1.5793 - acc: 0.430 - ETA: 0s - loss: 1.5768 - acc: 0.430 - ETA: 0s - loss: 1.5759 - acc: 0.431 - ETA: 0s - loss: 1.5780 - acc: 0.431 - ETA: 0s - loss: 1.5777 - acc: 0.430 - ETA: 0s - loss: 1.5753 - acc: 0.431 - ETA: 0s - loss: 1.5768 - acc: 0.430 - ETA: 0s - loss: 1.5755 - acc: 0.432 - ETA: 0s - loss: 1.5744 - acc: 0.433 - ETA: 0s - loss: 1.5706 - acc: 0.434 - ETA: 0s - loss: 1.5700 - acc: 0.432 - 1s 244us/step - loss: 1.5695 - acc: 0.4335\n",
      "Epoch 44/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3628 - acc: 0.406 - ETA: 1s - loss: 1.5361 - acc: 0.454 - ETA: 1s - loss: 1.5444 - acc: 0.433 - ETA: 1s - loss: 1.5478 - acc: 0.452 - ETA: 1s - loss: 1.5360 - acc: 0.442 - ETA: 0s - loss: 1.5434 - acc: 0.443 - ETA: 0s - loss: 1.5294 - acc: 0.446 - ETA: 0s - loss: 1.5384 - acc: 0.442 - ETA: 0s - loss: 1.5444 - acc: 0.441 - ETA: 0s - loss: 1.5591 - acc: 0.438 - ETA: 0s - loss: 1.5589 - acc: 0.436 - ETA: 0s - loss: 1.5642 - acc: 0.434 - ETA: 0s - loss: 1.5674 - acc: 0.431 - ETA: 0s - loss: 1.5631 - acc: 0.430 - ETA: 0s - loss: 1.5660 - acc: 0.426 - ETA: 0s - loss: 1.5748 - acc: 0.422 - ETA: 0s - loss: 1.5633 - acc: 0.428 - ETA: 0s - loss: 1.5650 - acc: 0.425 - ETA: 0s - loss: 1.5668 - acc: 0.427 - ETA: 0s - loss: 1.5690 - acc: 0.426 - ETA: 0s - loss: 1.5710 - acc: 0.426 - ETA: 0s - loss: 1.5700 - acc: 0.426 - ETA: 0s - loss: 1.5717 - acc: 0.426 - 1s 236us/step - loss: 1.5731 - acc: 0.4250\n",
      "Epoch 45/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6235 - acc: 0.468 - ETA: 1s - loss: 1.6122 - acc: 0.415 - ETA: 1s - loss: 1.5862 - acc: 0.434 - ETA: 1s - loss: 1.5753 - acc: 0.438 - ETA: 1s - loss: 1.5801 - acc: 0.436 - ETA: 1s - loss: 1.5910 - acc: 0.433 - ETA: 0s - loss: 1.5753 - acc: 0.438 - ETA: 0s - loss: 1.5792 - acc: 0.431 - ETA: 0s - loss: 1.5846 - acc: 0.429 - ETA: 0s - loss: 1.5783 - acc: 0.428 - ETA: 0s - loss: 1.5831 - acc: 0.425 - ETA: 0s - loss: 1.5759 - acc: 0.426 - ETA: 0s - loss: 1.5789 - acc: 0.423 - ETA: 0s - loss: 1.5720 - acc: 0.424 - ETA: 0s - loss: 1.5779 - acc: 0.426 - ETA: 0s - loss: 1.5767 - acc: 0.424 - ETA: 0s - loss: 1.5736 - acc: 0.425 - ETA: 0s - loss: 1.5731 - acc: 0.425 - ETA: 0s - loss: 1.5713 - acc: 0.426 - ETA: 0s - loss: 1.5779 - acc: 0.426 - ETA: 0s - loss: 1.5738 - acc: 0.429 - ETA: 0s - loss: 1.5789 - acc: 0.428 - ETA: 0s - loss: 1.5797 - acc: 0.428 - 1s 240us/step - loss: 1.5784 - acc: 0.4282\n",
      "Epoch 46/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.8253 - acc: 0.312 - ETA: 1s - loss: 1.5437 - acc: 0.398 - ETA: 1s - loss: 1.5377 - acc: 0.437 - ETA: 1s - loss: 1.5310 - acc: 0.448 - ETA: 1s - loss: 1.5393 - acc: 0.448 - ETA: 1s - loss: 1.5438 - acc: 0.447 - ETA: 0s - loss: 1.5411 - acc: 0.447 - ETA: 0s - loss: 1.5435 - acc: 0.443 - ETA: 0s - loss: 1.5264 - acc: 0.446 - ETA: 0s - loss: 1.5285 - acc: 0.446 - ETA: 0s - loss: 1.5359 - acc: 0.443 - ETA: 0s - loss: 1.5487 - acc: 0.440 - ETA: 0s - loss: 1.5474 - acc: 0.441 - ETA: 0s - loss: 1.5437 - acc: 0.439 - ETA: 0s - loss: 1.5432 - acc: 0.440 - ETA: 0s - loss: 1.5463 - acc: 0.440 - ETA: 0s - loss: 1.5472 - acc: 0.438 - ETA: 0s - loss: 1.5460 - acc: 0.440 - ETA: 0s - loss: 1.5478 - acc: 0.438 - ETA: 0s - loss: 1.5483 - acc: 0.438 - ETA: 0s - loss: 1.5525 - acc: 0.437 - ETA: 0s - loss: 1.5528 - acc: 0.436 - ETA: 0s - loss: 1.5512 - acc: 0.437 - ETA: 0s - loss: 1.5538 - acc: 0.436 - 1s 247us/step - loss: 1.5518 - acc: 0.4379\n",
      "Epoch 47/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6405 - acc: 0.437 - ETA: 1s - loss: 1.5623 - acc: 0.398 - ETA: 1s - loss: 1.5938 - acc: 0.409 - ETA: 1s - loss: 1.5783 - acc: 0.408 - ETA: 1s - loss: 1.5988 - acc: 0.410 - ETA: 0s - loss: 1.5910 - acc: 0.411 - ETA: 0s - loss: 1.5924 - acc: 0.408 - ETA: 0s - loss: 1.5984 - acc: 0.405 - ETA: 0s - loss: 1.5901 - acc: 0.404 - ETA: 0s - loss: 1.5882 - acc: 0.402 - ETA: 0s - loss: 1.5690 - acc: 0.412 - ETA: 0s - loss: 1.5746 - acc: 0.412 - ETA: 0s - loss: 1.5682 - acc: 0.415 - ETA: 0s - loss: 1.5705 - acc: 0.415 - ETA: 0s - loss: 1.5695 - acc: 0.418 - ETA: 0s - loss: 1.5697 - acc: 0.417 - ETA: 0s - loss: 1.5649 - acc: 0.420 - ETA: 0s - loss: 1.5650 - acc: 0.421 - ETA: 0s - loss: 1.5631 - acc: 0.422 - ETA: 0s - loss: 1.5598 - acc: 0.423 - ETA: 0s - loss: 1.5588 - acc: 0.423 - ETA: 0s - loss: 1.5572 - acc: 0.424 - ETA: 0s - loss: 1.5535 - acc: 0.426 - 1s 240us/step - loss: 1.5519 - acc: 0.4289\n",
      "Epoch 48/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5581 - acc: 0.437 - ETA: 1s - loss: 1.4480 - acc: 0.451 - ETA: 1s - loss: 1.4932 - acc: 0.441 - ETA: 1s - loss: 1.5077 - acc: 0.449 - ETA: 1s - loss: 1.4958 - acc: 0.457 - ETA: 1s - loss: 1.5043 - acc: 0.450 - ETA: 1s - loss: 1.5129 - acc: 0.452 - ETA: 0s - loss: 1.5307 - acc: 0.444 - ETA: 0s - loss: 1.5390 - acc: 0.438 - ETA: 0s - loss: 1.5301 - acc: 0.439 - ETA: 0s - loss: 1.5263 - acc: 0.443 - ETA: 0s - loss: 1.5281 - acc: 0.441 - ETA: 0s - loss: 1.5324 - acc: 0.443 - ETA: 0s - loss: 1.5283 - acc: 0.443 - ETA: 0s - loss: 1.5380 - acc: 0.439 - ETA: 0s - loss: 1.5447 - acc: 0.440 - ETA: 0s - loss: 1.5460 - acc: 0.441 - ETA: 0s - loss: 1.5435 - acc: 0.443 - ETA: 0s - loss: 1.5478 - acc: 0.440 - ETA: 0s - loss: 1.5514 - acc: 0.438 - ETA: 0s - loss: 1.5594 - acc: 0.434 - ETA: 0s - loss: 1.5621 - acc: 0.432 - ETA: 0s - loss: 1.5632 - acc: 0.431 - 1s 242us/step - loss: 1.5642 - acc: 0.4304\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7049 - acc: 0.375 - ETA: 1s - loss: 1.5169 - acc: 0.429 - ETA: 1s - loss: 1.5032 - acc: 0.447 - ETA: 1s - loss: 1.5397 - acc: 0.436 - ETA: 1s - loss: 1.5479 - acc: 0.437 - ETA: 1s - loss: 1.5464 - acc: 0.440 - ETA: 1s - loss: 1.5519 - acc: 0.434 - ETA: 0s - loss: 1.5556 - acc: 0.437 - ETA: 0s - loss: 1.5544 - acc: 0.446 - ETA: 0s - loss: 1.5492 - acc: 0.445 - ETA: 0s - loss: 1.5562 - acc: 0.444 - ETA: 0s - loss: 1.5530 - acc: 0.447 - ETA: 0s - loss: 1.5526 - acc: 0.444 - ETA: 0s - loss: 1.5603 - acc: 0.440 - ETA: 0s - loss: 1.5588 - acc: 0.437 - ETA: 0s - loss: 1.5535 - acc: 0.436 - ETA: 0s - loss: 1.5555 - acc: 0.432 - ETA: 0s - loss: 1.5536 - acc: 0.432 - ETA: 0s - loss: 1.5462 - acc: 0.435 - ETA: 0s - loss: 1.5389 - acc: 0.439 - ETA: 0s - loss: 1.5330 - acc: 0.441 - ETA: 0s - loss: 1.5340 - acc: 0.442 - ETA: 0s - loss: 1.5349 - acc: 0.440 - 1s 240us/step - loss: 1.5323 - acc: 0.4414\n",
      "Epoch 50/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5977 - acc: 0.500 - ETA: 1s - loss: 1.4502 - acc: 0.496 - ETA: 1s - loss: 1.4947 - acc: 0.462 - ETA: 1s - loss: 1.5164 - acc: 0.450 - ETA: 1s - loss: 1.5120 - acc: 0.453 - ETA: 1s - loss: 1.5220 - acc: 0.450 - ETA: 1s - loss: 1.5382 - acc: 0.442 - ETA: 1s - loss: 1.5366 - acc: 0.440 - ETA: 0s - loss: 1.5331 - acc: 0.440 - ETA: 0s - loss: 1.5410 - acc: 0.438 - ETA: 0s - loss: 1.5505 - acc: 0.436 - ETA: 0s - loss: 1.5482 - acc: 0.434 - ETA: 0s - loss: 1.5516 - acc: 0.433 - ETA: 0s - loss: 1.5511 - acc: 0.432 - ETA: 0s - loss: 1.5507 - acc: 0.431 - ETA: 0s - loss: 1.5514 - acc: 0.431 - ETA: 0s - loss: 1.5574 - acc: 0.428 - ETA: 0s - loss: 1.5652 - acc: 0.425 - ETA: 0s - loss: 1.5652 - acc: 0.424 - ETA: 0s - loss: 1.5696 - acc: 0.424 - ETA: 0s - loss: 1.5643 - acc: 0.426 - ETA: 0s - loss: 1.5653 - acc: 0.426 - ETA: 0s - loss: 1.5590 - acc: 0.429 - ETA: 0s - loss: 1.5503 - acc: 0.432 - ETA: 0s - loss: 1.5446 - acc: 0.435 - 1s 251us/step - loss: 1.5500 - acc: 0.4351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20999a3cb00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parser(row):\n",
    "    global test_error_count\n",
    "    global test_error_labels\n",
    "    path_to_wav_files = PATH_TO_TEST_AUDIO_FILES\n",
    "    file_path = path_to_wav_files + str(row.ID) + \".wav\"\n",
    "    try:\n",
    "        data, sampling_rate = librosa.load(file_path)\n",
    "        stft = np.abs(librosa.stft(data))\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sampling_rate).T,axis=0)\n",
    "    except Exception as ex:\n",
    "        test_error_count += 1\n",
    "        test_error_labels.append(row.ID)\n",
    "        return pd.Series([0]*7)\n",
    "    features = contrast\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1f0577ba39445fb27c44f82f596108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3297), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 samples had errors while parsing\n",
      "Errorneous samples []\n"
     ]
    }
   ],
   "source": [
    "test_features = test.progress_apply(test_parser,axis=1, reduce = True)\n",
    "print(\"%d samples had errors while parsing\" % test_error_count)\n",
    "print(\"Errorneous samples\", test_error_labels)\n",
    "save_as_pickle(data=train_features,pickle_file=PATH_TO_PICKLE + SUBMISSION_TITLE + \" test.pickle\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_features\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "test_labels_strings = lb.inverse_transform(test_labels.argmax(axis=1))\n",
    "# test_labels_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Class'] = test_labels_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(PATH_TO_SUBMISSION + SUBMISSION_TITLE + \".csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gives 56% accuracy with the above setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
