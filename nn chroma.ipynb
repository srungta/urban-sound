{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First set up code to visualize a sound form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['display', 'load']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "from librosa import load, display\n",
    "import glob\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "import pickle\n",
    "from common import save_as_pickle\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should change these paths according to the path of the files on your system.\n",
    "PATH_TO_TRAIN_LABELS = \"data/train/train.csv\"\n",
    "PATH_TO_TEST_LABELS = \"data/test/test.csv\"\n",
    "PATH_TO_TRAIN_AUDIO_FILES = \"data/train/wav/\"\n",
    "PATH_TO_TEST_AUDIO_FILES = \"data/test/wav/\"\n",
    "PATH_TO_SUBMISSION = \"submission/\"\n",
    "PATH_TO_PICKLE = \"pickles/\"\n",
    "SUBMISSION_TITLE = \"nn chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is easier to deal with csv if you can load it into a structure you can work with.\n",
    "# Pandas are the most convenient way to do that and are available with \n",
    "# inbuilt functionality to handle csv file.\n",
    "\n",
    "# Pandas assumes that the first row in your file is the header adn not the actual values.\n",
    "# This behavior can be overriden by passing header=None as a parameter.\n",
    "train = pd.read_csv(PATH_TO_TRAIN_LABELS)\n",
    "test = pd.read_csv(PATH_TO_TEST_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can reactivate this cell to make sure your model is working correctly in terms of dimensions.\n",
    "#train = train[:2]\n",
    "#test = test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_count = 0\n",
    "train_error_labels = []\n",
    "test_error_count = 0\n",
    "test_error_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start with classification, we first need to convert the wav sound files into a format we can work \n",
    "# with. It is easier to take the amplitude at each sampling point and use that \n",
    "# numeric value to form a feature vector.\n",
    "def train_parser(row):\n",
    "    global train_error_count\n",
    "    global train_error_labels\n",
    "    path_to_wav_files = PATH_TO_TRAIN_AUDIO_FILES\n",
    "    file_path = path_to_wav_files + str(row.ID) + \".wav\"\n",
    "    try:\n",
    "        data, sampling_rate = librosa.load(file_path)\n",
    "        stft = np.abs(librosa.stft(data))\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sampling_rate).T,axis=0)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        train_error_count += 1\n",
    "        train_error_labels.append(row.ID)\n",
    "        return [0]*12, row.Class\n",
    "    features = chroma\n",
    "    label = row.Class\n",
    "    return [features, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966caaf4750845de9046149aa8dbdb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5435), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\librosa\\core\\pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 samples had errors while parsing\n",
      "Errorneous samples []\n"
     ]
    }
   ],
   "source": [
    "# To create the training feature matrix, we can apply our parser to each training sample.\n",
    "train_features = train.progress_apply(train_parser,axis=1)\n",
    "print(\"%d samples had errors while parsing\" % train_error_count)\n",
    "print(\"Errorneous samples\", train_error_labels)\n",
    "save_as_pickle(data=train_features,pickle_file=PATH_TO_PICKLE + SUBMISSION_TITLE + \" train.pickle\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to singnify what they mean helps with documentation,\n",
    "# and also helps you keep track of them later on.\n",
    "train_features.columns = ['feature','label']\n",
    "# train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this library helps us convert string labels into easy to handle encoded labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_features.feature.tolist())\n",
    "Y = np.array(train_features.label.tolist())\n",
    "lb = LabelEncoder()\n",
    "# Since labels are categories they dont inherently have an order amongst themselves.\n",
    "# For example, Apples > oranges does not make any sense. So to madel such categorical \n",
    "# variables, we can convert them to one hot vectors.\n",
    "Y = to_categorical(lb.fit_transform(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_labels = Y.shape[1]\n",
    "filter_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, input_shape=(12,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(number_of_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 256)               3328      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 71,690\n",
      "Trainable params: 71,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics = ['accuracy'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5435/5435 [==============================] - ETA: 1:41 - loss: 2.2974 - acc: 0.093 - ETA: 13s - loss: 2.3241 - acc: 0.097 - ETA: 6s - loss: 2.3112 - acc: 0.1074 - ETA: 4s - loss: 2.3087 - acc: 0.106 - ETA: 3s - loss: 2.3064 - acc: 0.095 - ETA: 2s - loss: 2.2962 - acc: 0.100 - ETA: 2s - loss: 2.2852 - acc: 0.099 - ETA: 2s - loss: 2.2862 - acc: 0.097 - ETA: 1s - loss: 2.2818 - acc: 0.095 - ETA: 1s - loss: 2.2768 - acc: 0.099 - ETA: 1s - loss: 2.2753 - acc: 0.105 - ETA: 1s - loss: 2.2701 - acc: 0.106 - ETA: 0s - loss: 2.2724 - acc: 0.107 - ETA: 0s - loss: 2.2673 - acc: 0.109 - ETA: 0s - loss: 2.2648 - acc: 0.112 - ETA: 0s - loss: 2.2627 - acc: 0.115 - ETA: 0s - loss: 2.2606 - acc: 0.119 - ETA: 0s - loss: 2.2591 - acc: 0.121 - ETA: 0s - loss: 2.2587 - acc: 0.123 - ETA: 0s - loss: 2.2575 - acc: 0.125 - ETA: 0s - loss: 2.2527 - acc: 0.130 - ETA: 0s - loss: 2.2512 - acc: 0.133 - 2s 317us/step - loss: 2.2518 - acc: 0.1332\n",
      "Epoch 2/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 2.2718 - acc: 0.093 - ETA: 1s - loss: 2.1936 - acc: 0.203 - ETA: 0s - loss: 2.1888 - acc: 0.190 - ETA: 0s - loss: 2.1894 - acc: 0.188 - ETA: 0s - loss: 2.1713 - acc: 0.195 - ETA: 0s - loss: 2.1813 - acc: 0.189 - ETA: 0s - loss: 2.1830 - acc: 0.185 - ETA: 0s - loss: 2.1776 - acc: 0.184 - ETA: 0s - loss: 2.1730 - acc: 0.191 - ETA: 0s - loss: 2.1652 - acc: 0.197 - ETA: 0s - loss: 2.1659 - acc: 0.196 - ETA: 0s - loss: 2.1619 - acc: 0.196 - ETA: 0s - loss: 2.1559 - acc: 0.197 - ETA: 0s - loss: 2.1479 - acc: 0.200 - ETA: 0s - loss: 2.1419 - acc: 0.203 - ETA: 0s - loss: 2.1396 - acc: 0.203 - ETA: 0s - loss: 2.1346 - acc: 0.205 - ETA: 0s - loss: 2.1337 - acc: 0.207 - ETA: 0s - loss: 2.1262 - acc: 0.211 - ETA: 0s - loss: 2.1243 - acc: 0.213 - ETA: 0s - loss: 2.1193 - acc: 0.215 - ETA: 0s - loss: 2.1173 - acc: 0.216 - 1s 210us/step - loss: 2.1150 - acc: 0.2180\n",
      "Epoch 3/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 2.0356 - acc: 0.250 - ETA: 1s - loss: 1.9866 - acc: 0.243 - ETA: 0s - loss: 2.0164 - acc: 0.264 - ETA: 0s - loss: 2.0259 - acc: 0.247 - ETA: 0s - loss: 2.0342 - acc: 0.243 - ETA: 0s - loss: 2.0203 - acc: 0.250 - ETA: 0s - loss: 2.0202 - acc: 0.245 - ETA: 0s - loss: 2.0183 - acc: 0.242 - ETA: 0s - loss: 2.0207 - acc: 0.244 - ETA: 0s - loss: 2.0247 - acc: 0.240 - ETA: 0s - loss: 2.0205 - acc: 0.244 - ETA: 0s - loss: 2.0174 - acc: 0.246 - ETA: 0s - loss: 2.0203 - acc: 0.245 - ETA: 0s - loss: 2.0162 - acc: 0.246 - ETA: 0s - loss: 2.0074 - acc: 0.250 - ETA: 0s - loss: 2.0050 - acc: 0.251 - ETA: 0s - loss: 2.0047 - acc: 0.250 - ETA: 0s - loss: 2.0024 - acc: 0.247 - ETA: 0s - loss: 2.0004 - acc: 0.248 - ETA: 0s - loss: 1.9992 - acc: 0.249 - ETA: 0s - loss: 1.9978 - acc: 0.249 - ETA: 0s - loss: 1.9985 - acc: 0.249 - 1s 207us/step - loss: 1.9973 - acc: 0.2495\n",
      "Epoch 4/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9559 - acc: 0.281 - ETA: 1s - loss: 1.9630 - acc: 0.231 - ETA: 0s - loss: 1.9732 - acc: 0.235 - ETA: 0s - loss: 1.9723 - acc: 0.239 - ETA: 0s - loss: 1.9565 - acc: 0.245 - ETA: 0s - loss: 1.9597 - acc: 0.253 - ETA: 0s - loss: 1.9500 - acc: 0.253 - ETA: 0s - loss: 1.9470 - acc: 0.264 - ETA: 0s - loss: 1.9468 - acc: 0.268 - ETA: 0s - loss: 1.9365 - acc: 0.270 - ETA: 0s - loss: 1.9375 - acc: 0.271 - ETA: 0s - loss: 1.9408 - acc: 0.273 - ETA: 0s - loss: 1.9411 - acc: 0.275 - ETA: 0s - loss: 1.9423 - acc: 0.275 - ETA: 0s - loss: 1.9407 - acc: 0.274 - ETA: 0s - loss: 1.9378 - acc: 0.276 - ETA: 0s - loss: 1.9329 - acc: 0.277 - ETA: 0s - loss: 1.9350 - acc: 0.275 - ETA: 0s - loss: 1.9321 - acc: 0.276 - ETA: 0s - loss: 1.9302 - acc: 0.275 - ETA: 0s - loss: 1.9266 - acc: 0.276 - 1s 203us/step - loss: 1.9257 - acc: 0.2769\n",
      "Epoch 5/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6324 - acc: 0.406 - ETA: 1s - loss: 1.8680 - acc: 0.277 - ETA: 1s - loss: 1.8815 - acc: 0.288 - ETA: 0s - loss: 1.8814 - acc: 0.300 - ETA: 0s - loss: 1.9013 - acc: 0.286 - ETA: 0s - loss: 1.8986 - acc: 0.293 - ETA: 0s - loss: 1.8998 - acc: 0.295 - ETA: 0s - loss: 1.9038 - acc: 0.295 - ETA: 0s - loss: 1.9044 - acc: 0.296 - ETA: 0s - loss: 1.8961 - acc: 0.298 - ETA: 0s - loss: 1.8937 - acc: 0.295 - ETA: 0s - loss: 1.8959 - acc: 0.295 - ETA: 0s - loss: 1.8892 - acc: 0.296 - ETA: 0s - loss: 1.8825 - acc: 0.298 - ETA: 0s - loss: 1.8814 - acc: 0.298 - ETA: 0s - loss: 1.8789 - acc: 0.298 - ETA: 0s - loss: 1.8783 - acc: 0.296 - ETA: 0s - loss: 1.8775 - acc: 0.296 - ETA: 0s - loss: 1.8775 - acc: 0.296 - ETA: 0s - loss: 1.8801 - acc: 0.295 - ETA: 0s - loss: 1.8819 - acc: 0.292 - 1s 207us/step - loss: 1.8807 - acc: 0.2940\n",
      "Epoch 6/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.9116 - acc: 0.312 - ETA: 0s - loss: 1.8795 - acc: 0.337 - ETA: 0s - loss: 1.8780 - acc: 0.312 - ETA: 0s - loss: 1.8525 - acc: 0.332 - ETA: 0s - loss: 1.8495 - acc: 0.337 - ETA: 0s - loss: 1.8434 - acc: 0.325 - ETA: 0s - loss: 1.8497 - acc: 0.314 - ETA: 0s - loss: 1.8545 - acc: 0.311 - ETA: 0s - loss: 1.8557 - acc: 0.307 - ETA: 0s - loss: 1.8598 - acc: 0.305 - ETA: 0s - loss: 1.8604 - acc: 0.307 - ETA: 0s - loss: 1.8542 - acc: 0.307 - ETA: 0s - loss: 1.8482 - acc: 0.311 - ETA: 0s - loss: 1.8397 - acc: 0.311 - ETA: 0s - loss: 1.8460 - acc: 0.310 - ETA: 0s - loss: 1.8427 - acc: 0.310 - ETA: 0s - loss: 1.8398 - acc: 0.312 - ETA: 0s - loss: 1.8403 - acc: 0.312 - ETA: 0s - loss: 1.8377 - acc: 0.313 - ETA: 0s - loss: 1.8351 - acc: 0.313 - ETA: 0s - loss: 1.8384 - acc: 0.311 - 1s 199us/step - loss: 1.8382 - acc: 0.3115\n",
      "Epoch 7/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.7227 - acc: 0.312 - ETA: 0s - loss: 1.8581 - acc: 0.303 - ETA: 0s - loss: 1.8248 - acc: 0.294 - ETA: 0s - loss: 1.7885 - acc: 0.312 - ETA: 0s - loss: 1.7853 - acc: 0.314 - ETA: 0s - loss: 1.7985 - acc: 0.310 - ETA: 0s - loss: 1.7935 - acc: 0.318 - ETA: 0s - loss: 1.8013 - acc: 0.318 - ETA: 0s - loss: 1.7913 - acc: 0.323 - ETA: 0s - loss: 1.7987 - acc: 0.318 - ETA: 0s - loss: 1.8006 - acc: 0.320 - ETA: 0s - loss: 1.8001 - acc: 0.318 - ETA: 0s - loss: 1.8014 - acc: 0.318 - ETA: 0s - loss: 1.7991 - acc: 0.320 - ETA: 0s - loss: 1.7949 - acc: 0.319 - ETA: 0s - loss: 1.7958 - acc: 0.319 - ETA: 0s - loss: 1.7962 - acc: 0.319 - ETA: 0s - loss: 1.7984 - acc: 0.318 - ETA: 0s - loss: 1.8014 - acc: 0.319 - ETA: 0s - loss: 1.8012 - acc: 0.320 - 1s 195us/step - loss: 1.7973 - acc: 0.3216\n",
      "Epoch 8/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.6750 - acc: 0.343 - ETA: 1s - loss: 1.7671 - acc: 0.322 - ETA: 0s - loss: 1.7790 - acc: 0.336 - ETA: 0s - loss: 1.7706 - acc: 0.343 - ETA: 0s - loss: 1.7650 - acc: 0.332 - ETA: 0s - loss: 1.7720 - acc: 0.334 - ETA: 0s - loss: 1.7681 - acc: 0.332 - ETA: 0s - loss: 1.7607 - acc: 0.330 - ETA: 0s - loss: 1.7619 - acc: 0.329 - ETA: 0s - loss: 1.7612 - acc: 0.328 - ETA: 0s - loss: 1.7493 - acc: 0.336 - ETA: 0s - loss: 1.7523 - acc: 0.338 - ETA: 0s - loss: 1.7526 - acc: 0.336 - ETA: 0s - loss: 1.7529 - acc: 0.337 - ETA: 0s - loss: 1.7603 - acc: 0.335 - ETA: 0s - loss: 1.7591 - acc: 0.336 - ETA: 0s - loss: 1.7627 - acc: 0.333 - ETA: 0s - loss: 1.7602 - acc: 0.336 - ETA: 0s - loss: 1.7570 - acc: 0.338 - ETA: 0s - loss: 1.7568 - acc: 0.337 - 1s 194us/step - loss: 1.7549 - acc: 0.3389\n",
      "Epoch 9/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7196 - acc: 0.312 - ETA: 0s - loss: 1.8155 - acc: 0.306 - ETA: 0s - loss: 1.7867 - acc: 0.289 - ETA: 0s - loss: 1.7673 - acc: 0.311 - ETA: 0s - loss: 1.7567 - acc: 0.326 - ETA: 0s - loss: 1.7634 - acc: 0.328 - ETA: 0s - loss: 1.7559 - acc: 0.330 - ETA: 0s - loss: 1.7497 - acc: 0.338 - ETA: 0s - loss: 1.7382 - acc: 0.343 - ETA: 0s - loss: 1.7285 - acc: 0.352 - ETA: 0s - loss: 1.7337 - acc: 0.348 - ETA: 0s - loss: 1.7324 - acc: 0.350 - ETA: 0s - loss: 1.7294 - acc: 0.352 - ETA: 0s - loss: 1.7267 - acc: 0.350 - ETA: 0s - loss: 1.7232 - acc: 0.351 - ETA: 0s - loss: 1.7199 - acc: 0.353 - ETA: 0s - loss: 1.7154 - acc: 0.353 - ETA: 0s - loss: 1.7149 - acc: 0.352 - ETA: 0s - loss: 1.7121 - acc: 0.354 - ETA: 0s - loss: 1.7168 - acc: 0.350 - 1s 198us/step - loss: 1.7189 - acc: 0.3500\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6275 - acc: 0.406 - ETA: 1s - loss: 1.7258 - acc: 0.350 - ETA: 0s - loss: 1.7384 - acc: 0.358 - ETA: 0s - loss: 1.7082 - acc: 0.346 - ETA: 0s - loss: 1.7135 - acc: 0.342 - ETA: 0s - loss: 1.6958 - acc: 0.348 - ETA: 0s - loss: 1.7162 - acc: 0.341 - ETA: 0s - loss: 1.7134 - acc: 0.347 - ETA: 0s - loss: 1.7101 - acc: 0.343 - ETA: 0s - loss: 1.7098 - acc: 0.349 - ETA: 0s - loss: 1.7097 - acc: 0.348 - ETA: 0s - loss: 1.7088 - acc: 0.347 - ETA: 0s - loss: 1.7097 - acc: 0.349 - ETA: 0s - loss: 1.7135 - acc: 0.351 - ETA: 0s - loss: 1.7137 - acc: 0.350 - ETA: 0s - loss: 1.7092 - acc: 0.355 - ETA: 0s - loss: 1.7057 - acc: 0.356 - ETA: 0s - loss: 1.7019 - acc: 0.360 - ETA: 0s - loss: 1.7032 - acc: 0.363 - ETA: 0s - loss: 1.7031 - acc: 0.363 - ETA: 0s - loss: 1.7019 - acc: 0.363 - 1s 202us/step - loss: 1.6971 - acc: 0.3663\n",
      "Epoch 11/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8280 - acc: 0.312 - ETA: 1s - loss: 1.6605 - acc: 0.334 - ETA: 1s - loss: 1.6571 - acc: 0.335 - ETA: 1s - loss: 1.6483 - acc: 0.352 - ETA: 0s - loss: 1.6846 - acc: 0.348 - ETA: 0s - loss: 1.6854 - acc: 0.355 - ETA: 0s - loss: 1.6752 - acc: 0.366 - ETA: 0s - loss: 1.6700 - acc: 0.369 - ETA: 0s - loss: 1.6596 - acc: 0.377 - ETA: 0s - loss: 1.6628 - acc: 0.373 - ETA: 0s - loss: 1.6570 - acc: 0.375 - ETA: 0s - loss: 1.6652 - acc: 0.369 - ETA: 0s - loss: 1.6663 - acc: 0.371 - ETA: 0s - loss: 1.6701 - acc: 0.374 - ETA: 0s - loss: 1.6712 - acc: 0.379 - ETA: 0s - loss: 1.6690 - acc: 0.379 - ETA: 0s - loss: 1.6700 - acc: 0.379 - ETA: 0s - loss: 1.6745 - acc: 0.378 - ETA: 0s - loss: 1.6752 - acc: 0.378 - ETA: 0s - loss: 1.6788 - acc: 0.376 - ETA: 0s - loss: 1.6805 - acc: 0.374 - ETA: 0s - loss: 1.6815 - acc: 0.372 - 1s 218us/step - loss: 1.6806 - acc: 0.3720\n",
      "Epoch 12/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7063 - acc: 0.468 - ETA: 1s - loss: 1.7179 - acc: 0.382 - ETA: 1s - loss: 1.7018 - acc: 0.367 - ETA: 1s - loss: 1.6966 - acc: 0.365 - ETA: 1s - loss: 1.6864 - acc: 0.364 - ETA: 0s - loss: 1.6875 - acc: 0.367 - ETA: 0s - loss: 1.6835 - acc: 0.372 - ETA: 0s - loss: 1.6742 - acc: 0.380 - ETA: 0s - loss: 1.6615 - acc: 0.384 - ETA: 0s - loss: 1.6612 - acc: 0.384 - ETA: 0s - loss: 1.6603 - acc: 0.377 - ETA: 0s - loss: 1.6510 - acc: 0.380 - ETA: 0s - loss: 1.6535 - acc: 0.377 - ETA: 0s - loss: 1.6465 - acc: 0.380 - ETA: 0s - loss: 1.6443 - acc: 0.380 - ETA: 0s - loss: 1.6476 - acc: 0.381 - ETA: 0s - loss: 1.6500 - acc: 0.380 - ETA: 0s - loss: 1.6487 - acc: 0.378 - ETA: 0s - loss: 1.6463 - acc: 0.381 - ETA: 0s - loss: 1.6532 - acc: 0.379 - ETA: 0s - loss: 1.6497 - acc: 0.380 - ETA: 0s - loss: 1.6501 - acc: 0.382 - 1s 219us/step - loss: 1.6524 - acc: 0.3809\n",
      "Epoch 13/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.8005 - acc: 0.406 - ETA: 0s - loss: 1.6796 - acc: 0.377 - ETA: 0s - loss: 1.6744 - acc: 0.370 - ETA: 0s - loss: 1.6549 - acc: 0.363 - ETA: 0s - loss: 1.6512 - acc: 0.373 - ETA: 0s - loss: 1.6380 - acc: 0.386 - ETA: 0s - loss: 1.6313 - acc: 0.386 - ETA: 0s - loss: 1.6214 - acc: 0.391 - ETA: 0s - loss: 1.6188 - acc: 0.390 - ETA: 0s - loss: 1.6252 - acc: 0.388 - ETA: 0s - loss: 1.6303 - acc: 0.387 - ETA: 0s - loss: 1.6339 - acc: 0.385 - ETA: 0s - loss: 1.6385 - acc: 0.385 - ETA: 0s - loss: 1.6363 - acc: 0.388 - ETA: 0s - loss: 1.6391 - acc: 0.385 - ETA: 0s - loss: 1.6374 - acc: 0.384 - ETA: 0s - loss: 1.6369 - acc: 0.385 - ETA: 0s - loss: 1.6316 - acc: 0.388 - ETA: 0s - loss: 1.6343 - acc: 0.384 - ETA: 0s - loss: 1.6368 - acc: 0.386 - ETA: 0s - loss: 1.6302 - acc: 0.387 - 1s 203us/step - loss: 1.6270 - acc: 0.3890\n",
      "Epoch 14/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.4420 - acc: 0.500 - ETA: 0s - loss: 1.5818 - acc: 0.403 - ETA: 0s - loss: 1.5884 - acc: 0.389 - ETA: 0s - loss: 1.6095 - acc: 0.390 - ETA: 0s - loss: 1.6099 - acc: 0.393 - ETA: 0s - loss: 1.6040 - acc: 0.395 - ETA: 0s - loss: 1.6168 - acc: 0.394 - ETA: 0s - loss: 1.6145 - acc: 0.393 - ETA: 0s - loss: 1.6014 - acc: 0.401 - ETA: 0s - loss: 1.6037 - acc: 0.398 - ETA: 0s - loss: 1.6098 - acc: 0.401 - ETA: 0s - loss: 1.6183 - acc: 0.397 - ETA: 0s - loss: 1.6181 - acc: 0.398 - ETA: 0s - loss: 1.6222 - acc: 0.399 - ETA: 0s - loss: 1.6239 - acc: 0.397 - ETA: 0s - loss: 1.6259 - acc: 0.395 - ETA: 0s - loss: 1.6194 - acc: 0.398 - ETA: 0s - loss: 1.6204 - acc: 0.398 - ETA: 0s - loss: 1.6221 - acc: 0.394 - ETA: 0s - loss: 1.6222 - acc: 0.394 - ETA: 0s - loss: 1.6231 - acc: 0.394 - ETA: 0s - loss: 1.6213 - acc: 0.394 - 1s 210us/step - loss: 1.6210 - acc: 0.3941\n",
      "Epoch 15/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6140 - acc: 0.343 - ETA: 1s - loss: 1.5636 - acc: 0.375 - ETA: 1s - loss: 1.5924 - acc: 0.387 - ETA: 1s - loss: 1.6206 - acc: 0.377 - ETA: 0s - loss: 1.6286 - acc: 0.382 - ETA: 0s - loss: 1.6276 - acc: 0.382 - ETA: 0s - loss: 1.6330 - acc: 0.375 - ETA: 0s - loss: 1.6388 - acc: 0.382 - ETA: 0s - loss: 1.6197 - acc: 0.390 - ETA: 0s - loss: 1.6142 - acc: 0.398 - ETA: 0s - loss: 1.6078 - acc: 0.397 - ETA: 0s - loss: 1.6091 - acc: 0.398 - ETA: 0s - loss: 1.6039 - acc: 0.397 - ETA: 0s - loss: 1.6044 - acc: 0.396 - ETA: 0s - loss: 1.6112 - acc: 0.393 - ETA: 0s - loss: 1.6047 - acc: 0.395 - ETA: 0s - loss: 1.6029 - acc: 0.396 - ETA: 0s - loss: 1.6012 - acc: 0.398 - ETA: 0s - loss: 1.5998 - acc: 0.399 - ETA: 0s - loss: 1.6002 - acc: 0.399 - ETA: 0s - loss: 1.5983 - acc: 0.399 - ETA: 0s - loss: 1.6066 - acc: 0.396 - 1s 215us/step - loss: 1.6053 - acc: 0.3958\n",
      "Epoch 16/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.4441 - acc: 0.500 - ETA: 0s - loss: 1.6237 - acc: 0.428 - ETA: 0s - loss: 1.6127 - acc: 0.429 - ETA: 0s - loss: 1.5967 - acc: 0.423 - ETA: 0s - loss: 1.5948 - acc: 0.419 - ETA: 0s - loss: 1.5951 - acc: 0.416 - ETA: 0s - loss: 1.6020 - acc: 0.415 - ETA: 0s - loss: 1.6011 - acc: 0.419 - ETA: 0s - loss: 1.6085 - acc: 0.415 - ETA: 0s - loss: 1.6197 - acc: 0.409 - ETA: 0s - loss: 1.6224 - acc: 0.408 - ETA: 0s - loss: 1.6188 - acc: 0.407 - ETA: 0s - loss: 1.6184 - acc: 0.407 - ETA: 0s - loss: 1.6220 - acc: 0.406 - ETA: 0s - loss: 1.6198 - acc: 0.407 - ETA: 0s - loss: 1.6150 - acc: 0.409 - ETA: 0s - loss: 1.6201 - acc: 0.407 - ETA: 0s - loss: 1.6153 - acc: 0.410 - ETA: 0s - loss: 1.6091 - acc: 0.414 - ETA: 0s - loss: 1.6023 - acc: 0.416 - ETA: 0s - loss: 1.5983 - acc: 0.415 - ETA: 0s - loss: 1.5974 - acc: 0.417 - ETA: 0s - loss: 1.6022 - acc: 0.416 - 1s 226us/step - loss: 1.6025 - acc: 0.4158\n",
      "Epoch 17/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.4052 - acc: 0.406 - ETA: 1s - loss: 1.6764 - acc: 0.371 - ETA: 1s - loss: 1.6163 - acc: 0.394 - ETA: 1s - loss: 1.6121 - acc: 0.400 - ETA: 0s - loss: 1.5975 - acc: 0.401 - ETA: 0s - loss: 1.6080 - acc: 0.400 - ETA: 0s - loss: 1.6013 - acc: 0.394 - ETA: 0s - loss: 1.6073 - acc: 0.395 - ETA: 0s - loss: 1.6015 - acc: 0.397 - ETA: 0s - loss: 1.5984 - acc: 0.397 - ETA: 0s - loss: 1.5907 - acc: 0.402 - ETA: 0s - loss: 1.5996 - acc: 0.399 - ETA: 0s - loss: 1.6000 - acc: 0.399 - ETA: 0s - loss: 1.5917 - acc: 0.403 - ETA: 0s - loss: 1.5941 - acc: 0.401 - ETA: 0s - loss: 1.5920 - acc: 0.402 - ETA: 0s - loss: 1.5889 - acc: 0.404 - ETA: 0s - loss: 1.5917 - acc: 0.403 - ETA: 0s - loss: 1.5882 - acc: 0.402 - ETA: 0s - loss: 1.5857 - acc: 0.404 - ETA: 0s - loss: 1.5844 - acc: 0.404 - ETA: 0s - loss: 1.5869 - acc: 0.405 - 1s 220us/step - loss: 1.5878 - acc: 0.4048\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4980 - acc: 0.375 - ETA: 1s - loss: 1.6220 - acc: 0.371 - ETA: 1s - loss: 1.5801 - acc: 0.408 - ETA: 1s - loss: 1.5824 - acc: 0.411 - ETA: 0s - loss: 1.6124 - acc: 0.405 - ETA: 0s - loss: 1.5851 - acc: 0.418 - ETA: 0s - loss: 1.5738 - acc: 0.419 - ETA: 0s - loss: 1.5763 - acc: 0.420 - ETA: 0s - loss: 1.5681 - acc: 0.426 - ETA: 0s - loss: 1.5690 - acc: 0.429 - ETA: 0s - loss: 1.5687 - acc: 0.428 - ETA: 0s - loss: 1.5674 - acc: 0.425 - ETA: 0s - loss: 1.5549 - acc: 0.429 - ETA: 0s - loss: 1.5486 - acc: 0.430 - ETA: 0s - loss: 1.5524 - acc: 0.426 - ETA: 0s - loss: 1.5495 - acc: 0.428 - ETA: 0s - loss: 1.5517 - acc: 0.427 - ETA: 0s - loss: 1.5469 - acc: 0.428 - ETA: 0s - loss: 1.5452 - acc: 0.428 - ETA: 0s - loss: 1.5536 - acc: 0.425 - ETA: 0s - loss: 1.5506 - acc: 0.427 - ETA: 0s - loss: 1.5491 - acc: 0.427 - ETA: 0s - loss: 1.5547 - acc: 0.425 - ETA: 0s - loss: 1.5557 - acc: 0.425 - 1s 239us/step - loss: 1.5552 - acc: 0.4267\n",
      "Epoch 19/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.4113 - acc: 0.531 - ETA: 1s - loss: 1.5905 - acc: 0.419 - ETA: 1s - loss: 1.5731 - acc: 0.442 - ETA: 1s - loss: 1.5598 - acc: 0.446 - ETA: 1s - loss: 1.5867 - acc: 0.432 - ETA: 1s - loss: 1.5651 - acc: 0.440 - ETA: 1s - loss: 1.5564 - acc: 0.437 - ETA: 1s - loss: 1.5578 - acc: 0.441 - ETA: 0s - loss: 1.5405 - acc: 0.443 - ETA: 0s - loss: 1.5500 - acc: 0.442 - ETA: 0s - loss: 1.5440 - acc: 0.447 - ETA: 0s - loss: 1.5464 - acc: 0.443 - ETA: 0s - loss: 1.5459 - acc: 0.445 - ETA: 0s - loss: 1.5425 - acc: 0.446 - ETA: 0s - loss: 1.5393 - acc: 0.447 - ETA: 0s - loss: 1.5443 - acc: 0.444 - ETA: 0s - loss: 1.5506 - acc: 0.441 - ETA: 0s - loss: 1.5559 - acc: 0.435 - ETA: 0s - loss: 1.5522 - acc: 0.436 - ETA: 0s - loss: 1.5558 - acc: 0.434 - ETA: 0s - loss: 1.5568 - acc: 0.433 - ETA: 0s - loss: 1.5582 - acc: 0.430 - ETA: 0s - loss: 1.5622 - acc: 0.428 - ETA: 0s - loss: 1.5560 - acc: 0.432 - ETA: 0s - loss: 1.5579 - acc: 0.429 - 1s 247us/step - loss: 1.5570 - acc: 0.4300\n",
      "Epoch 20/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4569 - acc: 0.406 - ETA: 1s - loss: 1.5716 - acc: 0.396 - ETA: 0s - loss: 1.5614 - acc: 0.425 - ETA: 0s - loss: 1.5298 - acc: 0.441 - ETA: 0s - loss: 1.5519 - acc: 0.427 - ETA: 0s - loss: 1.5317 - acc: 0.427 - ETA: 0s - loss: 1.5337 - acc: 0.426 - ETA: 0s - loss: 1.5402 - acc: 0.425 - ETA: 0s - loss: 1.5347 - acc: 0.427 - ETA: 0s - loss: 1.5282 - acc: 0.429 - ETA: 0s - loss: 1.5369 - acc: 0.428 - ETA: 0s - loss: 1.5379 - acc: 0.429 - ETA: 0s - loss: 1.5416 - acc: 0.428 - ETA: 0s - loss: 1.5444 - acc: 0.426 - ETA: 0s - loss: 1.5456 - acc: 0.425 - ETA: 0s - loss: 1.5471 - acc: 0.426 - ETA: 0s - loss: 1.5511 - acc: 0.424 - ETA: 0s - loss: 1.5513 - acc: 0.424 - ETA: 0s - loss: 1.5470 - acc: 0.424 - ETA: 0s - loss: 1.5485 - acc: 0.423 - ETA: 0s - loss: 1.5477 - acc: 0.423 - ETA: 0s - loss: 1.5485 - acc: 0.423 - 1s 226us/step - loss: 1.5483 - acc: 0.4243\n",
      "Epoch 21/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.4811 - acc: 0.531 - ETA: 1s - loss: 1.5547 - acc: 0.414 - ETA: 1s - loss: 1.5126 - acc: 0.433 - ETA: 1s - loss: 1.5044 - acc: 0.436 - ETA: 1s - loss: 1.5276 - acc: 0.422 - ETA: 0s - loss: 1.5383 - acc: 0.411 - ETA: 0s - loss: 1.5419 - acc: 0.413 - ETA: 0s - loss: 1.5393 - acc: 0.416 - ETA: 0s - loss: 1.5373 - acc: 0.423 - ETA: 0s - loss: 1.5365 - acc: 0.424 - ETA: 0s - loss: 1.5384 - acc: 0.426 - ETA: 0s - loss: 1.5345 - acc: 0.425 - ETA: 0s - loss: 1.5377 - acc: 0.421 - ETA: 0s - loss: 1.5376 - acc: 0.421 - ETA: 0s - loss: 1.5427 - acc: 0.419 - ETA: 0s - loss: 1.5482 - acc: 0.418 - ETA: 0s - loss: 1.5464 - acc: 0.419 - ETA: 0s - loss: 1.5471 - acc: 0.422 - ETA: 0s - loss: 1.5470 - acc: 0.422 - ETA: 0s - loss: 1.5419 - acc: 0.425 - ETA: 0s - loss: 1.5382 - acc: 0.428 - ETA: 0s - loss: 1.5368 - acc: 0.430 - ETA: 0s - loss: 1.5361 - acc: 0.431 - 1s 234us/step - loss: 1.5357 - acc: 0.4316\n",
      "Epoch 22/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4719 - acc: 0.500 - ETA: 1s - loss: 1.5915 - acc: 0.390 - ETA: 1s - loss: 1.5904 - acc: 0.402 - ETA: 1s - loss: 1.5617 - acc: 0.414 - ETA: 0s - loss: 1.5606 - acc: 0.417 - ETA: 0s - loss: 1.5473 - acc: 0.420 - ETA: 0s - loss: 1.5513 - acc: 0.422 - ETA: 0s - loss: 1.5555 - acc: 0.422 - ETA: 0s - loss: 1.5417 - acc: 0.427 - ETA: 0s - loss: 1.5413 - acc: 0.429 - ETA: 0s - loss: 1.5461 - acc: 0.428 - ETA: 0s - loss: 1.5371 - acc: 0.433 - ETA: 0s - loss: 1.5462 - acc: 0.428 - ETA: 0s - loss: 1.5496 - acc: 0.428 - ETA: 0s - loss: 1.5486 - acc: 0.429 - ETA: 0s - loss: 1.5465 - acc: 0.429 - ETA: 0s - loss: 1.5428 - acc: 0.431 - ETA: 0s - loss: 1.5416 - acc: 0.431 - ETA: 0s - loss: 1.5395 - acc: 0.432 - ETA: 0s - loss: 1.5383 - acc: 0.433 - ETA: 0s - loss: 1.5326 - acc: 0.432 - ETA: 0s - loss: 1.5334 - acc: 0.430 - 1s 218us/step - loss: 1.5340 - acc: 0.4311\n",
      "Epoch 23/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6226 - acc: 0.406 - ETA: 1s - loss: 1.5738 - acc: 0.434 - ETA: 1s - loss: 1.5936 - acc: 0.432 - ETA: 0s - loss: 1.5265 - acc: 0.450 - ETA: 0s - loss: 1.5208 - acc: 0.443 - ETA: 0s - loss: 1.5268 - acc: 0.444 - ETA: 0s - loss: 1.5308 - acc: 0.442 - ETA: 0s - loss: 1.5374 - acc: 0.438 - ETA: 0s - loss: 1.5372 - acc: 0.437 - ETA: 0s - loss: 1.5393 - acc: 0.436 - ETA: 0s - loss: 1.5330 - acc: 0.435 - ETA: 0s - loss: 1.5260 - acc: 0.437 - ETA: 0s - loss: 1.5233 - acc: 0.437 - ETA: 0s - loss: 1.5130 - acc: 0.438 - ETA: 0s - loss: 1.5055 - acc: 0.442 - ETA: 0s - loss: 1.5023 - acc: 0.439 - ETA: 0s - loss: 1.5020 - acc: 0.437 - ETA: 0s - loss: 1.5072 - acc: 0.436 - ETA: 0s - loss: 1.5120 - acc: 0.435 - ETA: 0s - loss: 1.5156 - acc: 0.435 - ETA: 0s - loss: 1.5159 - acc: 0.436 - ETA: 0s - loss: 1.5152 - acc: 0.439 - ETA: 0s - loss: 1.5162 - acc: 0.440 - 1s 222us/step - loss: 1.5174 - acc: 0.4397\n",
      "Epoch 24/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5807 - acc: 0.437 - ETA: 1s - loss: 1.5527 - acc: 0.423 - ETA: 1s - loss: 1.5434 - acc: 0.441 - ETA: 1s - loss: 1.5851 - acc: 0.427 - ETA: 0s - loss: 1.5643 - acc: 0.433 - ETA: 0s - loss: 1.5424 - acc: 0.435 - ETA: 0s - loss: 1.5380 - acc: 0.438 - ETA: 0s - loss: 1.5227 - acc: 0.439 - ETA: 0s - loss: 1.5237 - acc: 0.437 - ETA: 0s - loss: 1.5107 - acc: 0.445 - ETA: 0s - loss: 1.5081 - acc: 0.447 - ETA: 0s - loss: 1.5044 - acc: 0.448 - ETA: 0s - loss: 1.5085 - acc: 0.446 - ETA: 0s - loss: 1.5032 - acc: 0.447 - ETA: 0s - loss: 1.5038 - acc: 0.450 - ETA: 0s - loss: 1.5105 - acc: 0.446 - ETA: 0s - loss: 1.5053 - acc: 0.446 - ETA: 0s - loss: 1.5022 - acc: 0.446 - ETA: 0s - loss: 1.5040 - acc: 0.445 - ETA: 0s - loss: 1.5011 - acc: 0.447 - ETA: 0s - loss: 1.5032 - acc: 0.448 - ETA: 0s - loss: 1.5043 - acc: 0.447 - ETA: 0s - loss: 1.5074 - acc: 0.446 - 1s 231us/step - loss: 1.5053 - acc: 0.4471\n",
      "Epoch 25/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3900 - acc: 0.437 - ETA: 1s - loss: 1.5470 - acc: 0.406 - ETA: 1s - loss: 1.5559 - acc: 0.417 - ETA: 1s - loss: 1.5240 - acc: 0.441 - ETA: 1s - loss: 1.5562 - acc: 0.432 - ETA: 0s - loss: 1.5402 - acc: 0.430 - ETA: 0s - loss: 1.5382 - acc: 0.432 - ETA: 0s - loss: 1.5287 - acc: 0.432 - ETA: 0s - loss: 1.5304 - acc: 0.429 - ETA: 0s - loss: 1.5300 - acc: 0.431 - ETA: 0s - loss: 1.5210 - acc: 0.436 - ETA: 0s - loss: 1.5118 - acc: 0.438 - ETA: 0s - loss: 1.5112 - acc: 0.440 - ETA: 0s - loss: 1.5064 - acc: 0.443 - ETA: 0s - loss: 1.5071 - acc: 0.444 - ETA: 0s - loss: 1.5053 - acc: 0.445 - ETA: 0s - loss: 1.5033 - acc: 0.446 - ETA: 0s - loss: 1.5046 - acc: 0.444 - ETA: 0s - loss: 1.5059 - acc: 0.445 - ETA: 0s - loss: 1.4998 - acc: 0.446 - ETA: 0s - loss: 1.4983 - acc: 0.446 - ETA: 0s - loss: 1.4968 - acc: 0.445 - 1s 217us/step - loss: 1.4978 - acc: 0.4449\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4154 - acc: 0.312 - ETA: 0s - loss: 1.4772 - acc: 0.457 - ETA: 0s - loss: 1.4865 - acc: 0.444 - ETA: 0s - loss: 1.5185 - acc: 0.441 - ETA: 0s - loss: 1.5057 - acc: 0.445 - ETA: 0s - loss: 1.5180 - acc: 0.447 - ETA: 0s - loss: 1.5097 - acc: 0.453 - ETA: 0s - loss: 1.5126 - acc: 0.445 - ETA: 0s - loss: 1.5017 - acc: 0.451 - ETA: 0s - loss: 1.4965 - acc: 0.451 - ETA: 0s - loss: 1.4880 - acc: 0.452 - ETA: 0s - loss: 1.4930 - acc: 0.453 - ETA: 0s - loss: 1.4887 - acc: 0.455 - ETA: 0s - loss: 1.4985 - acc: 0.452 - ETA: 0s - loss: 1.4957 - acc: 0.451 - ETA: 0s - loss: 1.4933 - acc: 0.454 - ETA: 0s - loss: 1.4933 - acc: 0.454 - ETA: 0s - loss: 1.4908 - acc: 0.458 - ETA: 0s - loss: 1.4887 - acc: 0.460 - ETA: 0s - loss: 1.4896 - acc: 0.460 - ETA: 0s - loss: 1.4888 - acc: 0.459 - ETA: 0s - loss: 1.4915 - acc: 0.459 - 1s 220us/step - loss: 1.4918 - acc: 0.4600\n",
      "Epoch 27/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.3100 - acc: 0.625 - ETA: 1s - loss: 1.4697 - acc: 0.500 - ETA: 1s - loss: 1.4579 - acc: 0.496 - ETA: 1s - loss: 1.4811 - acc: 0.468 - ETA: 1s - loss: 1.4854 - acc: 0.452 - ETA: 0s - loss: 1.5094 - acc: 0.449 - ETA: 0s - loss: 1.5109 - acc: 0.452 - ETA: 0s - loss: 1.5217 - acc: 0.446 - ETA: 0s - loss: 1.5163 - acc: 0.449 - ETA: 0s - loss: 1.5154 - acc: 0.448 - ETA: 0s - loss: 1.5049 - acc: 0.450 - ETA: 0s - loss: 1.4992 - acc: 0.456 - ETA: 0s - loss: 1.4977 - acc: 0.454 - ETA: 0s - loss: 1.4966 - acc: 0.452 - ETA: 0s - loss: 1.5009 - acc: 0.450 - ETA: 0s - loss: 1.4961 - acc: 0.452 - ETA: 0s - loss: 1.4931 - acc: 0.453 - ETA: 0s - loss: 1.4918 - acc: 0.454 - ETA: 0s - loss: 1.4883 - acc: 0.454 - ETA: 0s - loss: 1.4880 - acc: 0.453 - ETA: 0s - loss: 1.4856 - acc: 0.453 - ETA: 0s - loss: 1.4855 - acc: 0.454 - ETA: 0s - loss: 1.4857 - acc: 0.453 - 1s 232us/step - loss: 1.4873 - acc: 0.4524\n",
      "Epoch 28/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3386 - acc: 0.500 - ETA: 1s - loss: 1.4948 - acc: 0.413 - ETA: 1s - loss: 1.4928 - acc: 0.408 - ETA: 1s - loss: 1.4842 - acc: 0.428 - ETA: 0s - loss: 1.4950 - acc: 0.432 - ETA: 0s - loss: 1.4797 - acc: 0.443 - ETA: 0s - loss: 1.4797 - acc: 0.450 - ETA: 0s - loss: 1.4810 - acc: 0.452 - ETA: 0s - loss: 1.4845 - acc: 0.451 - ETA: 0s - loss: 1.4839 - acc: 0.452 - ETA: 0s - loss: 1.4877 - acc: 0.454 - ETA: 0s - loss: 1.4858 - acc: 0.457 - ETA: 0s - loss: 1.4821 - acc: 0.459 - ETA: 0s - loss: 1.4716 - acc: 0.465 - ETA: 0s - loss: 1.4722 - acc: 0.461 - ETA: 0s - loss: 1.4756 - acc: 0.458 - ETA: 0s - loss: 1.4756 - acc: 0.457 - ETA: 0s - loss: 1.4775 - acc: 0.458 - ETA: 0s - loss: 1.4744 - acc: 0.458 - ETA: 0s - loss: 1.4774 - acc: 0.456 - ETA: 0s - loss: 1.4725 - acc: 0.461 - ETA: 0s - loss: 1.4724 - acc: 0.460 - ETA: 0s - loss: 1.4668 - acc: 0.462 - 1s 229us/step - loss: 1.4670 - acc: 0.4629\n",
      "Epoch 29/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5658 - acc: 0.437 - ETA: 1s - loss: 1.4969 - acc: 0.449 - ETA: 1s - loss: 1.5307 - acc: 0.445 - ETA: 1s - loss: 1.5190 - acc: 0.440 - ETA: 1s - loss: 1.5168 - acc: 0.432 - ETA: 0s - loss: 1.5076 - acc: 0.443 - ETA: 0s - loss: 1.5137 - acc: 0.442 - ETA: 0s - loss: 1.5076 - acc: 0.437 - ETA: 0s - loss: 1.4846 - acc: 0.454 - ETA: 0s - loss: 1.4779 - acc: 0.457 - ETA: 0s - loss: 1.4763 - acc: 0.458 - ETA: 0s - loss: 1.4808 - acc: 0.456 - ETA: 0s - loss: 1.4845 - acc: 0.454 - ETA: 0s - loss: 1.4842 - acc: 0.453 - ETA: 0s - loss: 1.4836 - acc: 0.452 - ETA: 0s - loss: 1.4772 - acc: 0.452 - ETA: 0s - loss: 1.4863 - acc: 0.452 - ETA: 0s - loss: 1.4823 - acc: 0.455 - ETA: 0s - loss: 1.4730 - acc: 0.458 - ETA: 0s - loss: 1.4686 - acc: 0.459 - ETA: 0s - loss: 1.4646 - acc: 0.463 - ETA: 0s - loss: 1.4631 - acc: 0.465 - 1s 222us/step - loss: 1.4634 - acc: 0.4644\n",
      "Epoch 30/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4012 - acc: 0.562 - ETA: 1s - loss: 1.4341 - acc: 0.475 - ETA: 1s - loss: 1.4115 - acc: 0.475 - ETA: 1s - loss: 1.4164 - acc: 0.477 - ETA: 1s - loss: 1.4365 - acc: 0.465 - ETA: 1s - loss: 1.4329 - acc: 0.467 - ETA: 0s - loss: 1.4244 - acc: 0.466 - ETA: 0s - loss: 1.4282 - acc: 0.464 - ETA: 0s - loss: 1.4309 - acc: 0.467 - ETA: 0s - loss: 1.4357 - acc: 0.465 - ETA: 0s - loss: 1.4308 - acc: 0.467 - ETA: 0s - loss: 1.4351 - acc: 0.465 - ETA: 0s - loss: 1.4531 - acc: 0.461 - ETA: 0s - loss: 1.4584 - acc: 0.461 - ETA: 0s - loss: 1.4554 - acc: 0.464 - ETA: 0s - loss: 1.4622 - acc: 0.462 - ETA: 0s - loss: 1.4608 - acc: 0.463 - ETA: 0s - loss: 1.4644 - acc: 0.461 - ETA: 0s - loss: 1.4610 - acc: 0.461 - ETA: 0s - loss: 1.4556 - acc: 0.462 - ETA: 0s - loss: 1.4555 - acc: 0.461 - ETA: 0s - loss: 1.4601 - acc: 0.461 - ETA: 0s - loss: 1.4586 - acc: 0.462 - 1s 229us/step - loss: 1.4590 - acc: 0.4620\n",
      "Epoch 31/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.1765 - acc: 0.500 - ETA: 1s - loss: 1.4179 - acc: 0.456 - ETA: 1s - loss: 1.4214 - acc: 0.463 - ETA: 0s - loss: 1.4269 - acc: 0.464 - ETA: 0s - loss: 1.4241 - acc: 0.471 - ETA: 0s - loss: 1.4420 - acc: 0.462 - ETA: 0s - loss: 1.4506 - acc: 0.461 - ETA: 0s - loss: 1.4652 - acc: 0.454 - ETA: 0s - loss: 1.4637 - acc: 0.460 - ETA: 0s - loss: 1.4748 - acc: 0.455 - ETA: 0s - loss: 1.4667 - acc: 0.459 - ETA: 0s - loss: 1.4650 - acc: 0.459 - ETA: 0s - loss: 1.4631 - acc: 0.459 - ETA: 0s - loss: 1.4611 - acc: 0.457 - ETA: 0s - loss: 1.4608 - acc: 0.456 - ETA: 0s - loss: 1.4542 - acc: 0.460 - ETA: 0s - loss: 1.4528 - acc: 0.459 - ETA: 0s - loss: 1.4560 - acc: 0.460 - ETA: 0s - loss: 1.4537 - acc: 0.461 - ETA: 0s - loss: 1.4543 - acc: 0.460 - ETA: 0s - loss: 1.4527 - acc: 0.460 - ETA: 0s - loss: 1.4547 - acc: 0.461 - 1s 214us/step - loss: 1.4528 - acc: 0.4618\n",
      "Epoch 32/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5443 - acc: 0.500 - ETA: 0s - loss: 1.4154 - acc: 0.475 - ETA: 0s - loss: 1.3777 - acc: 0.508 - ETA: 0s - loss: 1.3970 - acc: 0.481 - ETA: 0s - loss: 1.3981 - acc: 0.482 - ETA: 0s - loss: 1.4044 - acc: 0.475 - ETA: 0s - loss: 1.4200 - acc: 0.470 - ETA: 0s - loss: 1.4252 - acc: 0.467 - ETA: 0s - loss: 1.4379 - acc: 0.464 - ETA: 0s - loss: 1.4423 - acc: 0.465 - ETA: 0s - loss: 1.4546 - acc: 0.460 - ETA: 0s - loss: 1.4494 - acc: 0.462 - ETA: 0s - loss: 1.4500 - acc: 0.462 - ETA: 0s - loss: 1.4460 - acc: 0.464 - ETA: 0s - loss: 1.4547 - acc: 0.460 - ETA: 0s - loss: 1.4609 - acc: 0.458 - ETA: 0s - loss: 1.4607 - acc: 0.457 - ETA: 0s - loss: 1.4665 - acc: 0.457 - ETA: 0s - loss: 1.4654 - acc: 0.459 - ETA: 0s - loss: 1.4657 - acc: 0.461 - ETA: 0s - loss: 1.4626 - acc: 0.464 - 1s 210us/step - loss: 1.4629 - acc: 0.4651\n",
      "Epoch 33/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.4232 - acc: 0.593 - ETA: 1s - loss: 1.3470 - acc: 0.531 - ETA: 1s - loss: 1.4005 - acc: 0.501 - ETA: 1s - loss: 1.4142 - acc: 0.483 - ETA: 0s - loss: 1.4261 - acc: 0.473 - ETA: 0s - loss: 1.4231 - acc: 0.476 - ETA: 0s - loss: 1.4256 - acc: 0.474 - ETA: 0s - loss: 1.4307 - acc: 0.469 - ETA: 0s - loss: 1.4294 - acc: 0.479 - ETA: 0s - loss: 1.4299 - acc: 0.478 - ETA: 0s - loss: 1.4300 - acc: 0.482 - ETA: 0s - loss: 1.4332 - acc: 0.481 - ETA: 0s - loss: 1.4343 - acc: 0.479 - ETA: 0s - loss: 1.4415 - acc: 0.478 - ETA: 0s - loss: 1.4464 - acc: 0.476 - ETA: 0s - loss: 1.4439 - acc: 0.473 - ETA: 0s - loss: 1.4432 - acc: 0.473 - ETA: 0s - loss: 1.4436 - acc: 0.472 - ETA: 0s - loss: 1.4438 - acc: 0.473 - ETA: 0s - loss: 1.4506 - acc: 0.470 - ETA: 0s - loss: 1.4515 - acc: 0.469 - ETA: 0s - loss: 1.4555 - acc: 0.468 - ETA: 0s - loss: 1.4549 - acc: 0.468 - 1s 225us/step - loss: 1.4547 - acc: 0.4672\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.2869 - acc: 0.593 - ETA: 1s - loss: 1.3729 - acc: 0.513 - ETA: 1s - loss: 1.3851 - acc: 0.501 - ETA: 1s - loss: 1.3543 - acc: 0.522 - ETA: 0s - loss: 1.3674 - acc: 0.517 - ETA: 0s - loss: 1.3789 - acc: 0.512 - ETA: 0s - loss: 1.3738 - acc: 0.510 - ETA: 0s - loss: 1.3871 - acc: 0.502 - ETA: 0s - loss: 1.3879 - acc: 0.500 - ETA: 0s - loss: 1.3819 - acc: 0.501 - ETA: 0s - loss: 1.3903 - acc: 0.498 - ETA: 0s - loss: 1.4010 - acc: 0.491 - ETA: 0s - loss: 1.3996 - acc: 0.491 - ETA: 0s - loss: 1.4054 - acc: 0.488 - ETA: 0s - loss: 1.4086 - acc: 0.488 - ETA: 0s - loss: 1.4126 - acc: 0.487 - ETA: 0s - loss: 1.4190 - acc: 0.486 - ETA: 0s - loss: 1.4232 - acc: 0.483 - ETA: 0s - loss: 1.4223 - acc: 0.483 - ETA: 0s - loss: 1.4299 - acc: 0.482 - ETA: 0s - loss: 1.4334 - acc: 0.481 - ETA: 0s - loss: 1.4369 - acc: 0.479 - ETA: 0s - loss: 1.4383 - acc: 0.478 - 1s 226us/step - loss: 1.4375 - acc: 0.4793\n",
      "Epoch 35/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3384 - acc: 0.500 - ETA: 0s - loss: 1.4100 - acc: 0.488 - ETA: 0s - loss: 1.5060 - acc: 0.463 - ETA: 0s - loss: 1.4778 - acc: 0.476 - ETA: 0s - loss: 1.4555 - acc: 0.473 - ETA: 0s - loss: 1.4753 - acc: 0.460 - ETA: 0s - loss: 1.4639 - acc: 0.465 - ETA: 0s - loss: 1.4570 - acc: 0.467 - ETA: 0s - loss: 1.4513 - acc: 0.467 - ETA: 0s - loss: 1.4395 - acc: 0.472 - ETA: 0s - loss: 1.4433 - acc: 0.470 - ETA: 0s - loss: 1.4370 - acc: 0.476 - ETA: 0s - loss: 1.4404 - acc: 0.476 - ETA: 0s - loss: 1.4373 - acc: 0.477 - ETA: 0s - loss: 1.4386 - acc: 0.475 - ETA: 0s - loss: 1.4355 - acc: 0.474 - ETA: 0s - loss: 1.4368 - acc: 0.475 - ETA: 0s - loss: 1.4286 - acc: 0.479 - ETA: 0s - loss: 1.4311 - acc: 0.477 - 1s 181us/step - loss: 1.4324 - acc: 0.4767\n",
      "Epoch 36/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.2888 - acc: 0.531 - ETA: 1s - loss: 1.5216 - acc: 0.451 - ETA: 1s - loss: 1.4653 - acc: 0.459 - ETA: 1s - loss: 1.4752 - acc: 0.464 - ETA: 0s - loss: 1.4748 - acc: 0.460 - ETA: 1s - loss: 1.4702 - acc: 0.463 - ETA: 1s - loss: 1.4625 - acc: 0.466 - ETA: 1s - loss: 1.4535 - acc: 0.473 - ETA: 1s - loss: 1.4525 - acc: 0.476 - ETA: 1s - loss: 1.4421 - acc: 0.475 - ETA: 1s - loss: 1.4352 - acc: 0.476 - ETA: 1s - loss: 1.4402 - acc: 0.472 - ETA: 1s - loss: 1.4366 - acc: 0.475 - ETA: 1s - loss: 1.4428 - acc: 0.474 - ETA: 1s - loss: 1.4383 - acc: 0.476 - ETA: 0s - loss: 1.4300 - acc: 0.474 - ETA: 0s - loss: 1.4373 - acc: 0.471 - ETA: 0s - loss: 1.4278 - acc: 0.478 - ETA: 0s - loss: 1.4229 - acc: 0.479 - ETA: 0s - loss: 1.4240 - acc: 0.481 - ETA: 0s - loss: 1.4228 - acc: 0.482 - ETA: 0s - loss: 1.4223 - acc: 0.484 - ETA: 0s - loss: 1.4186 - acc: 0.484 - ETA: 0s - loss: 1.4241 - acc: 0.481 - ETA: 0s - loss: 1.4190 - acc: 0.484 - ETA: 0s - loss: 1.4198 - acc: 0.483 - ETA: 0s - loss: 1.4175 - acc: 0.484 - ETA: 0s - loss: 1.4149 - acc: 0.483 - ETA: 0s - loss: 1.4185 - acc: 0.481 - ETA: 0s - loss: 1.4243 - acc: 0.479 - ETA: 0s - loss: 1.4228 - acc: 0.478 - ETA: 0s - loss: 1.4230 - acc: 0.478 - 2s 319us/step - loss: 1.4229 - acc: 0.4775\n",
      "Epoch 37/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.6491 - acc: 0.437 - ETA: 1s - loss: 1.3464 - acc: 0.517 - ETA: 1s - loss: 1.3795 - acc: 0.488 - ETA: 1s - loss: 1.3621 - acc: 0.503 - ETA: 1s - loss: 1.3794 - acc: 0.496 - ETA: 1s - loss: 1.3805 - acc: 0.493 - ETA: 1s - loss: 1.3795 - acc: 0.485 - ETA: 1s - loss: 1.3956 - acc: 0.484 - ETA: 1s - loss: 1.3876 - acc: 0.486 - ETA: 1s - loss: 1.3849 - acc: 0.491 - ETA: 1s - loss: 1.3901 - acc: 0.487 - ETA: 1s - loss: 1.3905 - acc: 0.483 - ETA: 1s - loss: 1.3891 - acc: 0.483 - ETA: 1s - loss: 1.3889 - acc: 0.487 - ETA: 1s - loss: 1.3870 - acc: 0.490 - ETA: 1s - loss: 1.3958 - acc: 0.485 - ETA: 1s - loss: 1.4022 - acc: 0.482 - ETA: 0s - loss: 1.4111 - acc: 0.477 - ETA: 0s - loss: 1.4152 - acc: 0.475 - ETA: 0s - loss: 1.4152 - acc: 0.476 - ETA: 0s - loss: 1.4145 - acc: 0.475 - ETA: 0s - loss: 1.4124 - acc: 0.478 - ETA: 0s - loss: 1.4121 - acc: 0.477 - ETA: 0s - loss: 1.4136 - acc: 0.478 - ETA: 0s - loss: 1.4112 - acc: 0.477 - ETA: 0s - loss: 1.4114 - acc: 0.477 - ETA: 0s - loss: 1.4140 - acc: 0.475 - ETA: 0s - loss: 1.4157 - acc: 0.473 - ETA: 0s - loss: 1.4189 - acc: 0.473 - ETA: 0s - loss: 1.4188 - acc: 0.474 - ETA: 0s - loss: 1.4167 - acc: 0.474 - ETA: 0s - loss: 1.4160 - acc: 0.474 - ETA: 0s - loss: 1.4149 - acc: 0.476 - ETA: 0s - loss: 1.4145 - acc: 0.475 - 2s 343us/step - loss: 1.4130 - acc: 0.4758\n",
      "Epoch 38/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.2665 - acc: 0.531 - ETA: 1s - loss: 1.3318 - acc: 0.492 - ETA: 1s - loss: 1.3014 - acc: 0.500 - ETA: 1s - loss: 1.3196 - acc: 0.503 - ETA: 1s - loss: 1.3518 - acc: 0.497 - ETA: 1s - loss: 1.3771 - acc: 0.495 - ETA: 0s - loss: 1.3933 - acc: 0.485 - ETA: 0s - loss: 1.3973 - acc: 0.483 - ETA: 0s - loss: 1.3913 - acc: 0.482 - ETA: 0s - loss: 1.4020 - acc: 0.482 - ETA: 0s - loss: 1.4039 - acc: 0.485 - ETA: 0s - loss: 1.3996 - acc: 0.488 - ETA: 0s - loss: 1.4067 - acc: 0.484 - ETA: 0s - loss: 1.4045 - acc: 0.488 - ETA: 0s - loss: 1.4035 - acc: 0.486 - ETA: 0s - loss: 1.3984 - acc: 0.489 - ETA: 0s - loss: 1.3958 - acc: 0.489 - ETA: 0s - loss: 1.3923 - acc: 0.489 - ETA: 0s - loss: 1.3942 - acc: 0.491 - ETA: 0s - loss: 1.3942 - acc: 0.490 - ETA: 0s - loss: 1.3948 - acc: 0.490 - ETA: 0s - loss: 1.3972 - acc: 0.489 - 1s 211us/step - loss: 1.3989 - acc: 0.4889\n",
      "Epoch 39/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3645 - acc: 0.500 - ETA: 0s - loss: 1.4693 - acc: 0.460 - ETA: 0s - loss: 1.4379 - acc: 0.465 - ETA: 0s - loss: 1.4060 - acc: 0.482 - ETA: 0s - loss: 1.4044 - acc: 0.488 - ETA: 0s - loss: 1.4022 - acc: 0.480 - ETA: 0s - loss: 1.3807 - acc: 0.490 - ETA: 0s - loss: 1.3966 - acc: 0.488 - ETA: 0s - loss: 1.4012 - acc: 0.486 - ETA: 0s - loss: 1.4052 - acc: 0.486 - ETA: 0s - loss: 1.4157 - acc: 0.485 - ETA: 0s - loss: 1.4166 - acc: 0.483 - ETA: 0s - loss: 1.4196 - acc: 0.481 - ETA: 0s - loss: 1.4255 - acc: 0.483 - ETA: 0s - loss: 1.4247 - acc: 0.482 - ETA: 0s - loss: 1.4275 - acc: 0.482 - ETA: 0s - loss: 1.4258 - acc: 0.481 - ETA: 0s - loss: 1.4228 - acc: 0.482 - ETA: 0s - loss: 1.4253 - acc: 0.480 - 1s 186us/step - loss: 1.4264 - acc: 0.4797\n",
      "Epoch 40/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5873 - acc: 0.312 - ETA: 0s - loss: 1.4158 - acc: 0.471 - ETA: 0s - loss: 1.4278 - acc: 0.468 - ETA: 1s - loss: 1.4472 - acc: 0.463 - ETA: 1s - loss: 1.4367 - acc: 0.464 - ETA: 1s - loss: 1.4546 - acc: 0.454 - ETA: 1s - loss: 1.4479 - acc: 0.458 - ETA: 1s - loss: 1.4341 - acc: 0.463 - ETA: 1s - loss: 1.4252 - acc: 0.468 - ETA: 1s - loss: 1.4222 - acc: 0.468 - ETA: 0s - loss: 1.4214 - acc: 0.471 - ETA: 0s - loss: 1.4228 - acc: 0.479 - ETA: 0s - loss: 1.4179 - acc: 0.478 - ETA: 0s - loss: 1.4180 - acc: 0.477 - ETA: 0s - loss: 1.4167 - acc: 0.477 - ETA: 0s - loss: 1.4163 - acc: 0.477 - ETA: 0s - loss: 1.4267 - acc: 0.471 - ETA: 0s - loss: 1.4207 - acc: 0.471 - ETA: 0s - loss: 1.4172 - acc: 0.472 - ETA: 0s - loss: 1.4165 - acc: 0.476 - ETA: 0s - loss: 1.4105 - acc: 0.478 - ETA: 0s - loss: 1.4124 - acc: 0.479 - ETA: 0s - loss: 1.4114 - acc: 0.481 - ETA: 0s - loss: 1.4129 - acc: 0.481 - ETA: 0s - loss: 1.4132 - acc: 0.479 - ETA: 0s - loss: 1.4128 - acc: 0.480 - ETA: 0s - loss: 1.4117 - acc: 0.480 - ETA: 0s - loss: 1.4071 - acc: 0.481 - ETA: 0s - loss: 1.4046 - acc: 0.481 - ETA: 0s - loss: 1.4089 - acc: 0.480 - ETA: 0s - loss: 1.4072 - acc: 0.482 - 2s 311us/step - loss: 1.4040 - acc: 0.4828\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5505 - acc: 0.406 - ETA: 1s - loss: 1.4390 - acc: 0.486 - ETA: 1s - loss: 1.3974 - acc: 0.494 - ETA: 1s - loss: 1.3698 - acc: 0.491 - ETA: 1s - loss: 1.3681 - acc: 0.486 - ETA: 1s - loss: 1.3814 - acc: 0.485 - ETA: 1s - loss: 1.3862 - acc: 0.488 - ETA: 1s - loss: 1.3791 - acc: 0.494 - ETA: 0s - loss: 1.3670 - acc: 0.497 - ETA: 0s - loss: 1.3618 - acc: 0.500 - ETA: 0s - loss: 1.3759 - acc: 0.489 - ETA: 0s - loss: 1.3832 - acc: 0.489 - ETA: 0s - loss: 1.3891 - acc: 0.485 - ETA: 0s - loss: 1.3921 - acc: 0.487 - ETA: 0s - loss: 1.4012 - acc: 0.484 - ETA: 0s - loss: 1.4046 - acc: 0.485 - ETA: 0s - loss: 1.4105 - acc: 0.481 - ETA: 0s - loss: 1.4050 - acc: 0.484 - ETA: 0s - loss: 1.3997 - acc: 0.486 - ETA: 0s - loss: 1.3980 - acc: 0.487 - ETA: 0s - loss: 1.3987 - acc: 0.488 - ETA: 0s - loss: 1.3988 - acc: 0.488 - ETA: 0s - loss: 1.4024 - acc: 0.486 - ETA: 0s - loss: 1.4010 - acc: 0.487 - ETA: 0s - loss: 1.4024 - acc: 0.486 - ETA: 0s - loss: 1.4021 - acc: 0.487 - 1s 255us/step - loss: 1.4000 - acc: 0.4876\n",
      "Epoch 42/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.6051 - acc: 0.531 - ETA: 1s - loss: 1.3374 - acc: 0.510 - ETA: 1s - loss: 1.3583 - acc: 0.493 - ETA: 1s - loss: 1.3316 - acc: 0.506 - ETA: 1s - loss: 1.3375 - acc: 0.504 - ETA: 1s - loss: 1.3316 - acc: 0.502 - ETA: 1s - loss: 1.3438 - acc: 0.498 - ETA: 1s - loss: 1.3415 - acc: 0.509 - ETA: 1s - loss: 1.3385 - acc: 0.513 - ETA: 1s - loss: 1.3509 - acc: 0.508 - ETA: 1s - loss: 1.3668 - acc: 0.502 - ETA: 1s - loss: 1.3576 - acc: 0.504 - ETA: 1s - loss: 1.3563 - acc: 0.503 - ETA: 1s - loss: 1.3555 - acc: 0.503 - ETA: 1s - loss: 1.3611 - acc: 0.501 - ETA: 1s - loss: 1.3675 - acc: 0.497 - ETA: 1s - loss: 1.3728 - acc: 0.498 - ETA: 0s - loss: 1.3775 - acc: 0.497 - ETA: 0s - loss: 1.3850 - acc: 0.493 - ETA: 0s - loss: 1.3901 - acc: 0.490 - ETA: 0s - loss: 1.3915 - acc: 0.489 - ETA: 0s - loss: 1.3973 - acc: 0.486 - ETA: 0s - loss: 1.3986 - acc: 0.488 - ETA: 0s - loss: 1.3971 - acc: 0.489 - ETA: 0s - loss: 1.3939 - acc: 0.490 - ETA: 0s - loss: 1.3948 - acc: 0.490 - ETA: 0s - loss: 1.3949 - acc: 0.491 - ETA: 0s - loss: 1.3924 - acc: 0.491 - ETA: 0s - loss: 1.3947 - acc: 0.489 - ETA: 0s - loss: 1.3955 - acc: 0.489 - ETA: 0s - loss: 1.3912 - acc: 0.492 - ETA: 0s - loss: 1.3922 - acc: 0.490 - ETA: 0s - loss: 1.3926 - acc: 0.490 - ETA: 0s - loss: 1.3904 - acc: 0.490 - 2s 337us/step - loss: 1.3914 - acc: 0.4903\n",
      "Epoch 43/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.7568 - acc: 0.562 - ETA: 1s - loss: 1.4732 - acc: 0.459 - ETA: 1s - loss: 1.4349 - acc: 0.476 - ETA: 1s - loss: 1.3717 - acc: 0.498 - ETA: 1s - loss: 1.3828 - acc: 0.500 - ETA: 1s - loss: 1.3963 - acc: 0.495 - ETA: 1s - loss: 1.4052 - acc: 0.491 - ETA: 1s - loss: 1.3998 - acc: 0.489 - ETA: 1s - loss: 1.3969 - acc: 0.490 - ETA: 1s - loss: 1.3988 - acc: 0.485 - ETA: 1s - loss: 1.4036 - acc: 0.486 - ETA: 0s - loss: 1.3947 - acc: 0.488 - ETA: 0s - loss: 1.3937 - acc: 0.490 - ETA: 0s - loss: 1.3903 - acc: 0.492 - ETA: 0s - loss: 1.3970 - acc: 0.487 - ETA: 0s - loss: 1.3971 - acc: 0.487 - ETA: 0s - loss: 1.4012 - acc: 0.485 - ETA: 0s - loss: 1.4035 - acc: 0.485 - ETA: 0s - loss: 1.4042 - acc: 0.486 - ETA: 0s - loss: 1.4014 - acc: 0.488 - ETA: 0s - loss: 1.3923 - acc: 0.491 - ETA: 0s - loss: 1.3954 - acc: 0.489 - ETA: 0s - loss: 1.3985 - acc: 0.489 - ETA: 0s - loss: 1.4045 - acc: 0.489 - ETA: 0s - loss: 1.4050 - acc: 0.489 - ETA: 0s - loss: 1.4040 - acc: 0.489 - 1s 259us/step - loss: 1.4040 - acc: 0.4889\n",
      "Epoch 44/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.3752 - acc: 0.468 - ETA: 1s - loss: 1.3238 - acc: 0.527 - ETA: 1s - loss: 1.3938 - acc: 0.491 - ETA: 1s - loss: 1.4010 - acc: 0.496 - ETA: 1s - loss: 1.3718 - acc: 0.506 - ETA: 1s - loss: 1.3985 - acc: 0.495 - ETA: 0s - loss: 1.3984 - acc: 0.492 - ETA: 1s - loss: 1.3966 - acc: 0.490 - ETA: 0s - loss: 1.4048 - acc: 0.492 - ETA: 0s - loss: 1.4039 - acc: 0.493 - ETA: 0s - loss: 1.4033 - acc: 0.494 - ETA: 0s - loss: 1.3977 - acc: 0.490 - ETA: 0s - loss: 1.4048 - acc: 0.489 - ETA: 0s - loss: 1.4082 - acc: 0.488 - ETA: 0s - loss: 1.4124 - acc: 0.487 - ETA: 0s - loss: 1.4163 - acc: 0.484 - ETA: 0s - loss: 1.4093 - acc: 0.485 - ETA: 0s - loss: 1.4053 - acc: 0.486 - ETA: 0s - loss: 1.4035 - acc: 0.487 - ETA: 0s - loss: 1.4072 - acc: 0.485 - ETA: 0s - loss: 1.4010 - acc: 0.487 - ETA: 0s - loss: 1.4077 - acc: 0.486 - ETA: 0s - loss: 1.4038 - acc: 0.487 - ETA: 0s - loss: 1.4013 - acc: 0.488 - ETA: 0s - loss: 1.4017 - acc: 0.488 - ETA: 0s - loss: 1.3995 - acc: 0.488 - ETA: 0s - loss: 1.3937 - acc: 0.493 - ETA: 0s - loss: 1.3892 - acc: 0.495 - ETA: 0s - loss: 1.3942 - acc: 0.493 - ETA: 0s - loss: 1.3943 - acc: 0.493 - ETA: 0s - loss: 1.3927 - acc: 0.493 - 2s 311us/step - loss: 1.3932 - acc: 0.4948\n",
      "Epoch 45/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.3862 - acc: 0.468 - ETA: 1s - loss: 1.4472 - acc: 0.500 - ETA: 1s - loss: 1.4433 - acc: 0.491 - ETA: 1s - loss: 1.4221 - acc: 0.482 - ETA: 1s - loss: 1.4150 - acc: 0.477 - ETA: 1s - loss: 1.4214 - acc: 0.473 - ETA: 0s - loss: 1.4269 - acc: 0.474 - ETA: 0s - loss: 1.4207 - acc: 0.475 - ETA: 0s - loss: 1.4149 - acc: 0.478 - ETA: 0s - loss: 1.4187 - acc: 0.480 - ETA: 0s - loss: 1.4209 - acc: 0.475 - ETA: 0s - loss: 1.4196 - acc: 0.473 - ETA: 0s - loss: 1.4180 - acc: 0.475 - ETA: 0s - loss: 1.4112 - acc: 0.478 - ETA: 0s - loss: 1.4066 - acc: 0.480 - ETA: 0s - loss: 1.4022 - acc: 0.486 - ETA: 0s - loss: 1.3999 - acc: 0.488 - ETA: 0s - loss: 1.4033 - acc: 0.486 - ETA: 0s - loss: 1.4132 - acc: 0.480 - ETA: 0s - loss: 1.4157 - acc: 0.479 - ETA: 0s - loss: 1.4166 - acc: 0.480 - ETA: 0s - loss: 1.4122 - acc: 0.482 - ETA: 0s - loss: 1.4100 - acc: 0.482 - ETA: 0s - loss: 1.4069 - acc: 0.482 - ETA: 0s - loss: 1.4012 - acc: 0.485 - 1s 242us/step - loss: 1.4007 - acc: 0.4848\n",
      "Epoch 46/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.2105 - acc: 0.500 - ETA: 0s - loss: 1.3053 - acc: 0.525 - ETA: 0s - loss: 1.3622 - acc: 0.491 - ETA: 0s - loss: 1.3804 - acc: 0.487 - ETA: 0s - loss: 1.3666 - acc: 0.491 - ETA: 0s - loss: 1.3948 - acc: 0.488 - ETA: 0s - loss: 1.3867 - acc: 0.492 - ETA: 0s - loss: 1.3919 - acc: 0.487 - ETA: 0s - loss: 1.3912 - acc: 0.489 - ETA: 0s - loss: 1.3951 - acc: 0.490 - ETA: 0s - loss: 1.3891 - acc: 0.491 - ETA: 0s - loss: 1.3877 - acc: 0.493 - ETA: 0s - loss: 1.3865 - acc: 0.495 - ETA: 0s - loss: 1.3897 - acc: 0.497 - ETA: 0s - loss: 1.3875 - acc: 0.500 - ETA: 0s - loss: 1.3932 - acc: 0.497 - ETA: 0s - loss: 1.3927 - acc: 0.496 - ETA: 0s - loss: 1.3941 - acc: 0.496 - ETA: 0s - loss: 1.3899 - acc: 0.497 - ETA: 0s - loss: 1.3871 - acc: 0.498 - 1s 191us/step - loss: 1.3890 - acc: 0.4973\n",
      "Epoch 47/50\n",
      "5435/5435 [==============================] - ETA: 1s - loss: 1.5715 - acc: 0.437 - ETA: 1s - loss: 1.3297 - acc: 0.531 - ETA: 1s - loss: 1.3716 - acc: 0.501 - ETA: 0s - loss: 1.3870 - acc: 0.483 - ETA: 0s - loss: 1.4010 - acc: 0.483 - ETA: 0s - loss: 1.3946 - acc: 0.495 - ETA: 0s - loss: 1.3961 - acc: 0.493 - ETA: 0s - loss: 1.4099 - acc: 0.487 - ETA: 0s - loss: 1.4098 - acc: 0.490 - ETA: 0s - loss: 1.4100 - acc: 0.491 - ETA: 0s - loss: 1.4044 - acc: 0.493 - ETA: 0s - loss: 1.3914 - acc: 0.499 - ETA: 0s - loss: 1.3890 - acc: 0.499 - ETA: 0s - loss: 1.3859 - acc: 0.499 - ETA: 0s - loss: 1.3834 - acc: 0.496 - ETA: 0s - loss: 1.3825 - acc: 0.498 - ETA: 0s - loss: 1.3812 - acc: 0.500 - ETA: 0s - loss: 1.3817 - acc: 0.497 - ETA: 0s - loss: 1.3881 - acc: 0.496 - ETA: 0s - loss: 1.3894 - acc: 0.496 - ETA: 0s - loss: 1.3884 - acc: 0.494 - ETA: 0s - loss: 1.3901 - acc: 0.493 - ETA: 0s - loss: 1.3898 - acc: 0.492 - ETA: 0s - loss: 1.3892 - acc: 0.493 - ETA: 0s - loss: 1.3891 - acc: 0.494 - ETA: 0s - loss: 1.3878 - acc: 0.494 - ETA: 0s - loss: 1.3865 - acc: 0.494 - 1s 265us/step - loss: 1.3856 - acc: 0.4944\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5435/5435 [==============================] - ETA: 0s - loss: 1.3319 - acc: 0.531 - ETA: 1s - loss: 1.3933 - acc: 0.489 - ETA: 1s - loss: 1.3895 - acc: 0.482 - ETA: 1s - loss: 1.3623 - acc: 0.497 - ETA: 1s - loss: 1.3836 - acc: 0.486 - ETA: 1s - loss: 1.3968 - acc: 0.482 - ETA: 1s - loss: 1.3802 - acc: 0.493 - ETA: 1s - loss: 1.3883 - acc: 0.500 - ETA: 0s - loss: 1.3915 - acc: 0.498 - ETA: 0s - loss: 1.3865 - acc: 0.499 - ETA: 0s - loss: 1.3836 - acc: 0.497 - ETA: 0s - loss: 1.3801 - acc: 0.497 - ETA: 0s - loss: 1.3709 - acc: 0.499 - ETA: 0s - loss: 1.3658 - acc: 0.499 - ETA: 0s - loss: 1.3646 - acc: 0.500 - ETA: 0s - loss: 1.3646 - acc: 0.503 - ETA: 0s - loss: 1.3680 - acc: 0.499 - ETA: 0s - loss: 1.3645 - acc: 0.500 - ETA: 0s - loss: 1.3632 - acc: 0.500 - ETA: 0s - loss: 1.3615 - acc: 0.501 - ETA: 0s - loss: 1.3628 - acc: 0.503 - ETA: 0s - loss: 1.3670 - acc: 0.501 - ETA: 0s - loss: 1.3698 - acc: 0.500 - ETA: 0s - loss: 1.3686 - acc: 0.502 - ETA: 0s - loss: 1.3692 - acc: 0.501 - 1s 245us/step - loss: 1.3700 - acc: 0.5001\n",
      "Epoch 49/50\n",
      "5435/5435 [==============================] - ETA: 0s - loss: 1.4646 - acc: 0.468 - ETA: 1s - loss: 1.4480 - acc: 0.468 - ETA: 1s - loss: 1.3976 - acc: 0.470 - ETA: 1s - loss: 1.3545 - acc: 0.491 - ETA: 1s - loss: 1.3741 - acc: 0.482 - ETA: 1s - loss: 1.3643 - acc: 0.485 - ETA: 0s - loss: 1.3776 - acc: 0.480 - ETA: 0s - loss: 1.3744 - acc: 0.480 - ETA: 0s - loss: 1.3730 - acc: 0.489 - ETA: 0s - loss: 1.3642 - acc: 0.493 - ETA: 0s - loss: 1.3601 - acc: 0.496 - ETA: 0s - loss: 1.3680 - acc: 0.492 - ETA: 0s - loss: 1.3617 - acc: 0.495 - ETA: 0s - loss: 1.3664 - acc: 0.492 - ETA: 0s - loss: 1.3645 - acc: 0.492 - ETA: 0s - loss: 1.3682 - acc: 0.494 - ETA: 0s - loss: 1.3626 - acc: 0.498 - ETA: 0s - loss: 1.3571 - acc: 0.501 - ETA: 0s - loss: 1.3604 - acc: 0.501 - ETA: 0s - loss: 1.3610 - acc: 0.501 - ETA: 0s - loss: 1.3612 - acc: 0.500 - ETA: 0s - loss: 1.3574 - acc: 0.504 - ETA: 0s - loss: 1.3556 - acc: 0.504 - ETA: 0s - loss: 1.3620 - acc: 0.503 - ETA: 0s - loss: 1.3589 - acc: 0.505 - ETA: 0s - loss: 1.3591 - acc: 0.504 - 1s 250us/step - loss: 1.3597 - acc: 0.5045\n",
      "Epoch 50/50\n",
      "5435/5435 [==============================] - ETA: 2s - loss: 1.4063 - acc: 0.406 - ETA: 1s - loss: 1.3556 - acc: 0.477 - ETA: 2s - loss: 1.3020 - acc: 0.506 - ETA: 1s - loss: 1.3202 - acc: 0.500 - ETA: 1s - loss: 1.2979 - acc: 0.516 - ETA: 1s - loss: 1.3077 - acc: 0.524 - ETA: 1s - loss: 1.3246 - acc: 0.513 - ETA: 1s - loss: 1.3304 - acc: 0.512 - ETA: 1s - loss: 1.3372 - acc: 0.510 - ETA: 0s - loss: 1.3477 - acc: 0.506 - ETA: 0s - loss: 1.3593 - acc: 0.501 - ETA: 0s - loss: 1.3630 - acc: 0.501 - ETA: 0s - loss: 1.3687 - acc: 0.503 - ETA: 0s - loss: 1.3629 - acc: 0.504 - ETA: 0s - loss: 1.3623 - acc: 0.505 - ETA: 0s - loss: 1.3628 - acc: 0.504 - ETA: 0s - loss: 1.3579 - acc: 0.504 - ETA: 0s - loss: 1.3578 - acc: 0.506 - ETA: 0s - loss: 1.3590 - acc: 0.504 - ETA: 0s - loss: 1.3659 - acc: 0.502 - ETA: 0s - loss: 1.3646 - acc: 0.503 - ETA: 0s - loss: 1.3644 - acc: 0.505 - 1s 214us/step - loss: 1.3628 - acc: 0.5063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18b1cd08f28>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parser(row):\n",
    "    global test_error_count\n",
    "    global test_error_labels\n",
    "    path_to_wav_files = PATH_TO_TEST_AUDIO_FILES\n",
    "    file_path = path_to_wav_files + str(row.ID) + \".wav\"\n",
    "    try:\n",
    "        data, sampling_rate = librosa.load(file_path)\n",
    "        stft = np.abs(librosa.stft(data))\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sampling_rate).T,axis=0)\n",
    "    except Exception as ex:\n",
    "        test_error_count += 1\n",
    "        test_error_labels.append(row.ID)\n",
    "        return pd.Series([0]*12)\n",
    "    features = chroma\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580fa44a2c484e6b98cf23f4efbba7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3297), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\librosa\\core\\pitch.py:145: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 samples had errors while parsing\n",
      "Errorneous samples []\n"
     ]
    }
   ],
   "source": [
    "test_features = test.progress_apply(test_parser,axis=1, reduce = True)\n",
    "print(\"%d samples had errors while parsing\" % test_error_count)\n",
    "print(\"Errorneous samples\", test_error_labels)\n",
    "save_as_pickle(data=train_features,pickle_file=PATH_TO_PICKLE + SUBMISSION_TITLE + \" test.pickle\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_features\n",
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrung\\AppData\\Local\\Continuum\\miniconda3\\envs\\deeplearning\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "test_labels_strings = lb.inverse_transform(test_labels.argmax(axis=1))\n",
    "# test_labels_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Class'] = test_labels_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(PATH_TO_SUBMISSION + SUBMISSION_TITLE + \".csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach gives 56% accuracy with the above setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
